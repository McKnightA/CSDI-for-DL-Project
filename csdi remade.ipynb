{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2a8fbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e436b4",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7aa59b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PM25_Dataset(Dataset):\n",
    "    def __init__(self, eval_length=36, target_dim=36, mode=\"train\", validindex=0):\n",
    "        self.eval_length = eval_length\n",
    "        self.target_dim = target_dim\n",
    "\n",
    "        path = \"./data/pm25/pm25_meanstd.pk\"\n",
    "        with open(path, \"rb\") as f:\n",
    "            self.train_mean, self.train_std = pickle.load(f)\n",
    "        if mode == \"train\":\n",
    "            month_list = [1, 2, 4, 5, 7, 8, 10, 11]\n",
    "            # 1st,4th,7th,10th months are excluded from histmask (since the months are used for creating missing patterns in test dataset)\n",
    "            flag_for_histmask = [0, 1, 0, 1, 0, 1, 0, 1] \n",
    "            month_list.pop(validindex)\n",
    "            flag_for_histmask.pop(validindex)\n",
    "        elif mode == \"valid\":\n",
    "            month_list = [1, 2, 4, 5, 7, 8, 10, 11]\n",
    "            month_list = month_list[validindex : validindex + 1]\n",
    "        elif mode == \"test\":\n",
    "            month_list = [3, 6, 9, 12]\n",
    "        self.month_list = month_list\n",
    "\n",
    "        # create data for batch\n",
    "        self.observed_data = []  # values (separated into each month)\n",
    "        self.observed_mask = []  # masks (separated into each month)\n",
    "        self.gt_mask = []  # ground-truth masks (separated into each month)\n",
    "        self.index_month = []  # indicate month\n",
    "        self.position_in_month = []  # indicate the start position in month (length is the same as index_month)\n",
    "        self.valid_for_histmask = []  # whether the sample is used for histmask\n",
    "        self.use_index = []  # to separate train/valid/test\n",
    "        self.cut_length = []  # excluded from evaluation targets\n",
    "\n",
    "        df = pd.read_csv(\n",
    "            \"./data/pm25/Code/STMVL/SampleData/pm25_ground.txt\",\n",
    "            index_col=\"datetime\",\n",
    "            parse_dates=True,\n",
    "        )\n",
    "        df_gt = pd.read_csv(\n",
    "            \"./data/pm25/Code/STMVL/SampleData/pm25_missing.txt\",\n",
    "            index_col=\"datetime\",\n",
    "            parse_dates=True,\n",
    "        )\n",
    "        for i in range(len(month_list)):\n",
    "            current_df = df[df.index.month == month_list[i]]\n",
    "            current_df_gt = df_gt[df_gt.index.month == month_list[i]]\n",
    "            current_length = len(current_df) - eval_length + 1\n",
    "\n",
    "            last_index = len(self.index_month)\n",
    "            self.index_month += np.array([i] * current_length).tolist()\n",
    "            self.position_in_month += np.arange(current_length).tolist()\n",
    "            if mode == \"train\":\n",
    "                self.valid_for_histmask += np.array(\n",
    "                    [flag_for_histmask[i]] * current_length\n",
    "                ).tolist()\n",
    "\n",
    "            # mask values for observed indices are 1\n",
    "            c_mask = 1 - current_df.isnull().values\n",
    "            c_gt_mask = 1 - current_df_gt.isnull().values\n",
    "            c_data = (\n",
    "                (current_df.fillna(0).values - self.train_mean) / self.train_std\n",
    "            ) * c_mask\n",
    "\n",
    "            self.observed_mask.append(c_mask)\n",
    "            self.gt_mask.append(c_gt_mask)\n",
    "            self.observed_data.append(c_data)\n",
    "\n",
    "            if mode == \"test\":\n",
    "                n_sample = len(current_df) // eval_length\n",
    "                # interval size is eval_length (missing values are imputed only once)\n",
    "                c_index = np.arange(\n",
    "                    last_index, last_index + eval_length * n_sample, eval_length\n",
    "                )\n",
    "                self.use_index += c_index.tolist()\n",
    "                self.cut_length += [0] * len(c_index)\n",
    "                if len(current_df) % eval_length != 0:  # avoid double-count for the last time-series\n",
    "                    self.use_index += [len(self.index_month) - 1]\n",
    "                    self.cut_length += [eval_length - len(current_df) % eval_length]\n",
    "\n",
    "        if mode != \"test\":\n",
    "            self.use_index = np.arange(len(self.index_month))\n",
    "            self.cut_length = [0] * len(self.use_index)\n",
    "\n",
    "        # masks for 1st,4th,7th,10th months are used for creating missing patterns in test data,\n",
    "        # so these months are excluded from histmask to avoid leakage\n",
    "        if mode == \"train\":\n",
    "            ind = -1\n",
    "            self.index_month_histmask = []\n",
    "            self.position_in_month_histmask = []\n",
    "\n",
    "            for i in range(len(self.index_month)):\n",
    "                while True:\n",
    "                    ind += 1\n",
    "                    if ind == len(self.index_month):\n",
    "                        ind = 0\n",
    "                    if self.valid_for_histmask[ind] == 1:\n",
    "                        self.index_month_histmask.append(self.index_month[ind])\n",
    "                        self.position_in_month_histmask.append(\n",
    "                            self.position_in_month[ind]\n",
    "                        )\n",
    "                        break\n",
    "        else:  # dummy (histmask is only used for training)\n",
    "            self.index_month_histmask = self.index_month\n",
    "            self.position_in_month_histmask = self.position_in_month\n",
    "\n",
    "    def __getitem__(self, org_index):\n",
    "        index = self.use_index[org_index]\n",
    "        c_month = self.index_month[index]\n",
    "        c_index = self.position_in_month[index]\n",
    "        hist_month = self.index_month_histmask[index]\n",
    "        hist_index = self.position_in_month_histmask[index]\n",
    "        s = {\n",
    "            \"observed_data\": self.observed_data[c_month][\n",
    "                c_index : c_index + self.eval_length\n",
    "            ],\n",
    "            \"observed_mask\": self.observed_mask[c_month][\n",
    "                c_index : c_index + self.eval_length\n",
    "            ],\n",
    "            \"gt_mask\": self.gt_mask[c_month][\n",
    "                c_index : c_index + self.eval_length\n",
    "            ],\n",
    "            \"hist_mask\": self.observed_mask[hist_month][\n",
    "                hist_index : hist_index + self.eval_length\n",
    "            ],\n",
    "            \"timepoints\": np.arange(self.eval_length),\n",
    "            \"cut_length\": self.cut_length[org_index],\n",
    "        }\n",
    "\n",
    "        return s\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.use_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52736cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(batch_size, device, validindex=0):\n",
    "    dataset = PM25_Dataset(mode=\"train\", validindex=validindex)\n",
    "    train_loader = DataLoader(\n",
    "        dataset, batch_size=batch_size, num_workers=3, shuffle=True\n",
    "    )\n",
    "    dataset_test = PM25_Dataset(mode=\"test\", validindex=validindex)\n",
    "    test_loader = DataLoader(\n",
    "        dataset_test, batch_size=batch_size, num_workers=1, shuffle=False\n",
    "    )\n",
    "    dataset_valid = PM25_Dataset(mode=\"valid\", validindex=validindex)\n",
    "    valid_loader = DataLoader(\n",
    "        dataset_valid, batch_size=batch_size, num_workers=1, shuffle=False\n",
    "    )\n",
    "\n",
    "    scaler = torch.from_numpy(dataset.train_std).to(device).float()\n",
    "    mean_scaler = torch.from_numpy(dataset.train_mean).to(device).float()\n",
    "\n",
    "    return train_loader, valid_loader, test_loader, scaler, mean_scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5722262",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0543cd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_torch_trans(heads=8, layers=1, channels=64):\n",
    "    encoder_layer = nn.TransformerEncoderLayer(\n",
    "        d_model=channels, nhead=heads, dim_feedforward=64, activation=\"gelu\"\n",
    "    )\n",
    "    return nn.TransformerEncoder(encoder_layer, num_layers=layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2b1519d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv1d_with_init(in_channels, out_channels, kernel_size):\n",
    "    layer = nn.Conv1d(in_channels, out_channels, kernel_size)\n",
    "    nn.init.kaiming_normal_(layer.weight)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb58742f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionEmbedding(nn.Module):\n",
    "    def __init__(self, num_steps, embedding_dim=128, projection_dim=None):\n",
    "        super().__init__()\n",
    "        if projection_dim is None:\n",
    "            projection_dim = embedding_dim\n",
    "        self.register_buffer(\n",
    "            \"embedding\",\n",
    "            self._build_embedding(num_steps, embedding_dim / 2),\n",
    "            persistent=False,\n",
    "        )\n",
    "        self.projection1 = nn.Linear(embedding_dim, projection_dim)\n",
    "        self.projection2 = nn.Linear(projection_dim, projection_dim)\n",
    "\n",
    "    def forward(self, diffusion_step):\n",
    "        x = self.embedding[diffusion_step]\n",
    "        x = self.projection1(x)\n",
    "        x = F.silu(x)\n",
    "        x = self.projection2(x)\n",
    "        x = F.silu(x)\n",
    "        return x\n",
    "\n",
    "    def _build_embedding(self, num_steps, dim=64):\n",
    "        steps = torch.arange(num_steps).unsqueeze(1)  # (T,1)\n",
    "        frequencies = 10.0 ** (torch.arange(dim) / (dim - 1) * 4.0).unsqueeze(0)  # (1,dim)\n",
    "        table = steps * frequencies  # (T,dim)\n",
    "        table = torch.cat([torch.sin(table), torch.cos(table)], dim=1)  # (T,dim*2)\n",
    "        return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca48fcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class diff_CSDI(nn.Module):\n",
    "    def __init__(self, config, inputdim=2):\n",
    "        super().__init__()\n",
    "        self.channels = config[\"channels\"]\n",
    "\n",
    "        self.diffusion_embedding = DiffusionEmbedding(\n",
    "            num_steps=config[\"num_steps\"],\n",
    "            embedding_dim=config[\"diffusion_embedding_dim\"],\n",
    "        )\n",
    "\n",
    "        self.input_projection = Conv1d_with_init(inputdim, self.channels, 1)\n",
    "        self.output_projection1 = Conv1d_with_init(self.channels, self.channels, 1)\n",
    "        self.output_projection2 = Conv1d_with_init(self.channels, 1, 1)\n",
    "        nn.init.zeros_(self.output_projection2.weight)\n",
    "\n",
    "        self.residual_layers = nn.ModuleList(\n",
    "            [\n",
    "                ResidualBlock(\n",
    "                    side_dim=config[\"side_dim\"],\n",
    "                    channels=self.channels,\n",
    "                    diffusion_embedding_dim=config[\"diffusion_embedding_dim\"],\n",
    "                    nheads=config[\"nheads\"],\n",
    "                )\n",
    "                for _ in range(config[\"layers\"])\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, cond_info, diffusion_step):\n",
    "        B, inputdim, K, L = x.shape\n",
    "\n",
    "        x = x.reshape(B, inputdim, K * L)\n",
    "        x = self.input_projection(x)\n",
    "        x = F.relu(x)\n",
    "        x = x.reshape(B, self.channels, K, L)\n",
    "\n",
    "        diffusion_emb = self.diffusion_embedding(diffusion_step)\n",
    "\n",
    "        skip = []\n",
    "        for layer in self.residual_layers:\n",
    "            x, skip_connection = layer(x, cond_info, diffusion_emb)\n",
    "            skip.append(skip_connection)\n",
    "\n",
    "        x = torch.sum(torch.stack(skip), dim=0) / math.sqrt(len(self.residual_layers))\n",
    "        x = x.reshape(B, self.channels, K * L)\n",
    "        x = self.output_projection1(x)  # (B,channel,K*L)\n",
    "        x = F.relu(x)\n",
    "        x = self.output_projection2(x)  # (B,1,K*L)\n",
    "        x = x.reshape(B, K, L)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8165b8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, side_dim, channels, diffusion_embedding_dim, nheads):\n",
    "        super().__init__()\n",
    "        self.diffusion_projection = nn.Linear(diffusion_embedding_dim, channels)\n",
    "        self.cond_projection = Conv1d_with_init(side_dim, 2 * channels, 1)\n",
    "        self.mid_projection = Conv1d_with_init(channels, 2 * channels, 1)\n",
    "        self.output_projection = Conv1d_with_init(channels, 2 * channels, 1)\n",
    "\n",
    "        self.time_layer = get_torch_trans(heads=nheads, layers=1, channels=channels)\n",
    "        self.feature_layer = get_torch_trans(heads=nheads, layers=1, channels=channels)\n",
    "\n",
    "    def forward_time(self, y, base_shape):\n",
    "        B, channel, K, L = base_shape\n",
    "        if L == 1:\n",
    "            return y\n",
    "        y = y.reshape(B, channel, K, L).permute(0, 2, 1, 3).reshape(B * K, channel, L)\n",
    "        y = self.time_layer(y.permute(2, 0, 1)).permute(1, 2, 0)\n",
    "        y = y.reshape(B, K, channel, L).permute(0, 2, 1, 3).reshape(B, channel, K * L)\n",
    "        return y\n",
    "\n",
    "    def forward_feature(self, y, base_shape):\n",
    "        B, channel, K, L = base_shape\n",
    "        if K == 1:\n",
    "            return y\n",
    "        y = y.reshape(B, channel, K, L).permute(0, 3, 1, 2).reshape(B * L, channel, K)\n",
    "        y = self.feature_layer(y.permute(2, 0, 1)).permute(1, 2, 0)\n",
    "        y = y.reshape(B, L, channel, K).permute(0, 2, 3, 1).reshape(B, channel, K * L)\n",
    "        return y\n",
    "\n",
    "    def forward(self, x, cond_info, diffusion_emb):\n",
    "        B, channel, K, L = x.shape\n",
    "        base_shape = x.shape\n",
    "        x = x.reshape(B, channel, K * L)\n",
    "\n",
    "        diffusion_emb = self.diffusion_projection(diffusion_emb).unsqueeze(-1)  # (B,channel,1)\n",
    "        y = x + diffusion_emb\n",
    "\n",
    "        y = self.forward_time(y, base_shape)\n",
    "        y = self.forward_feature(y, base_shape)  # (B,channel,K*L)\n",
    "        y = self.mid_projection(y)  # (B,2*channel,K*L)\n",
    "\n",
    "        _, cond_dim, _, _ = cond_info.shape\n",
    "        cond_info = cond_info.reshape(B, cond_dim, K * L)\n",
    "        cond_info = self.cond_projection(cond_info)  # (B,2*channel,K*L)\n",
    "        y = y + cond_info\n",
    "\n",
    "        gate, filter = torch.chunk(y, 2, dim=1)\n",
    "        y = torch.sigmoid(gate) * torch.tanh(filter)  # (B,channel,K*L)\n",
    "        y = self.output_projection(y)\n",
    "\n",
    "        residual, skip = torch.chunk(y, 2, dim=1)\n",
    "        x = x.reshape(base_shape)\n",
    "        residual = residual.reshape(base_shape)\n",
    "        skip = skip.reshape(base_shape)\n",
    "        return (x + residual) / math.sqrt(2.0), skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7a8382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSDI_base(nn.Module):\n",
    "    def __init__(self, target_dim, config, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.target_dim = target_dim\n",
    "\n",
    "        self.emb_time_dim = config[\"model\"][\"timeemb\"]\n",
    "        self.emb_feature_dim = config[\"model\"][\"featureemb\"]\n",
    "        self.is_unconditional = config[\"model\"][\"is_unconditional\"]\n",
    "        self.target_strategy = config[\"model\"][\"target_strategy\"]\n",
    "\n",
    "        self.emb_total_dim = self.emb_time_dim + self.emb_feature_dim\n",
    "        if self.is_unconditional == False:\n",
    "            self.emb_total_dim += 1  # for conditional mask\n",
    "        self.embed_layer = nn.Embedding(\n",
    "            num_embeddings=self.target_dim, embedding_dim=self.emb_feature_dim\n",
    "        )\n",
    "\n",
    "        config_diff = config[\"diffusion\"]\n",
    "        config_diff[\"side_dim\"] = self.emb_total_dim\n",
    "\n",
    "        input_dim = 1 if self.is_unconditional == True else 2\n",
    "        self.diffmodel = diff_CSDI(config_diff, input_dim)\n",
    "\n",
    "        # parameters for diffusion models\n",
    "        self.num_steps = config_diff[\"num_steps\"]\n",
    "        if config_diff[\"schedule\"] == \"quad\":\n",
    "            self.beta = np.linspace(\n",
    "                config_diff[\"beta_start\"] ** 0.5, config_diff[\"beta_end\"] ** 0.5, self.num_steps\n",
    "            ) ** 2\n",
    "        elif config_diff[\"schedule\"] == \"linear\":\n",
    "            self.beta = np.linspace(\n",
    "                config_diff[\"beta_start\"], config_diff[\"beta_end\"], self.num_steps\n",
    "            )\n",
    "\n",
    "        self.alpha_hat = 1 - self.beta\n",
    "        self.alpha = np.cumprod(self.alpha_hat)\n",
    "        self.alpha_torch = torch.tensor(self.alpha).float().to(self.device).unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "    def time_embedding(self, pos, d_model=128):\n",
    "        pe = torch.zeros(pos.shape[0], pos.shape[1], d_model).to(self.device)\n",
    "        position = pos.unsqueeze(2)\n",
    "        div_term = 1 / torch.pow(\n",
    "            10000.0, torch.arange(0, d_model, 2).to(self.device) / d_model\n",
    "        )\n",
    "        pe[:, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, :, 1::2] = torch.cos(position * div_term)\n",
    "        return pe\n",
    "\n",
    "    def get_randmask(self, observed_mask):\n",
    "        rand_for_mask = torch.rand_like(observed_mask) * observed_mask\n",
    "        rand_for_mask = rand_for_mask.reshape(len(rand_for_mask), -1)\n",
    "        for i in range(len(observed_mask)):\n",
    "            sample_ratio = np.random.rand()  # missing ratio\n",
    "            num_observed = observed_mask[i].sum().item()\n",
    "            num_masked = round(num_observed * sample_ratio)\n",
    "            rand_for_mask[i][rand_for_mask[i].topk(num_masked).indices] = -1\n",
    "        cond_mask = (rand_for_mask > 0).reshape(observed_mask.shape).float()\n",
    "        return cond_mask\n",
    "\n",
    "    def get_hist_mask(self, observed_mask, for_pattern_mask=None):\n",
    "        if for_pattern_mask is None:\n",
    "            for_pattern_mask = observed_mask\n",
    "        if self.target_strategy == \"mix\":\n",
    "            rand_mask = self.get_randmask(observed_mask)\n",
    "\n",
    "        cond_mask = observed_mask.clone()\n",
    "        for i in range(len(cond_mask)):\n",
    "            mask_choice = np.random.rand()\n",
    "            if self.target_strategy == \"mix\" and mask_choice > 0.5:\n",
    "                cond_mask[i] = rand_mask[i]\n",
    "            else:  # draw another sample for histmask (i-1 corresponds to another sample)\n",
    "                cond_mask[i] = cond_mask[i] * for_pattern_mask[i - 1] \n",
    "        return cond_mask\n",
    "\n",
    "    def get_side_info(self, observed_tp, cond_mask):\n",
    "        B, K, L = cond_mask.shape\n",
    "\n",
    "        time_embed = self.time_embedding(observed_tp, self.emb_time_dim)  # (B,L,emb)\n",
    "        time_embed = time_embed.unsqueeze(2).expand(-1, -1, K, -1)\n",
    "        feature_embed = self.embed_layer(\n",
    "            torch.arange(self.target_dim).to(self.device)\n",
    "        )  # (K,emb)\n",
    "        feature_embed = feature_embed.unsqueeze(0).unsqueeze(0).expand(B, L, -1, -1)\n",
    "\n",
    "        side_info = torch.cat([time_embed, feature_embed], dim=-1)  # (B,L,K,*)\n",
    "        side_info = side_info.permute(0, 3, 2, 1)  # (B,*,K,L)\n",
    "\n",
    "        if self.is_unconditional == False:\n",
    "            side_mask = cond_mask.unsqueeze(1)  # (B,1,K,L)\n",
    "            side_info = torch.cat([side_info, side_mask], dim=1)\n",
    "\n",
    "        return side_info\n",
    "\n",
    "    def calc_loss_valid(\n",
    "        self, observed_data, cond_mask, observed_mask, side_info, is_train\n",
    "    ):\n",
    "        loss_sum = 0\n",
    "        for t in range(self.num_steps):  # calculate loss for all t\n",
    "            loss = self.calc_loss(\n",
    "                observed_data, cond_mask, observed_mask, side_info, is_train, set_t=t\n",
    "            )\n",
    "            loss_sum += loss.detach()\n",
    "        return loss_sum / self.num_steps\n",
    "\n",
    "    def calc_loss(\n",
    "        self, observed_data, cond_mask, observed_mask, side_info, is_train, set_t=-1\n",
    "    ):\n",
    "        B, K, L = observed_data.shape\n",
    "        if is_train != 1:  # for validation\n",
    "            t = (torch.ones(B) * set_t).long().to(self.device)\n",
    "        else:\n",
    "            t = torch.randint(0, self.num_steps, [B]).to(self.device)\n",
    "        current_alpha = self.alpha_torch[t]  # (B,1,1)\n",
    "        noise = torch.randn_like(observed_data)\n",
    "        noisy_data = (current_alpha ** 0.5) * observed_data + (1.0 - current_alpha) ** 0.5 * noise\n",
    "\n",
    "        total_input = self.set_input_to_diffmodel(noisy_data, observed_data, cond_mask)\n",
    "\n",
    "        predicted = self.diffmodel(total_input, side_info, t)  # (B,K,L)\n",
    "\n",
    "        target_mask = observed_mask - cond_mask\n",
    "        residual = (noise - predicted) * target_mask\n",
    "        num_eval = target_mask.sum()\n",
    "        loss = (residual ** 2).sum() / (num_eval if num_eval > 0 else 1)\n",
    "        return loss\n",
    "\n",
    "    def set_input_to_diffmodel(self, noisy_data, observed_data, cond_mask):\n",
    "        if self.is_unconditional == True:\n",
    "            total_input = noisy_data.unsqueeze(1)  # (B,1,K,L)\n",
    "        else:\n",
    "            cond_obs = (cond_mask * observed_data).unsqueeze(1)\n",
    "            noisy_target = ((1 - cond_mask) * noisy_data).unsqueeze(1)\n",
    "            total_input = torch.cat([cond_obs, noisy_target], dim=1)  # (B,2,K,L)\n",
    "\n",
    "        return total_input\n",
    "\n",
    "    def impute(self, observed_data, cond_mask, side_info, n_samples):\n",
    "        B, K, L = observed_data.shape\n",
    "\n",
    "        imputed_samples = torch.zeros(B, n_samples, K, L).to(self.device)\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            # generate noisy observation for unconditional model\n",
    "            if self.is_unconditional == True:\n",
    "                noisy_obs = observed_data\n",
    "                noisy_cond_history = []\n",
    "                for t in range(self.num_steps):\n",
    "                    noise = torch.randn_like(noisy_obs)\n",
    "                    noisy_obs = (self.alpha_hat[t] ** 0.5) * noisy_obs + self.beta[t] ** 0.5 * noise\n",
    "                    noisy_cond_history.append(noisy_obs * cond_mask)\n",
    "\n",
    "            current_sample = torch.randn_like(observed_data)\n",
    "\n",
    "            for t in range(self.num_steps - 1, -1, -1):\n",
    "                if self.is_unconditional == True:\n",
    "                    diff_input = cond_mask * noisy_cond_history[t] + (1.0 - cond_mask) * current_sample\n",
    "                    diff_input = diff_input.unsqueeze(1)  # (B,1,K,L)\n",
    "                else:\n",
    "                    cond_obs = (cond_mask * observed_data).unsqueeze(1)\n",
    "                    noisy_target = ((1 - cond_mask) * current_sample).unsqueeze(1)\n",
    "                    diff_input = torch.cat([cond_obs, noisy_target], dim=1)  # (B,2,K,L)\n",
    "                predicted = self.diffmodel(diff_input, side_info, torch.tensor([t]).to(self.device))\n",
    "\n",
    "                coeff1 = 1 / self.alpha_hat[t] ** 0.5\n",
    "                coeff2 = (1 - self.alpha_hat[t]) / (1 - self.alpha[t]) ** 0.5\n",
    "                current_sample = coeff1 * (current_sample - coeff2 * predicted)\n",
    "\n",
    "                if t > 0:\n",
    "                    noise = torch.randn_like(current_sample)\n",
    "                    sigma = (\n",
    "                        (1.0 - self.alpha[t - 1]) / (1.0 - self.alpha[t]) * self.beta[t]\n",
    "                    ) ** 0.5\n",
    "                    current_sample += sigma * noise\n",
    "\n",
    "            imputed_samples[:, i] = current_sample.detach()\n",
    "        return imputed_samples\n",
    "\n",
    "    def forward(self, batch, is_train=1):\n",
    "        (\n",
    "            observed_data,\n",
    "            observed_mask,\n",
    "            observed_tp,\n",
    "            gt_mask,\n",
    "            for_pattern_mask,\n",
    "            _,\n",
    "        ) = self.process_data(batch)\n",
    "        if is_train == 0:\n",
    "            cond_mask = gt_mask\n",
    "        elif self.target_strategy != \"random\":\n",
    "            cond_mask = self.get_hist_mask(\n",
    "                observed_mask, for_pattern_mask=for_pattern_mask\n",
    "            )\n",
    "        else:\n",
    "            cond_mask = self.get_randmask(observed_mask)\n",
    "\n",
    "        side_info = self.get_side_info(observed_tp, cond_mask)\n",
    "\n",
    "        loss_func = self.calc_loss if is_train == 1 else self.calc_loss_valid\n",
    "\n",
    "        return loss_func(observed_data, cond_mask, observed_mask, side_info, is_train)\n",
    "\n",
    "    def evaluate(self, batch, n_samples):\n",
    "        (\n",
    "            observed_data,\n",
    "            observed_mask,\n",
    "            observed_tp,\n",
    "            gt_mask,\n",
    "            _,\n",
    "            cut_length,\n",
    "        ) = self.process_data(batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            cond_mask = gt_mask\n",
    "            target_mask = observed_mask - cond_mask\n",
    "\n",
    "            side_info = self.get_side_info(observed_tp, cond_mask)\n",
    "\n",
    "            samples = self.impute(observed_data, cond_mask, side_info, n_samples)\n",
    "\n",
    "            for i in range(len(cut_length)):  # to avoid double evaluation\n",
    "                target_mask[i, ..., 0 : cut_length[i].item()] = 0\n",
    "        return samples, observed_data, target_mask, observed_mask, observed_tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afba27fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSDI_PM25(CSDI_base):\n",
    "    def __init__(self, config, device, target_dim=36):\n",
    "        super(CSDI_PM25, self).__init__(target_dim, config, device)\n",
    "\n",
    "    def process_data(self, batch):\n",
    "        observed_data = batch[\"observed_data\"].to(self.device).float()\n",
    "        observed_mask = batch[\"observed_mask\"].to(self.device).float()\n",
    "        observed_tp = batch[\"timepoints\"].to(self.device).float()\n",
    "        gt_mask = batch[\"gt_mask\"].to(self.device).float()\n",
    "        cut_length = batch[\"cut_length\"].to(self.device).long()\n",
    "        for_pattern_mask = batch[\"hist_mask\"].to(self.device).float()\n",
    "\n",
    "        observed_data = observed_data.permute(0, 2, 1)\n",
    "        observed_mask = observed_mask.permute(0, 2, 1)\n",
    "        gt_mask = gt_mask.permute(0, 2, 1)\n",
    "        for_pattern_mask = for_pattern_mask.permute(0, 2, 1)\n",
    "\n",
    "        return (\n",
    "            observed_data,\n",
    "            observed_mask,\n",
    "            observed_tp,\n",
    "            gt_mask,\n",
    "            for_pattern_mask,\n",
    "            cut_length,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcaa177",
   "metadata": {},
   "source": [
    "## using the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6d822a",
   "metadata": {},
   "source": [
    "### our using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f9a34c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "def train(model, data_loader, epochs, lr, loss_func, device = \"cuda\", verbose = True):\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    epoch_loss = 0\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            optimizer.zero_grad()\n",
    "            # predicted_noise, noise = \n",
    "            loss = model(batch.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_list.append(loss.item())\n",
    "            if i % 2 == 0:       \n",
    "                ax.clear()\n",
    "                ax.set_ylim(0, 1)\n",
    "                ax.plot(loss_list)\n",
    "                #ax.text(len(loss_list) - 1, loss_list[-1], str(round(loss_list[-1], 3)))\n",
    "                #add a smooth line to the plot every 100 steps\n",
    "                if len(loss_list) > 100:\n",
    "                    ax.plot(np.convolve(loss_list, np.ones((100,))/100, mode='valid'))\n",
    "                    #show the last loss value on the plot\n",
    "                    ax.text(len(loss_list) - 1, np.convolve(loss_list, np.ones((100,))/100, mode='valid')[-1],\n",
    "                             str(round(np.convolve(loss_list, np.ones((100,))/100, mode='valid')[-1], 3)))\n",
    "                ax.text(0.1, 0.1, \"Epoch: \" + str(epoch) + \" Loss: \" + str(round(epoch_loss, 3)))\n",
    "                display(fig)\n",
    "                clear_output(wait=True)\n",
    "            #print(\"Epoch: \", epoch, \"Loss: \", loss.item())\n",
    "        end = time.time()    \n",
    "        if verbose:\n",
    "            #add the epoch average loss to the plot\n",
    "            #find the number of batches in the epoch\n",
    "            num_batches = len(data_loader)\n",
    "            #find the average loss for the epoch\n",
    "            epoch_loss = sum(loss_list[-num_batches:]) / num_batches\n",
    "            \n",
    "\n",
    "\n",
    "    return(model, loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded0a919",
   "metadata": {},
   "source": [
    "### data definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0705974f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4842\n",
      "709\n",
      "82\n"
     ]
    }
   ],
   "source": [
    "data_loader_model = get_dataloader(16, \"cpu\")\n",
    "# len(data_loader_model[0])\n",
    "# for i, thing in enumerate(data_loader_model[0]):\n",
    "    # print(thing.keys())\n",
    "    # if i > 0:\n",
    "        # break\n",
    "\n",
    "train_set = PM25_Dataset(mode=\"train\")\n",
    "valid_set = PM25_Dataset(mode=\"valid\")\n",
    "test_set  = PM25_Dataset(mode=\"test\")\n",
    "\n",
    "print(len(train_set))\n",
    "print(len(valid_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df55c691",
   "metadata": {},
   "source": [
    "### model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfce4e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"model\":{\n",
    "        \"is_unconditional\": 0,\n",
    "        \"timeemb\": 128,\n",
    "        \"featureemb\": 16,\n",
    "        \"target_strategy\": \"random\"},\n",
    "    \"diffusion\":{\n",
    "        \"layers\": 4,\n",
    "        \"channels\": 64,\n",
    "        \"nheads\": 8,\n",
    "        \"diffusion_embedding_dim\": 128,\n",
    "        \"beta_start\": 0.0001,\n",
    "        \"beta_end\": 0.5,\n",
    "        \"num_steps\": 50,\n",
    "        \"schedule\": \"quad\"}\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83bfbc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CSDI_PM25(config, \"cpu\").to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf59a3f",
   "metadata": {},
   "source": [
    "### model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20c0bead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(self, predicted_noise, noise):\n",
    "\n",
    "    residual = noise - predicted_noise\n",
    "    num_obs = torch.sum(noise!=0)\n",
    "    loss = (residual**2).sum() / num_obs\n",
    "    return(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe022794",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, data_loader_model[0], epochs = 200, lr = 0.001, loss_func = loss_func, device = \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9379730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
