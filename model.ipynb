{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZITCIZpOYp5"
   },
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SxCOTIObffZt"
   },
   "outputs": [],
   "source": [
    "import enum\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBa4rhJuOcT3"
   },
   "source": [
    "## collecting and preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BHMPXtp6Ps2a"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import sys\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JK4V0wpsSj9M"
   },
   "source": [
    "### downloading the data (should only need to do this once)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZL6MB0nObuC"
   },
   "source": [
    "os.makedirs(\"data/\", exist_ok=True)\n",
    "url = \"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/STMVL-Release.zip\"\n",
    "urlData = requests.get(url).content\n",
    "filename = \"data/STMVL-Release.zip\"\n",
    "with open(filename, mode=\"wb\") as f:\n",
    "    # f.write(urlData)\n",
    "with zipfile.ZipFile(filename) as z:\n",
    "    z.extractall(\"data/pm25\")\n",
    "        \n",
    "def create_normalizer_pm25():\n",
    "    df = pd.read_csv(\n",
    "        \"./data/pm25/Code/STMVL/SampleData/pm25_ground.txt\",\n",
    "        index_col=\"datetime\",\n",
    "        parse_dates=True,\n",
    "    )\n",
    "    test_month = [3, 6, 9, 12]\n",
    "    for i in test_month:\n",
    "        df = df[df.index.month != i]\n",
    "    mean = df.describe().loc[\"mean\"].values\n",
    "    std = df.describe().loc[\"std\"].values\n",
    "    path = \"./data/pm25/pm25_meanstd.pk\"\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump([mean, std], f)\n",
    "create_normalizer_pm25()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wxzBn8fdRTal"
   },
   "outputs": [],
   "source": [
    "class PM25_Dataset(Dataset):\n",
    "    def __init__(self, eval_length=36, target_dim=36, mode=\"train\", validindex=0):\n",
    "        self.eval_length = eval_length\n",
    "        self.target_dim = target_dim\n",
    "\n",
    "        path = \"./data/pm25/pm25_meanstd.pk\"\n",
    "        with open(path, \"rb\") as f:\n",
    "            self.train_mean, self.train_std = pickle.load(f)\n",
    "        if mode == \"train\":\n",
    "            month_list = [1, 2, 4, 5, 7, 8, 10, 11]\n",
    "            # 1st,4th,7th,10th months are excluded from histmask (since the months are used for creating missing patterns in test dataset)\n",
    "            flag_for_histmask = [0, 1, 0, 1, 0, 1, 0, 1] \n",
    "            month_list.pop(validindex)\n",
    "            flag_for_histmask.pop(validindex)\n",
    "        elif mode == \"valid\":\n",
    "            month_list = [1, 2, 4, 5, 7, 8, 10, 11]\n",
    "            month_list = month_list[validindex : validindex + 1]\n",
    "        elif mode == \"test\":\n",
    "            month_list = [3, 6, 9, 12]\n",
    "        self.month_list = month_list\n",
    "\n",
    "        # create data for batch\n",
    "        self.observed_data = []  # values (separated into each month)\n",
    "        self.observed_mask = []  # masks (separated into each month)\n",
    "        self.gt_mask = []  # ground-truth masks (separated into each month)\n",
    "        self.index_month = []  # indicate month\n",
    "        self.position_in_month = []  # indicate the start position in month (length is the same as index_month)\n",
    "        self.valid_for_histmask = []  # whether the sample is used for histmask\n",
    "        self.use_index = []  # to separate train/valid/test\n",
    "        self.cut_length = []  # excluded from evaluation targets\n",
    "\n",
    "        df = pd.read_csv(\n",
    "            \"./data/pm25/Code/STMVL/SampleData/pm25_ground.txt\",\n",
    "            index_col=\"datetime\",\n",
    "            parse_dates=True,\n",
    "        )\n",
    "        df_gt = pd.read_csv(\n",
    "            \"./data/pm25/Code/STMVL/SampleData/pm25_missing.txt\",\n",
    "            index_col=\"datetime\",\n",
    "            parse_dates=True,\n",
    "        )\n",
    "        for i in range(len(month_list)):\n",
    "            current_df = df[df.index.month == month_list[i]]\n",
    "            current_df_gt = df_gt[df_gt.index.month == month_list[i]]\n",
    "            current_length = len(current_df) - eval_length + 1\n",
    "\n",
    "            last_index = len(self.index_month)\n",
    "            self.index_month += np.array([i] * current_length).tolist()\n",
    "            self.position_in_month += np.arange(current_length).tolist()\n",
    "            if mode == \"train\":\n",
    "                self.valid_for_histmask += np.array(\n",
    "                    [flag_for_histmask[i]] * current_length\n",
    "                ).tolist()\n",
    "\n",
    "            # mask values for observed indices are 1\n",
    "            c_mask = 1 - current_df.isnull().values\n",
    "            c_gt_mask = 1 - current_df_gt.isnull().values\n",
    "            c_data = (\n",
    "                (current_df.fillna(0).values - self.train_mean) / self.train_std\n",
    "            ) * c_mask\n",
    "\n",
    "            self.observed_mask.append(c_mask)\n",
    "            self.gt_mask.append(c_gt_mask)\n",
    "            self.observed_data.append(c_data)\n",
    "\n",
    "            if mode == \"test\":\n",
    "                n_sample = len(current_df) // eval_length\n",
    "                # interval size is eval_length (missing values are imputed only once)\n",
    "                c_index = np.arange(\n",
    "                    last_index, last_index + eval_length * n_sample, eval_length\n",
    "                )\n",
    "                self.use_index += c_index.tolist()\n",
    "                self.cut_length += [0] * len(c_index)\n",
    "                if len(current_df) % eval_length != 0:  # avoid double-count for the last time-series\n",
    "                    self.use_index += [len(self.index_month) - 1]\n",
    "                    self.cut_length += [eval_length - len(current_df) % eval_length]\n",
    "\n",
    "        if mode != \"test\":\n",
    "            self.use_index = np.arange(len(self.index_month))\n",
    "            self.cut_length = [0] * len(self.use_index)\n",
    "\n",
    "        # masks for 1st,4th,7th,10th months are used for creating missing patterns in test data,\n",
    "        # so these months are excluded from histmask to avoid leakage\n",
    "        if mode == \"train\":\n",
    "            ind = -1\n",
    "            self.index_month_histmask = []\n",
    "            self.position_in_month_histmask = []\n",
    "\n",
    "            for i in range(len(self.index_month)):\n",
    "                while True:\n",
    "                    ind += 1\n",
    "                    if ind == len(self.index_month):\n",
    "                        ind = 0\n",
    "                    if self.valid_for_histmask[ind] == 1:\n",
    "                        self.index_month_histmask.append(self.index_month[ind])\n",
    "                        self.position_in_month_histmask.append(\n",
    "                            self.position_in_month[ind]\n",
    "                        )\n",
    "                        break\n",
    "        else:  # dummy (histmask is only used for training)\n",
    "            self.index_month_histmask = self.index_month\n",
    "            self.position_in_month_histmask = self.position_in_month\n",
    "\n",
    "    def __getitem__(self, org_index):\n",
    "        index = self.use_index[org_index]\n",
    "        c_month = self.index_month[index]\n",
    "        c_index = self.position_in_month[index]\n",
    "        hist_month = self.index_month_histmask[index]\n",
    "        hist_index = self.position_in_month_histmask[index]\n",
    "        s = {\n",
    "            \"observed_data\": self.observed_data[c_month][\n",
    "                c_index : c_index + self.eval_length\n",
    "            ],\n",
    "            \"observed_mask\": self.observed_mask[c_month][\n",
    "                c_index : c_index + self.eval_length\n",
    "            ],\n",
    "            \"gt_mask\": self.gt_mask[c_month][\n",
    "                c_index : c_index + self.eval_length\n",
    "            ],\n",
    "            \"hist_mask\": self.observed_mask[hist_month][\n",
    "                hist_index : hist_index + self.eval_length\n",
    "            ],\n",
    "            \"timepoints\": np.arange(self.eval_length),\n",
    "            \"cut_length\": self.cut_length[org_index],\n",
    "        }\n",
    "\n",
    "        return s\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.use_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_dataloader(batch_size, device, validindex=0):\n",
    "    dataset = PM25_Dataset(mode=\"train\", validindex=validindex)\n",
    "    train_loader = DataLoader(\n",
    "        dataset, batch_size=batch_size, num_workers=1, shuffle=True\n",
    "    )\n",
    "    dataset_test = PM25_Dataset(mode=\"test\", validindex=validindex)\n",
    "    test_loader = DataLoader(\n",
    "        dataset_test, batch_size=batch_size, num_workers=1, shuffle=False\n",
    "    )\n",
    "    dataset_valid = PM25_Dataset(mode=\"valid\", validindex=validindex)\n",
    "    valid_loader = DataLoader(\n",
    "        dataset_valid, batch_size=batch_size, num_workers=1, shuffle=False\n",
    "    )\n",
    "\n",
    "    scaler = torch.from_numpy(dataset.train_std).to(device).float()\n",
    "    mean_scaler = torch.from_numpy(dataset.train_mean).to(device).float()\n",
    "\n",
    "    return train_loader, valid_loader, test_loader, scaler, mean_scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stuff = get_dataloader(18, \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(stuff[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i, thing in enumerate(stuff[0]):\n",
    "    print(thing.keys())\n",
    "    if i > 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = PM25_Dataset(mode=\"train\")\n",
    "valid_set = PM25_Dataset(mode=\"valid\")\n",
    "test_set  = PM25_Dataset(mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4842\n",
      "709\n",
      "82\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set))\n",
    "print(len(valid_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8f6F5pw1EW4C"
   },
   "source": [
    "## Moded Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4qEDMxV6EXvm"
   },
   "outputs": [],
   "source": [
    "class TimesSeriesAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A module that computes multi-head attention given query, key, and value tensors for time series data of shape (b, t, f, e)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, num_heads: int):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "        \n",
    "        Inputs:\n",
    "        - input_dim: Dimension of the input query, key, and value. We assume they all have\n",
    "          the same dimensions. This is basically the dimension of the embedding.\n",
    "        - num_heads: Number of attention heads\n",
    "        \"\"\"\n",
    "        super(TimesSeriesAttention, self).__init__()\n",
    "\n",
    "        assert embed_dim % num_heads == 0\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_per_head = embed_dim // num_heads\n",
    "\n",
    "\n",
    "        self.linear_query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.linear_key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.linear_value = nn.Linear(embed_dim, embed_dim)\n",
    "        self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "\n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        Compute the attended feature representations.\n",
    "        \n",
    "        Inputs:\n",
    "        - query: Tensor of the shape BxTxFXE, where B is the batch size, T is the time dimension, F is the feature dimension, \n",
    "        and E is the embedding dimension\n",
    "        - key: Tensor of the shape BxTxFXE\n",
    "        - value: Tensor of the shape BxTxFXE\n",
    "        - mask: Tensor indicating where the attention should *not* be performed\n",
    "        \"\"\"\n",
    "        b = query.shape[0]\n",
    "        t = query.shape[1]\n",
    "        f = query.shape[2]\n",
    "        e = query.shape[3]\n",
    "\n",
    "\n",
    "        query_linear = self.linear_query(query)\n",
    "        key_linear = self.linear_key(key)\n",
    "        value_linear = self.linear_value(value)\n",
    "\n",
    "        query_reshaped = query_linear.reshape(b, t, f, self.num_heads, self.dim_per_head)\n",
    "        key_reshaped = key_linear.reshape(b, t, f, self.num_heads, self.dim_per_head)\n",
    "        value_reshaped = value_linear.reshape(b, t, f, self.num_heads, self.dim_per_head)\n",
    "\n",
    "        query_reshaped = query_reshaped.permute(0, 3, 1, 2, 4) # BxHxTxFxE\n",
    "        key_reshaped = key_reshaped.permute(0, 3, 1, 2, 4) # BxHxTxFxE\n",
    "        value_reshaped = value_reshaped.permute(0, 3, 1, 2, 4) # BxHxTxFxE\n",
    "\n",
    "\n",
    "        kq = torch.einsum(\"bhtfe,bhxye->bhtfxy\", key_reshaped, query_reshaped)\n",
    "\n",
    "        dot_prod_scores = kq/math.sqrt(self.dim_per_head)\n",
    "\n",
    "        # if mask is not None:\n",
    "        #     # We simply set the similarity scores to be near zero for the positions\n",
    "        #     # where the attention should not be done. Think of why we do this.\n",
    "        #     dot_prod_scores = dot_prod_scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        #softmac across time and features\n",
    "        dot_prod_scores = F.softmax(dot_prod_scores, dim=4)\n",
    "        dot_prod_scores = F.softmax(dot_prod_scores, dim=5)\n",
    "\n",
    "        out = torch.einsum(\"bhtfxy,bhtfe->btfhe\",\n",
    "                           dot_prod_scores, value_reshaped)\n",
    "        out = self.output_linear(out.reshape(b, t, f, e))\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "96Aa8eM9EyOQ"
   },
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple feedforward network. Essentially, it is a two-layer fully-connected\n",
    "    neural network.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, ff_dim, dropout):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - embed_dim: embedding dimension\n",
    "        - ff_dim: Hidden dimension\n",
    "        \"\"\"\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        \n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "        - x: Tensor of the shape BxTxFXE, where B is the batch size, T is the time dimension, F is the feature dimension,\n",
    "        and E is the embedding dimension\n",
    "          \n",
    "        Return:\n",
    "        - y: Tensor of the shape BxTxFXE\n",
    "        \"\"\"\n",
    "\n",
    "        y = self.feedforward(x)\n",
    "\n",
    "        \n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "hf0Ign4KE96l"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoderCell(nn.Module):\n",
    "    \"\"\"\n",
    "    A single cell (unit) for the Transformer encoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, dropout: float):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - embed_dim: embedding dimension for each element in the time series data\n",
    "        - num_heads: Number of attention heads in a multi-head attention module\n",
    "        - ff_dim: The hidden dimension for a feedforward network\n",
    "        - dropout: Dropout ratio for the output of the multi-head attention and feedforward\n",
    "          modules.\n",
    "        \"\"\"\n",
    "        super(TransformerEncoderCell, self).__init__()\n",
    "\n",
    "        self.time_series_attention = TimesSeriesAttention(embed_dim, num_heads)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "        self.feedforward = FeedForwardNetwork(embed_dim, ff_dim, dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - x: Tensor of the shape BxTxFXE, where B is the batch size, T is the time dimension, F is the feature dimension,\n",
    "        and E is the embedding dimension\n",
    "        - mask: Tensor for multi-head attention\n",
    "        \"\"\"\n",
    "\n",
    "        attention = self.time_series_attention(x, x, x, mask)\n",
    "        attention = self.dropout(attention)\n",
    "        attention = torch.add(attention, x)\n",
    "        attention = self.layer_norm(attention)\n",
    "\n",
    "        y = self.feedforward(attention)\n",
    "        y = torch.add(y, attention)\n",
    "        y = self.layer_norm(y)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Qkb86MIuE_dj"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A full encoder consisting of a set of TransformerEncoderCell.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, num_cells: int, dropout: float=0.1):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - embed_dim: embedding dimension for each element in the time series data\n",
    "        - num_heads: Number of attention heads in a multi-head attention module\n",
    "        - ff_dim: The hidden dimension for a feedforward network\n",
    "        - num_cells: Number of time series attention cells in the encoder\n",
    "        - dropout: Dropout ratio for the output of the multi-head attention and feedforward\n",
    "          modules.\n",
    "        \"\"\"\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        \n",
    "        self.norm = None\n",
    "\n",
    "        self.encoder_modules = nn.ModuleList(TransformerEncoderCell(embed_dim, num_heads, ff_dim, dropout) for _ in range(num_cells))\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor=None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - x: Tensor of the shape BxTxFXE, where B is the batch size, T is the time dimension, F is the feature dimension,\n",
    "        and E is the embedding dimension\n",
    "        - mask: Tensor for multi-head attention\n",
    "        \n",
    "        Return:\n",
    "        - y: Tensor of the shape BxTxFXE\n",
    "        \"\"\"\n",
    "\n",
    "        for encoder_module in self.encoder_modules:\n",
    "            x = encoder_module(x, mask)\n",
    "          \n",
    "        #y = self.layer_norm(x)\n",
    "        y = x\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tA_zr9vuDHhw"
   },
   "source": [
    "## Beta Schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "FQ4oQrQvftyD"
   },
   "outputs": [],
   "source": [
    "def get_named_beta_schedule(schedule_name, num_diffusion_timesteps):\n",
    "    \"\"\"\n",
    "    Get a pre-defined beta schedule for the given name.\n",
    "\n",
    "    The beta schedule library consists of beta schedules which remain similar\n",
    "    in the limit of num_diffusion_timesteps.\n",
    "    Beta schedules may be added, but should not be removed or changed once\n",
    "    they are committed to maintain backwards compatibility.\n",
    "    \"\"\"\n",
    "    if schedule_name == \"linear\":\n",
    "        # Linear schedule from Ho et al, extended to work for any number of\n",
    "        # diffusion steps.\n",
    "        scale = 1000 / num_diffusion_timesteps\n",
    "        beta_start = scale * 0.0001\n",
    "        beta_end = scale * 0.02\n",
    "        return torch.linspace(\n",
    "            beta_start, beta_end, num_diffusion_timesteps\n",
    "        )\n",
    "    elif schedule_name == \"cosine\":\n",
    "        return betas_for_alpha_bar(\n",
    "            num_diffusion_timesteps,\n",
    "            lambda t: math.cos((t + 0.008) / 1.008 * math.pi / 2) ** 2,\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError(f\"unknown beta schedule: {schedule_name}\")\n",
    "\n",
    "\n",
    "def betas_for_alpha_bar(num_diffusion_timesteps, alpha_bar, max_beta=0.999):\n",
    "    \"\"\"\n",
    "    Create a beta schedule that discretizes the given alpha_t_bar function,\n",
    "    which defines the cumulative product of (1-beta) over time from t = [0,1].\n",
    "\n",
    "    :param num_diffusion_timesteps: the number of betas to produce.\n",
    "    :param alpha_bar: a lambda that takes an argument t from 0 to 1 and\n",
    "                      produces the cumulative product of (1-beta) up to that\n",
    "                      part of the diffusion process.\n",
    "    :param max_beta: the maximum beta to use; use values lower than 1 to\n",
    "                     prevent singularities.\n",
    "    \"\"\"\n",
    "    betas = []\n",
    "    for i in range(num_diffusion_timesteps):\n",
    "        t1 = i / num_diffusion_timesteps\n",
    "        t2 = (i + 1) / num_diffusion_timesteps\n",
    "        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n",
    "    return torch.tensor(betas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUTqjtgxDUD-"
   },
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "rru6kSdhfu0I"
   },
   "outputs": [],
   "source": [
    "class DiffusionEmbedding(nn.Module):\n",
    "    def __init__(self, num_steps, embedding_dim, projection_dim=None):\n",
    "        super(DiffusionEmbedding, self).__init__()\n",
    "        if projection_dim is None:\n",
    "            projection_dim = embedding_dim\n",
    "        self.register_buffer(\n",
    "            \"embedding\",\n",
    "            self._build_embedding(num_steps, embedding_dim / 2),\n",
    "            persistent=False,\n",
    "        )\n",
    "        self.projection1 = nn.Linear(embedding_dim, projection_dim)\n",
    "        self.projection2 = nn.Linear(projection_dim, embedding_dim)        \n",
    "\n",
    "    def forward(self, diffusion_step, data, device=\"cpu\"):\n",
    "        x = self.embedding[diffusion_step]\n",
    "        x = self.projection1(x)\n",
    "        x = F.silu(x)\n",
    "        x = self.projection2(x)\n",
    "        x = F.silu(x)\n",
    "        x = torch.zeros(data.shape).to(device) + x.unsqueeze(1).unsqueeze(1)\n",
    "        return x\n",
    "\n",
    "    def _build_embedding(self, num_steps, dim=64):\n",
    "        steps = torch.arange(num_steps).unsqueeze(1)  # (T,1)\n",
    "        frequencies = 10.0 ** (torch.arange(dim) / (dim - 1) * 4.0).unsqueeze(0)  # (1,dim)\n",
    "        table = steps * frequencies  # (T,dim)\n",
    "        table = torch.cat([torch.sin(table), torch.cos(table)], dim=1)  # (T,dim*2)\n",
    "        return table\n",
    "    \n",
    "\n",
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_len=10000.0):\n",
    "        super(TimeEmbedding, self).__init__()\n",
    "        self.max_len = max_len\n",
    "        self.learnable = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(embedding_dim, embedding_dim),\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, data, device=\"cpu\"):\n",
    "        b, l, f, e = data.shape\n",
    "        pe = torch.arange(l).unsqueeze(0).unsqueeze(-1).unsqueeze(-1).to(device)\n",
    "        pe = torch.zeros(data.shape).to(device) + pe\n",
    "        \n",
    "        div_term = 1 / torch.pow(\n",
    "            self.max_len, torch.arange(0, f, 2) / f\n",
    "        ).unsqueeze(-1).to(device)\n",
    "\n",
    "        pe[:, :, 0::2] = torch.sin(pe[:, :, 0::2] * div_term)\n",
    "        pe[:, :, 1::2] = torch.cos(pe[:, :, 1::2] * div_term)\n",
    "\n",
    "        return self.learnable(pe) \n",
    "    \n",
    "class FeatureEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_len=10000.0):\n",
    "        super(FeatureEmbedding, self).__init__()\n",
    "        self.max_len = max_len\n",
    "        self.learnable = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(embedding_dim, embedding_dim),\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, data, device=\"cpu\"):\n",
    "        b, l, f, e = data.shape\n",
    "        pe = torch.arange(f).unsqueeze(0).unsqueeze(0).unsqueeze(-1).to(device)\n",
    "        pe = torch.zeros(data.shape).to(device) + pe\n",
    "\n",
    "        div_term = 1 / torch.pow(\n",
    "            self.max_len, torch.arange(0, e, 2) / e\n",
    "        ).to(device)\n",
    "\n",
    "        pe[:, :, :, 0::2] = torch.sin(pe[:, :, :, 0::2] * div_term)\n",
    "        pe[:, :, :, 1::2] = torch.cos(pe[:, :, :, 1::2] * div_term)\n",
    "\n",
    "        return self.learnable(pe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_KtXfg0iDiQS"
   },
   "source": [
    "## Diffusion Imputation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "p_ENQFhEfvOj"
   },
   "outputs": [],
   "source": [
    "class diffusion_imputation(nn.Module):\n",
    "    def __init__(self, emb_dim,\n",
    "                vocab_size,\n",
    "                pad_idx= None,\n",
    "                features_to_impute = None,\n",
    "                missing_prp = 0.1,\n",
    "                diffusion_steps = 1000,\n",
    "                diffusion_beta_schedule = \"cosine\",\n",
    "                is_unconditional=False,\n",
    "                conv_out_channels = 4,\n",
    "                num_heads = 8,\n",
    "                ff_dim = 512,\n",
    "                num_cells = 2,\n",
    "                dropout = 0.1,\n",
    "                device = \"cpu\"):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.emb_dim = emb_dim\n",
    "        self.is_unconditional = is_unconditional\n",
    "        self.features_to_impute = features_to_impute\n",
    "        self.missing_prp = missing_prp\n",
    "        self.conv_out_channels = conv_out_channels\n",
    "        self.diffusion_steps = diffusion_steps\n",
    "\n",
    "        #set device to cuda if available\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = \"cuda\"        \n",
    "\n",
    "        self.data_embedding_linear = nn.Sequential(\n",
    "            nn.Linear(1, emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "        )        \n",
    "        self.diffusion_embedding = DiffusionEmbedding(diffusion_steps, emb_dim)\n",
    "        self.time_embedding = TimeEmbedding(emb_dim)\n",
    "        self.feature_embedding = FeatureEmbedding(emb_dim)\n",
    "        self.embedding_conv = nn.Conv1d(in_channels = 4, out_channels= conv_out_channels, kernel_size = 1)\n",
    "\n",
    "        #number of heads for the transformer should be divisivle by the conv_out_channels,\n",
    "        # so that each head gets input from a single channel\n",
    "        self.embed_dim_transformer_input = conv_out_channels * emb_dim\n",
    "\n",
    "        self.transformer = TransformerEncoder(embed_dim = self.embed_dim_transformer_input,\n",
    "                                              num_heads = num_heads,\n",
    "                                              ff_dim = ff_dim,\n",
    "                                              num_cells = num_cells,\n",
    "                                              dropout = dropout)\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(self.embed_dim_transformer_input, self.embed_dim_transformer_input),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(self.embed_dim_transformer_input, 1),\n",
    "        )\n",
    "        #define beta schedule\n",
    "        self.beta = get_named_beta_schedule(diffusion_beta_schedule, \n",
    "                                            diffusion_steps)\n",
    "        \n",
    "        self.alpha_hat = 1 - self.beta \n",
    "        self.alpha = torch.cumprod(self.alpha_hat, dim=0)\n",
    "        self.alpha_torch = torch.tensor(self.alpha).float()\n",
    "\n",
    "    def get_mask(self, data, strategy = \"forecasting\"):\n",
    "        \n",
    "        b = data.shape[0]\n",
    "        t = data.shape[1]\n",
    "        f = data.shape[2]\n",
    "        \n",
    "        if strategy == \"forecasting\":\n",
    "            forecasted_time = torch.randint(t-1, t, (b, 1, 1, 1))\n",
    "            mask = torch.zeros_like(data)\n",
    "            for i in range(b):\n",
    "                mask[i, forecasted_time[i]:, :, :] = 1\n",
    "        \n",
    "        if strategy == \"random_features\":\n",
    "            selected_features = torch.randint(0, f, (b, 1, 1, 1))\n",
    "            mask = torch.zeros_like(data)\n",
    "            mask[:, :, selected_features, :] = 1\n",
    "        \n",
    "        if strategy == \"selected_features\":\n",
    "            mask = torch.zeros_like(data)\n",
    "            mask[:, :, self.features_to_impute, :] = 1\n",
    "        \n",
    "        if strategy == \"selected_features_after_time\":\n",
    "            selected_time = torch.randint(1, t, (b, 1, 1))\n",
    "            mask = torch.zeros_like(data)\n",
    "            mask[:, selected_time:, self.features_to_impute, :] = 1\n",
    "        \n",
    "        if strategy == \"random\":\n",
    "            mask = torch.rand(size=(b, t, f))#.unsqueeze(3)\n",
    "            #mask = mask.repeat(1, 1, 1, e)\n",
    "            mask = mask < self.missing_prp\n",
    "            mask = mask.float()\n",
    "        return mask\n",
    "    \n",
    "    def loss_func(self, predicted_noise, noise):\n",
    "\n",
    "        residual = noise - predicted_noise\n",
    "        num_obs = torch.sum(noise!=0)\n",
    "        loss = (residual**2).sum() / num_obs\n",
    "        return(loss)\n",
    "    \n",
    "    def forward(self, data, strategy = \"forecasting\"):\n",
    "         \n",
    "        b, t, f = data.shape\n",
    "\n",
    "        noise_mask = self.get_mask(data, strategy).to(self.device)\n",
    "        noise = torch.randn((b, t, f)).to(self.device)\n",
    "        noise = (noise_mask * noise)\n",
    "\n",
    "        diffusion_t = torch.randint(1, self.diffusion_steps, (b,1)).squeeze(1)\n",
    "        alpha = self.alpha_torch[diffusion_t].unsqueeze(1).unsqueeze(2).to(self.device)\n",
    "        alpha_prev = self.alpha_torch[diffusion_t - 1].unsqueeze(1).unsqueeze(2).to(self.device)\n",
    "\n",
    "        noised_data = data * noise_mask\n",
    "        noised_data = noised_data * (alpha**0.5) + noise * (1 - alpha)\n",
    "        conditional_data = data * (1 - noise_mask)\n",
    "        noised_data = noised_data + conditional_data\n",
    "        noised_data = noised_data.unsqueeze(3)\n",
    "\n",
    "        noised_data = self.data_embedding_linear(noised_data.float())\n",
    "        diffusion_embedding = self.diffusion_embedding(diffusion_t, noised_data, device = self.device)\n",
    "        time_embedding = self.time_embedding(noised_data, device = self.device)\n",
    "        feature_embedding = self.feature_embedding(noised_data, device = self.device)\n",
    "\n",
    "        noised_data = torch.stack((noised_data, diffusion_embedding, time_embedding, feature_embedding), dim = -1)\n",
    "        noised_data = noised_data.reshape(1, -1, 4)\n",
    "        noised_data = noised_data.permute(0, 2, 1)\n",
    "        noised_data = self.embedding_conv(noised_data)\n",
    "        noised_data = noised_data.permute(0, 2, 1)\n",
    "        noised_data = noised_data.reshape(b, t, f, self.embed_dim_transformer_input)\n",
    "        \n",
    "        predicted_noise = self.transformer(noised_data)\n",
    "        predicted_noise = self.output(predicted_noise).squeeze(3)\n",
    "        predicted_noise = predicted_noise * noise_mask\n",
    "\n",
    "        return (predicted_noise, noise*((1-alpha)-(1 - alpha_prev)))\n",
    "    \n",
    "    def eval(self, data, imputation_mask):\n",
    "        \n",
    "        conditional_data = data * (1 - imputation_mask)\n",
    "        random_noise = torch.randn_like(data) * imputation_mask\n",
    "        data = (conditional_data + random_noise).unsqueeze(3)\n",
    "\n",
    "        b, ti, f, e = data.shape\n",
    "        imputed_samples = torch.zeros((b, ti, f)).to(self.device)\n",
    "        x = (conditional_data + random_noise)\n",
    "\n",
    "        for t in range(self.diffusion_steps - 1, -1, -1):\n",
    "\n",
    "            x = x.unsqueeze(3)\n",
    "            current_sample = self.data_embedding_linear(x.float())\n",
    "            diffusion_embedding = self.diffusion_embedding([t], current_sample, device = self.device)\n",
    "            time_embedding = self.time_embedding(current_sample, device = self.device)\n",
    "            feature_embedding = self.feature_embedding(current_sample, device = self.device)\n",
    "            data_to_transformer = torch.stack((current_sample, diffusion_embedding, time_embedding, feature_embedding), dim = -1)\n",
    "            data_to_transformer = data_to_transformer.reshape(1, -1, 4)\n",
    "            data_to_transformer = data_to_transformer.permute(0, 2, 1)\n",
    "            data_to_transformer = self.embedding_conv(data_to_transformer)\n",
    "            data_to_transformer = data_to_transformer.permute(0, 2, 1)\n",
    "            data_to_transformer = data_to_transformer.reshape(b, ti, f, self.embed_dim_transformer_input)\n",
    "\n",
    "            predicted_noise = self.transformer(data_to_transformer)\n",
    "            predicted_noise = self.output(predicted_noise).squeeze(3)\n",
    "            predicted_noise = predicted_noise * imputation_mask\n",
    "\n",
    "            coeff1 = 1 / self.alpha_hat[t] ** 0.5\n",
    "            coeff2 = (1 - self.alpha_hat[t]) / (1 - self.alpha[t]) ** 0.5\n",
    "            \n",
    "            x = x.squeeze(3)\n",
    "            x = coeff1 * (x - coeff2 * predicted_noise)\n",
    "            \n",
    "            if t > 0:\n",
    "                noise = torch.randn_like(x)\n",
    "                sigma = (\n",
    "                    (1.0 - self.alpha[t - 1]) / (1.0 - self.alpha[t]) * self.beta[t]\n",
    "                ) ** 0.5\n",
    "                x += sigma * noise\n",
    "            \n",
    "            x = data.squeeze(3) * (1 - imputation_mask) + x * imputation_mask\n",
    "\n",
    "        imputed_samples = x.detach()\n",
    "        return(imputed_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gkPeyhduNrkI"
   },
   "source": [
    "## using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "39xAaTihN2Xw",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(model, data, epochs, lr, loss_func, device = \"cpu\", verbose = True):\n",
    "    model = model.to(device)\n",
    "    data = data.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        predicted_noise, noise = model(data, strategy='random')\n",
    "        loss = loss_func(predicted_noise, noise)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        end = time.time()\n",
    "        if verbose:\n",
    "            #print every 10 epochs\n",
    "            if epoch % 10 == 0:\n",
    "                print(\"Epoch: \", epoch, \"Loss: \", loss.item(), \"Time: \", end - start)\n",
    "    return(model)\n",
    "\n",
    "#restart the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\18147\\AppData\\Local\\Temp/ipykernel_12972/44158967.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.alpha_torch = torch.tensor(self.alpha).float()\n"
     ]
    }
   ],
   "source": [
    "diffusion_imputer = diffusion_imputation(emb_dim = 128,\n",
    "                                        is_unconditional = False,\n",
    "                                        conv_out_channels = 1,\n",
    "                                        num_heads = 8,\n",
    "                                        ff_dim = 1024,\n",
    "                                        num_cells = 2,\n",
    "                                        dropout = 0.0,\n",
    "                                        device = \"cpu\",\n",
    "                                        vocab_size= 100,\n",
    "                                        pad_idx=None,\n",
    "                                        diffusion_steps= 1000,\n",
    "                                        missing_prp= 0.1,\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3148 2502 2010 ... 1952 3735 3683]\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(len(train_set))\n",
    "np.random.shuffle(indices)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "data = []\n",
    "for j in range(int(len(train_set) / batch_size)):\n",
    "    batch = []\n",
    "    for i in range(batch_size):\n",
    "        batch.append(torch.Tensor(train_set[indices[i + j*batch_size]][\"observed_data\"]))\n",
    "        \n",
    "    batch = torch.stack(batch, dim=0)\n",
    "    data.append(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-3.0163e-01, -5.6203e-01, -3.1724e-01,  ..., -2.7119e-01,\n",
      "          -6.8585e-01, -5.8594e-01],\n",
      "         [-2.3890e-01, -3.3416e-01, -5.2385e-01,  ..., -2.6111e-01,\n",
      "          -6.8585e-01, -5.3200e-01],\n",
      "         [-2.6399e-01, -1.2458e-02, -0.0000e+00,  ..., -2.7119e-01,\n",
      "          -6.1783e-01, -5.4099e-01],\n",
      "         ...,\n",
      "         [-3.8173e-02,  5.7733e-01,  4.1880e-01,  ..., -7.9566e-02,\n",
      "          -9.6356e-02, -3.5220e-01],\n",
      "         [ 1.6255e-01,  5.7733e-01,  3.9298e-01,  ...,  5.1546e-02,\n",
      "           2.2106e-01, -8.2497e-02],\n",
      "         [ 2.6292e-01,  5.3711e-01,  2.3802e-01,  ...,  5.1546e-02,\n",
      "           7.3689e-02, -1.3644e-01]],\n",
      "\n",
      "        [[-1.2599e-01,  9.4657e-04,  3.8006e-01,  ..., -3.7205e-01,\n",
      "          -1.4170e-01, -6.4517e-02],\n",
      "         [-5.3718e-04,  2.4222e-01,  3.4132e-01,  ..., -4.1239e-01,\n",
      "          -1.1903e-01, -2.3533e-01],\n",
      "         [ 3.6328e-01,  5.7733e-01, -1.2354e-01,  ..., -4.8299e-01,\n",
      "          -2.3239e-01, -2.8028e-01],\n",
      "         ...,\n",
      "         [-9.2890e-01, -9.7756e-01, -1.0533e+00,  ..., -9.1666e-01,\n",
      "          -8.6723e-01, -8.6464e-01],\n",
      "         [-9.9162e-01, -9.7756e-01, -1.0791e+00,  ..., -9.6709e-01,\n",
      "          -1.0146e+00, -9.8151e-01],\n",
      "         [-9.2890e-01, -1.0044e+00, -1.0275e+00,  ..., -9.6709e-01,\n",
      "          -1.0033e+00, -9.9949e-01]],\n",
      "\n",
      "        [[-1.0544e+00, -9.2394e-01, -9.6289e-01,  ..., -9.2675e-01,\n",
      "          -5.3847e-01, -0.0000e+00],\n",
      "         [-8.9126e-01, -8.5692e-01, -5.4967e-01,  ..., -9.2675e-01,\n",
      "          -6.5184e-01, -0.0000e+00],\n",
      "         [-9.7908e-01, -8.4352e-01, -6.4006e-01,  ..., -8.8641e-01,\n",
      "          -9.1257e-01, -0.0000e+00],\n",
      "         ...,\n",
      "         [ 1.2492e-01,  4.1159e-02,  5.5866e-03,  ..., -3.8213e-01,\n",
      "           1.7007e-02, -0.0000e+00],\n",
      "         [ 1.2008e-02,  4.1159e-02, -7.3264e-03,  ..., -4.0230e-01,\n",
      "          -2.8338e-02, -0.0000e+00],\n",
      "         [ 8.7281e-02,  1.0818e-01,  1.8500e-02,  ..., -3.0145e-01,\n",
      "          -1.7002e-02, -0.0000e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-9.5399e-01, -8.0330e-01, -8.7250e-01,  ..., -9.7718e-01,\n",
      "          -9.2391e-01, -9.8151e-01],\n",
      "         [-1.0042e+00, -7.7650e-01, -9.3706e-01,  ..., -8.7632e-01,\n",
      "          -9.5792e-01, -9.4555e-01],\n",
      "         [-8.5362e-01, -7.7650e-01, -9.1124e-01,  ..., -8.9649e-01,\n",
      "          -8.6723e-01, -9.2757e-01],\n",
      "         ...,\n",
      "         [-6.1526e-01, -3.2075e-01, -2.5267e-01,  ..., -6.8470e-01,\n",
      "          -5.0446e-01, -4.5109e-01],\n",
      "         [-5.2744e-01, -2.5373e-01, -2.2685e-01,  ..., -4.9307e-01,\n",
      "          -4.9313e-01, -3.7917e-01],\n",
      "         [-5.6508e-01, -3.4756e-01, -2.9141e-01,  ..., -4.6282e-01,\n",
      "          -3.0041e-01, -3.9715e-01]],\n",
      "\n",
      "        [[-5.7763e-01, -5.8884e-01, -4.4637e-01,  ...,  1.8266e-01,\n",
      "           4.7046e-01, -3.2523e-01],\n",
      "         [-4.8981e-01, -4.1458e-01, -3.8180e-01,  ...,  1.8266e-01,\n",
      "           3.6843e-01, -4.0614e-01],\n",
      "         [-4.1454e-01, -2.9395e-01, -3.4306e-01,  ...,  1.9274e-01,\n",
      "           3.0042e-01, -4.1513e-01],\n",
      "         ...,\n",
      "         [-4.8981e-01, -2.6714e-01, -3.3152e-02,  ..., -6.0401e-01,\n",
      "          -3.5709e-01, -3.7018e-01],\n",
      "         [-3.6435e-01, -1.4650e-01,  7.0152e-02,  ..., -4.9307e-01,\n",
      "          -3.9110e-01, -3.4321e-01],\n",
      "         [-2.8908e-01, -1.2458e-02,  1.8500e-02,  ..., -3.7205e-01,\n",
      "          -3.5709e-01, -3.2523e-01]],\n",
      "\n",
      "        [[ 1.5001e-01, -4.4139e-01, -3.6889e-01,  ..., -4.0230e-01,\n",
      "          -2.7774e-01, -4.7806e-01],\n",
      "         [-1.6363e-01, -4.2799e-01, -2.6559e-01,  ..., -3.7205e-01,\n",
      "          -3.0041e-01, -4.0614e-01],\n",
      "         [-2.0126e-01, -4.2799e-01, -3.0433e-01,  ..., -3.9222e-01,\n",
      "          -3.0041e-01, -3.7018e-01],\n",
      "         ...,\n",
      "         [-4.0199e-01, -7.9478e-02, -3.3015e-01,  ...,  9.1888e-02,\n",
      "          -2.8338e-02, -1.5863e-03],\n",
      "         [-3.0163e-01, -2.5373e-01, -7.3264e-03,  ...,  3.5411e-01,\n",
      "           2.8343e-02,  1.0630e-01],\n",
      "         [ 2.5037e-01,  2.5563e-01,  2.5093e-01,  ..., -0.0000e+00,\n",
      "           2.8908e-01,  3.4004e-01]]])\n",
      "tensor([[[-0.2264,  0.3092, -0.5238,  ...,  0.6264, -0.4818, -0.0000],\n",
      "         [-0.7909, -0.6559, -0.7175,  ...,  1.0500, -0.2211, -0.0000],\n",
      "         [-0.9164, -0.7765, -0.8079,  ...,  0.0717, -0.4478, -0.0000],\n",
      "         ...,\n",
      "         [-0.5776, -0.6827, -0.6659,  ..., -0.9469, -0.8559, -0.0000],\n",
      "         [-0.6654, -0.6290, -0.5755,  ..., -0.9268, -0.6292, -0.0000],\n",
      "         [-0.4898, -0.6022, -0.4851,  ..., -0.9066, -0.8672, -0.0000]],\n",
      "\n",
      "        [[ 0.6016,  0.3361,  0.1864,  ..., -0.1703, -0.0000, -0.5680],\n",
      "         [ 0.6267,  0.1886,  0.2897,  ..., -0.1804, -0.0000, -0.5410],\n",
      "         [ 0.6644,  0.2690,  0.1347,  ..., -0.1703, -0.0000, -0.5050],\n",
      "         ...,\n",
      "         [ 2.0318,  1.5022,  1.5939,  ...,  0.4045, -0.0000,  0.6367],\n",
      "         [ 1.7935,  1.4352,  1.5810,  ...,  0.4651, -0.0000,  0.5019],\n",
      "         [ 1.9816,  1.7301,  1.4389,  ...,  0.4852, -0.0000,  0.4659]],\n",
      "\n",
      "        [[ 2.8222,  3.0437,  3.9311,  ...,  2.8553,  0.6972,  2.3898],\n",
      "         [ 3.0856,  3.5262,  3.9828,  ...,  2.0687,  0.7652,  2.4078],\n",
      "         [ 3.3993,  3.0839,  3.0918,  ...,  2.1191,  0.9693,  2.6954],\n",
      "         ...,\n",
      "         [-0.0000, -0.0000, -0.3043,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [ 0.2755, -0.4280, -0.3947,  ..., -0.3014,  0.2324, -0.0555],\n",
      "         [ 0.5138, -0.4414, -0.3818,  ..., -0.0695,  0.3684,  0.1423]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.3131,  0.3092, -0.2010,  ...,  1.1408,  0.7879,  0.0524],\n",
      "         [ 0.2755,  0.4031, -0.2268,  ...,  0.7878,  0.6292,  0.2861],\n",
      "         [ 0.2378,  0.4701, -0.1235,  ...,  0.5861,  0.5952,  0.6367],\n",
      "         ...,\n",
      "         [-0.0000, -0.0000, -0.9887,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [-0.0000, -0.0000, -1.0404,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [-0.0000, -0.0000, -1.0920,  ..., -0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "        [[-0.2891, -0.0393, -0.3172,  ..., -0.8662, -0.4931, -0.6129],\n",
      "         [-0.3267, -0.1867, -0.3947,  ..., -0.5939, -0.5385, -0.6039],\n",
      "         [-0.4145, -0.3878, -0.4076,  ..., -0.6948, -0.4478, -0.4871],\n",
      "         ...,\n",
      "         [ 0.2629,  0.2824,  0.2768,  ...,  0.0213, -0.0283, -0.2263],\n",
      "         [ 0.3507,  0.3361,  0.3542,  ...,  0.1020,  0.0397, -0.2173],\n",
      "         [ 0.4887,  0.3361,  0.4834,  ...,  0.2835,  0.0964, -0.1724]],\n",
      "\n",
      "        [[-0.2138, -0.2939, -0.1365,  ..., -0.0000, -0.2777, -0.2623],\n",
      "         [-0.1511, -0.1331, -0.2268,  ..., -0.0000, -0.2097, -0.1544],\n",
      "         [-0.1385, -0.4816, -0.1752,  ..., -0.0000, -0.1644, -0.2353],\n",
      "         ...,\n",
      "         [-0.6654, -0.9105, -0.6917,  ..., -0.0000, -1.0146, -0.7388],\n",
      "         [-0.0000, -0.0000, -0.7434,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [-0.6529, -0.8167, -0.6272,  ..., -0.0000, -0.6745, -0.6039]]])\n"
     ]
    }
   ],
   "source": [
    "print(data[0])\n",
    "print(data[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Loss:  0.9905232191085815 Time:  32.146663427352905\n",
      "Epoch:  0 Loss:  2.9492194652557373 Time:  17.986151218414307\n",
      "Epoch:  0 Loss:  0.9818966388702393 Time:  18.569625854492188\n",
      "Epoch:  0 Loss:  2.116868019104004 Time:  20.665844440460205\n",
      "Epoch:  0 Loss:  0.997160792350769 Time:  21.173161268234253\n",
      "Epoch:  0 Loss:  1.5242390632629395 Time:  21.627450704574585\n",
      "Epoch:  0 Loss:  1.0162651538848877 Time:  21.581300497055054\n",
      "Epoch:  0 Loss:  1.085257649421692 Time:  21.579834699630737\n",
      "Epoch:  0 Loss:  1.514567255973816 Time:  21.821297883987427\n",
      "Epoch:  0 Loss:  1.0256640911102295 Time:  21.151304244995117\n",
      "Epoch:  0 Loss:  1.3373727798461914 Time:  22.237067699432373\n",
      "Epoch:  0 Loss:  0.9817448258399963 Time:  20.17267155647278\n",
      "Epoch:  0 Loss:  1.261452078819275 Time:  21.520265579223633\n",
      "Epoch:  0 Loss:  0.9915372729301453 Time:  23.009172677993774\n",
      "Epoch:  0 Loss:  1.231690764427185 Time:  21.02595090866089\n",
      "Epoch:  0 Loss:  0.9613065123558044 Time:  21.62278175354004\n",
      "Epoch:  0 Loss:  1.0910178422927856 Time:  21.583304166793823\n",
      "Epoch:  0 Loss:  1.1392910480499268 Time:  19.6767635345459\n",
      "Epoch:  0 Loss:  0.9495145082473755 Time:  21.12222194671631\n",
      "Epoch:  0 Loss:  1.1004716157913208 Time:  21.818635940551758\n",
      "Epoch:  0 Loss:  0.914369523525238 Time:  21.00720500946045\n",
      "Epoch:  0 Loss:  0.9303129315376282 Time:  22.373693704605103\n",
      "Epoch:  0 Loss:  1.2271617650985718 Time:  21.070318698883057\n",
      "Epoch:  0 Loss:  0.9392184615135193 Time:  21.58940577507019\n",
      "Epoch:  0 Loss:  1.025907039642334 Time:  21.291686534881592\n",
      "Epoch:  0 Loss:  0.8382854461669922 Time:  21.85900592803955\n",
      "Epoch:  0 Loss:  1.1998090744018555 Time:  19.803459644317627\n",
      "Epoch:  0 Loss:  0.9733392596244812 Time:  20.64750647544861\n",
      "Epoch:  0 Loss:  0.9609657526016235 Time:  21.245453119277954\n",
      "Epoch:  0 Loss:  0.8211897015571594 Time:  22.91939926147461\n",
      "Epoch:  0 Loss:  0.6563959717750549 Time:  21.69965410232544\n",
      "Epoch:  0 Loss:  0.9301666021347046 Time:  20.710102081298828\n",
      "Epoch:  0 Loss:  0.7050536274909973 Time:  20.89204454421997\n",
      "Epoch:  0 Loss:  0.9702883362770081 Time:  20.734683513641357\n",
      "Epoch:  0 Loss:  0.7684869766235352 Time:  21.077318906784058\n",
      "Epoch:  0 Loss:  0.8140199184417725 Time:  23.394341468811035\n",
      "Epoch:  0 Loss:  1.389766812324524 Time:  20.541104316711426\n",
      "Epoch:  0 Loss:  0.7038780450820923 Time:  20.504478931427002\n",
      "Epoch:  0 Loss:  1.1513252258300781 Time:  22.174736976623535\n",
      "Epoch:  0 Loss:  0.7876028418540955 Time:  20.35113525390625\n",
      "Epoch:  0 Loss:  1.1271822452545166 Time:  21.564780235290527\n",
      "Epoch:  0 Loss:  0.895684540271759 Time:  20.915334939956665\n",
      "Epoch:  0 Loss:  0.6432636976242065 Time:  20.91154932975769\n",
      "Epoch:  0 Loss:  1.127923607826233 Time:  21.368143796920776\n",
      "Epoch:  0 Loss:  0.7484801411628723 Time:  21.576173305511475\n",
      "Epoch:  0 Loss:  0.8385605216026306 Time:  21.374502897262573\n",
      "Epoch:  0 Loss:  0.8011125326156616 Time:  22.066916465759277\n",
      "Epoch:  0 Loss:  0.9059467315673828 Time:  22.533079862594604\n",
      "Epoch:  0 Loss:  0.9577092528343201 Time:  21.190735578536987\n",
      "Epoch:  0 Loss:  0.7990753054618835 Time:  21.085195541381836\n",
      "Epoch:  0 Loss:  0.7982003092765808 Time:  21.259862184524536\n",
      "Epoch:  0 Loss:  1.125227689743042 Time:  20.938525676727295\n",
      "Epoch:  0 Loss:  0.7807367444038391 Time:  21.151750564575195\n",
      "Epoch:  0 Loss:  0.8459814786911011 Time:  21.459279775619507\n",
      "Epoch:  0 Loss:  0.7432237267494202 Time:  20.834341049194336\n",
      "Epoch:  0 Loss:  0.9357224106788635 Time:  22.915148735046387\n",
      "Epoch:  0 Loss:  0.9035893082618713 Time:  22.661823749542236\n",
      "Epoch:  0 Loss:  0.7175207138061523 Time:  19.666189432144165\n",
      "Epoch:  0 Loss:  0.7352756261825562 Time:  21.00536298751831\n",
      "Epoch:  0 Loss:  1.1513410806655884 Time:  21.261130332946777\n",
      "Epoch:  0 Loss:  0.6552314758300781 Time:  20.639974117279053\n",
      "Epoch:  0 Loss:  0.9287756085395813 Time:  20.281697273254395\n",
      "Epoch:  0 Loss:  0.6654876470565796 Time:  20.66366219520569\n",
      "Epoch:  0 Loss:  0.766657829284668 Time:  21.775306701660156\n",
      "Epoch:  0 Loss:  0.7590678930282593 Time:  21.497283220291138\n",
      "Epoch:  0 Loss:  0.8132996559143066 Time:  20.72919487953186\n",
      "Epoch:  0 Loss:  0.7657986879348755 Time:  20.494524717330933\n",
      "Epoch:  0 Loss:  0.7596152424812317 Time:  21.068349361419678\n",
      "Epoch:  0 Loss:  0.7245690226554871 Time:  20.217302322387695\n",
      "Epoch:  0 Loss:  0.7455660104751587 Time:  21.195042848587036\n",
      "Epoch:  0 Loss:  0.6939764618873596 Time:  22.18433165550232\n",
      "Epoch:  0 Loss:  0.8583428859710693 Time:  20.4315128326416\n",
      "Epoch:  0 Loss:  0.8199788928031921 Time:  20.963701248168945\n",
      "Epoch:  0 Loss:  0.8323742747306824 Time:  20.433637619018555\n",
      "Epoch:  0 Loss:  0.7365898489952087 Time:  20.791335105895996\n",
      "Epoch:  0 Loss:  0.8006162047386169 Time:  21.927419424057007\n",
      "Epoch:  0 Loss:  0.7761023640632629 Time:  21.443225383758545\n",
      "Epoch:  0 Loss:  0.6423951387405396 Time:  21.52208399772644\n",
      "Epoch:  0 Loss:  0.5700361132621765 Time:  21.466947078704834\n",
      "Epoch:  0 Loss:  0.9188493490219116 Time:  21.038105010986328\n",
      "Epoch:  0 Loss:  0.7870014309883118 Time:  21.76904797554016\n",
      "Epoch:  0 Loss:  0.8269088864326477 Time:  20.93430805206299\n",
      "Epoch:  0 Loss:  0.7830318808555603 Time:  22.990394830703735\n",
      "Epoch:  0 Loss:  0.6731963753700256 Time:  21.5988552570343\n",
      "Epoch:  0 Loss:  0.7947264909744263 Time:  21.271061182022095\n",
      "Epoch:  0 Loss:  0.7695434093475342 Time:  20.932880401611328\n",
      "Epoch:  0 Loss:  0.7410143613815308 Time:  21.872859477996826\n",
      "Epoch:  0 Loss:  0.7930416464805603 Time:  21.896798849105835\n",
      "Epoch:  0 Loss:  0.5973575711250305 Time:  19.919862985610962\n",
      "Epoch:  0 Loss:  0.8412902355194092 Time:  21.45814085006714\n",
      "Epoch:  0 Loss:  0.7012295126914978 Time:  21.58556890487671\n",
      "Epoch:  0 Loss:  0.8545977473258972 Time:  21.119765043258667\n",
      "Epoch:  0 Loss:  0.8009148240089417 Time:  20.972751140594482\n",
      "Epoch:  0 Loss:  0.7181118726730347 Time:  19.652217388153076\n",
      "Epoch:  0 Loss:  0.7544277906417847 Time:  20.496488571166992\n",
      "Epoch:  0 Loss:  0.6851114630699158 Time:  21.103509187698364\n",
      "Epoch:  0 Loss:  0.756557285785675 Time:  21.19870686531067\n",
      "Epoch:  0 Loss:  0.7160387635231018 Time:  20.178181886672974\n",
      "Epoch:  0 Loss:  0.6186791062355042 Time:  21.061895847320557\n",
      "Epoch:  0 Loss:  0.7735491991043091 Time:  19.584341287612915\n",
      "Epoch:  0 Loss:  0.751258373260498 Time:  19.748945474624634\n",
      "Epoch:  0 Loss:  0.5910743474960327 Time:  20.898897409439087\n",
      "Epoch:  0 Loss:  0.7268993854522705 Time:  19.75460696220398\n",
      "Epoch:  0 Loss:  0.8216046690940857 Time:  21.12679958343506\n",
      "Epoch:  0 Loss:  0.8265117406845093 Time:  20.327669620513916\n",
      "Epoch:  0 Loss:  0.6723870038986206 Time:  20.117735624313354\n",
      "Epoch:  0 Loss:  0.8251437544822693 Time:  20.116617441177368\n",
      "Epoch:  0 Loss:  0.5305628776550293 Time:  21.783647537231445\n",
      "Epoch:  0 Loss:  0.5122058391571045 Time:  21.53642964363098\n",
      "Epoch:  0 Loss:  0.49704569578170776 Time:  22.19466233253479\n",
      "Epoch:  0 Loss:  0.6408741474151611 Time:  23.75635075569153\n",
      "Epoch:  0 Loss:  0.6901278495788574 Time:  22.463505506515503\n",
      "Epoch:  0 Loss:  0.7935558557510376 Time:  21.66409397125244\n",
      "Epoch:  0 Loss:  0.4850050210952759 Time:  21.558796167373657\n",
      "Epoch:  0 Loss:  0.776269793510437 Time:  21.45711898803711\n",
      "Epoch:  0 Loss:  0.6730044484138489 Time:  21.020006895065308\n",
      "Epoch:  0 Loss:  0.7538226842880249 Time:  20.434749841690063\n",
      "Epoch:  0 Loss:  0.7135409712791443 Time:  21.389410495758057\n",
      "Epoch:  0 Loss:  0.8113449215888977 Time:  20.513267755508423\n",
      "Epoch:  0 Loss:  0.7143800854682922 Time:  20.679025650024414\n",
      "Epoch:  0 Loss:  0.9114750623703003 Time:  21.622777223587036\n",
      "Epoch:  0 Loss:  0.7895124554634094 Time:  21.095786571502686\n",
      "Epoch:  0 Loss:  0.8055620789527893 Time:  21.326692819595337\n",
      "Epoch:  0 Loss:  0.7549399733543396 Time:  20.81973099708557\n",
      "Epoch:  0 Loss:  0.9797616004943848 Time:  21.21859884262085\n",
      "Epoch:  0 Loss:  0.6128583550453186 Time:  21.855417490005493\n",
      "Epoch:  0 Loss:  0.6536126732826233 Time:  22.61503791809082\n",
      "Epoch:  0 Loss:  0.5786443948745728 Time:  21.858041286468506\n",
      "Epoch:  0 Loss:  0.606597900390625 Time:  20.98597764968872\n",
      "Epoch:  0 Loss:  0.5126780867576599 Time:  22.26645040512085\n",
      "Epoch:  0 Loss:  0.8539891242980957 Time:  21.597017288208008\n",
      "Epoch:  0 Loss:  0.5783913731575012 Time:  20.613370418548584\n",
      "Epoch:  0 Loss:  0.6949125528335571 Time:  21.37775444984436\n",
      "Epoch:  0 Loss:  0.6059869527816772 Time:  20.05437994003296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Loss:  0.5810464024543762 Time:  20.70523476600647\n",
      "Epoch:  0 Loss:  0.5428428649902344 Time:  21.6849365234375\n",
      "Epoch:  0 Loss:  0.8868204951286316 Time:  23.189069271087646\n",
      "Epoch:  0 Loss:  0.5617613792419434 Time:  21.11746120452881\n",
      "Epoch:  0 Loss:  0.6050630211830139 Time:  20.459795713424683\n",
      "Epoch:  0 Loss:  0.6934335827827454 Time:  21.69450545310974\n",
      "Epoch:  0 Loss:  0.708590567111969 Time:  23.080452919006348\n",
      "Epoch:  0 Loss:  0.6736850738525391 Time:  20.18697690963745\n",
      "Epoch:  0 Loss:  0.654220700263977 Time:  21.711844205856323\n",
      "Epoch:  0 Loss:  0.6853005886077881 Time:  21.510510683059692\n",
      "Epoch:  0 Loss:  0.7764245271682739 Time:  21.813430786132812\n",
      "Epoch:  0 Loss:  0.7353265285491943 Time:  21.567088842391968\n",
      "Epoch:  0 Loss:  0.7281514406204224 Time:  21.913143634796143\n",
      "Epoch:  0 Loss:  0.5520310997962952 Time:  21.282390594482422\n",
      "Epoch:  0 Loss:  0.7319174408912659 Time:  21.001721143722534\n",
      "Epoch:  0 Loss:  0.7111045718193054 Time:  21.473987579345703\n",
      "Epoch:  0 Loss:  1.0513126850128174 Time:  21.24641728401184\n",
      "Epoch:  0 Loss:  0.7146728038787842 Time:  21.25131320953369\n",
      "Epoch:  0 Loss:  0.7300636768341064 Time:  20.662688493728638\n",
      "Epoch:  0 Loss:  0.6704510450363159 Time:  21.8429958820343\n",
      "Epoch:  0 Loss:  0.8482616543769836 Time:  20.6583833694458\n",
      "Epoch:  0 Loss:  0.6525530219078064 Time:  21.315884113311768\n",
      "Epoch:  0 Loss:  0.6957982778549194 Time:  21.40831995010376\n",
      "Epoch:  0 Loss:  0.8419097065925598 Time:  21.89613389968872\n",
      "Epoch:  0 Loss:  0.6830587387084961 Time:  21.450654983520508\n",
      "Epoch:  0 Loss:  0.6117455363273621 Time:  21.46128821372986\n",
      "Epoch:  0 Loss:  0.6826668977737427 Time:  20.815669536590576\n",
      "Epoch:  0 Loss:  0.5785899758338928 Time:  20.60956883430481\n",
      "Epoch:  0 Loss:  0.6568926572799683 Time:  22.156200647354126\n",
      "Epoch:  0 Loss:  0.8261845111846924 Time:  21.692153215408325\n",
      "Epoch:  0 Loss:  0.6753666996955872 Time:  21.200966119766235\n",
      "Epoch:  0 Loss:  0.9280381798744202 Time:  23.657679319381714\n",
      "Epoch:  0 Loss:  0.8143966197967529 Time:  22.32923126220703\n",
      "Epoch:  0 Loss:  0.6735003590583801 Time:  22.358845233917236\n",
      "Epoch:  0 Loss:  0.6833183169364929 Time:  22.19447422027588\n",
      "Epoch:  0 Loss:  0.6373683214187622 Time:  21.62006688117981\n",
      "Epoch:  0 Loss:  0.7238126397132874 Time:  22.702600240707397\n",
      "Epoch:  0 Loss:  0.6673399806022644 Time:  21.765859603881836\n",
      "Epoch:  0 Loss:  0.6969789862632751 Time:  20.872766971588135\n",
      "Epoch:  0 Loss:  0.6042479276657104 Time:  21.170647382736206\n",
      "Epoch:  0 Loss:  0.6262131929397583 Time:  20.747102975845337\n",
      "Epoch:  0 Loss:  0.7526038885116577 Time:  20.105898141860962\n",
      "Epoch:  0 Loss:  0.6694260835647583 Time:  21.02805495262146\n",
      "Epoch:  0 Loss:  0.8590194582939148 Time:  19.99776816368103\n",
      "Epoch:  0 Loss:  0.7018442749977112 Time:  21.13568663597107\n",
      "Epoch:  0 Loss:  0.6619979739189148 Time:  20.90586757659912\n",
      "Epoch:  0 Loss:  0.8810621500015259 Time:  20.688511848449707\n",
      "Epoch:  0 Loss:  0.6732174158096313 Time:  21.558515310287476\n",
      "Epoch:  0 Loss:  0.8924381732940674 Time:  21.00775933265686\n",
      "Epoch:  0 Loss:  0.7588509917259216 Time:  19.94799542427063\n",
      "Epoch:  0 Loss:  0.616814136505127 Time:  20.297159671783447\n",
      "Epoch:  0 Loss:  0.6241769194602966 Time:  20.976556539535522\n",
      "Epoch:  0 Loss:  0.6054772734642029 Time:  19.505537271499634\n",
      "Epoch:  0 Loss:  0.5679658651351929 Time:  20.765483856201172\n",
      "Epoch:  0 Loss:  0.7018699049949646 Time:  22.157812356948853\n",
      "Epoch:  0 Loss:  0.823523759841919 Time:  22.13854146003723\n",
      "Epoch:  0 Loss:  0.7000179886817932 Time:  20.163981199264526\n",
      "Epoch:  0 Loss:  0.8415563702583313 Time:  21.075246572494507\n",
      "Epoch:  0 Loss:  0.7086886167526245 Time:  21.922489881515503\n",
      "Epoch:  0 Loss:  0.6232395768165588 Time:  20.653440237045288\n",
      "Epoch:  0 Loss:  0.6765684485435486 Time:  20.69820785522461\n",
      "Epoch:  0 Loss:  0.7189269065856934 Time:  21.21821427345276\n",
      "Epoch:  0 Loss:  0.6416750550270081 Time:  21.312644243240356\n",
      "Epoch:  0 Loss:  0.6757211685180664 Time:  21.184770822525024\n",
      "Epoch:  0 Loss:  0.5822461843490601 Time:  21.234519004821777\n",
      "Epoch:  0 Loss:  0.9107191562652588 Time:  20.91687822341919\n",
      "Epoch:  0 Loss:  0.5165660381317139 Time:  21.4238383769989\n",
      "Epoch:  0 Loss:  0.6985520720481873 Time:  21.70925211906433\n",
      "Epoch:  0 Loss:  0.6796074509620667 Time:  21.56174349784851\n",
      "Epoch:  0 Loss:  0.6472116112709045 Time:  21.59171748161316\n",
      "Epoch:  0 Loss:  0.7503286600112915 Time:  21.276955366134644\n",
      "Epoch:  0 Loss:  0.5359456539154053 Time:  21.080336570739746\n",
      "Epoch:  0 Loss:  0.6822019219398499 Time:  22.236229419708252\n",
      "Epoch:  0 Loss:  0.9513728022575378 Time:  21.02762770652771\n",
      "Epoch:  0 Loss:  0.8554478883743286 Time:  20.637691736221313\n",
      "Epoch:  0 Loss:  0.6683497428894043 Time:  20.494919776916504\n",
      "Epoch:  0 Loss:  0.7504916787147522 Time:  20.817275762557983\n",
      "Epoch:  0 Loss:  0.8297355771064758 Time:  20.762397289276123\n",
      "Epoch:  0 Loss:  0.6257369518280029 Time:  20.4435133934021\n",
      "Epoch:  0 Loss:  0.5217708349227905 Time:  21.265372037887573\n",
      "Epoch:  0 Loss:  0.7108796238899231 Time:  20.88623809814453\n",
      "Epoch:  0 Loss:  0.7017568945884705 Time:  21.721178770065308\n",
      "Epoch:  0 Loss:  0.7054635286331177 Time:  21.943671703338623\n",
      "Epoch:  0 Loss:  0.62543123960495 Time:  21.78160572052002\n",
      "Epoch:  0 Loss:  0.8506942987442017 Time:  20.86998677253723\n",
      "Epoch:  0 Loss:  0.8302279114723206 Time:  21.341511249542236\n",
      "Epoch:  0 Loss:  0.7461907267570496 Time:  21.48486351966858\n",
      "Epoch:  0 Loss:  0.7847360372543335 Time:  23.074750423431396\n",
      "Epoch:  0 Loss:  0.6632971167564392 Time:  21.95820116996765\n",
      "Epoch:  0 Loss:  0.8972951173782349 Time:  21.15193510055542\n",
      "Epoch:  0 Loss:  0.6283898949623108 Time:  21.579675436019897\n",
      "Epoch:  0 Loss:  0.3993986248970032 Time:  20.632189989089966\n",
      "Epoch:  0 Loss:  0.7389304637908936 Time:  21.299720525741577\n",
      "Epoch:  0 Loss:  0.6311468482017517 Time:  20.810771942138672\n",
      "Epoch:  0 Loss:  0.6818289756774902 Time:  21.48350429534912\n",
      "Epoch:  0 Loss:  0.6662667989730835 Time:  20.889370679855347\n",
      "Epoch:  0 Loss:  0.7339882254600525 Time:  20.953396797180176\n",
      "Epoch:  0 Loss:  0.5918669104576111 Time:  20.969587564468384\n",
      "Epoch:  0 Loss:  0.827067494392395 Time:  20.950706958770752\n",
      "Epoch:  0 Loss:  0.5723688006401062 Time:  20.43141222000122\n",
      "Epoch:  0 Loss:  0.8255600929260254 Time:  20.88227605819702\n",
      "Epoch:  0 Loss:  0.6598942875862122 Time:  20.87041735649109\n",
      "Epoch:  0 Loss:  0.8060477375984192 Time:  20.590316772460938\n",
      "Epoch:  0 Loss:  0.7482740879058838 Time:  21.706146478652954\n",
      "Epoch:  0 Loss:  0.6796355247497559 Time:  20.471482038497925\n",
      "Epoch:  0 Loss:  0.6076014637947083 Time:  21.091931343078613\n",
      "Epoch:  0 Loss:  0.5504942536354065 Time:  21.084741592407227\n",
      "Epoch:  0 Loss:  0.5972862243652344 Time:  21.296812534332275\n",
      "Epoch:  0 Loss:  0.8951706886291504 Time:  20.828489065170288\n",
      "Epoch:  0 Loss:  0.717860758304596 Time:  21.41411828994751\n",
      "Epoch:  0 Loss:  0.5072677731513977 Time:  20.79438018798828\n",
      "Epoch:  0 Loss:  0.7607890367507935 Time:  19.99521517753601\n",
      "Epoch:  0 Loss:  0.7277842164039612 Time:  20.68321990966797\n",
      "Epoch:  0 Loss:  0.5243593454360962 Time:  19.775254487991333\n",
      "Epoch:  0 Loss:  0.6981930732727051 Time:  20.449615001678467\n",
      "Epoch:  0 Loss:  0.5841603875160217 Time:  20.355782508850098\n",
      "Epoch:  0 Loss:  0.6489852070808411 Time:  20.66711401939392\n",
      "Epoch:  0 Loss:  0.7537760138511658 Time:  21.095842599868774\n",
      "Epoch:  0 Loss:  0.7360268235206604 Time:  20.44885802268982\n",
      "Epoch:  0 Loss:  0.5545897483825684 Time:  20.918015718460083\n",
      "Epoch:  0 Loss:  0.714934766292572 Time:  21.82952570915222\n",
      "Epoch:  0 Loss:  0.6694496870040894 Time:  20.795515298843384\n",
      "Epoch:  0 Loss:  0.6214402318000793 Time:  20.52658176422119\n",
      "Epoch:  0 Loss:  0.5340327620506287 Time:  21.15692949295044\n",
      "Epoch:  0 Loss:  0.7311521172523499 Time:  19.99644923210144\n",
      "Epoch:  0 Loss:  0.5378138422966003 Time:  20.36787462234497\n",
      "Epoch:  0 Loss:  0.5309274792671204 Time:  20.44972848892212\n",
      "Epoch:  0 Loss:  0.5655441284179688 Time:  20.795893669128418\n",
      "Epoch:  0 Loss:  0.6820844411849976 Time:  19.94583797454834\n",
      "Epoch:  0 Loss:  0.43406012654304504 Time:  20.37251305580139\n",
      "Epoch:  0 Loss:  0.647277295589447 Time:  20.485545873641968\n",
      "Epoch:  0 Loss:  0.6587528586387634 Time:  19.94452452659607\n",
      "Epoch:  0 Loss:  0.5814360976219177 Time:  20.36844825744629\n",
      "Epoch:  0 Loss:  0.647057831287384 Time:  20.355133056640625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Loss:  0.8101319074630737 Time:  22.08026385307312\n",
      "Epoch:  0 Loss:  0.598782479763031 Time:  21.481080532073975\n",
      "Epoch:  0 Loss:  0.9547820091247559 Time:  20.32007646560669\n",
      "Epoch:  0 Loss:  0.8376452922821045 Time:  19.69811701774597\n",
      "Epoch:  0 Loss:  0.35714957118034363 Time:  22.180818557739258\n",
      "Epoch:  0 Loss:  0.4686408042907715 Time:  21.34193253517151\n",
      "Epoch:  0 Loss:  0.59559565782547 Time:  21.34326481819153\n",
      "Epoch:  0 Loss:  0.6982118487358093 Time:  20.527160167694092\n",
      "Epoch:  0 Loss:  0.5266467332839966 Time:  21.45438003540039\n",
      "Epoch:  0 Loss:  0.8020787835121155 Time:  20.23964262008667\n",
      "Epoch:  0 Loss:  0.794892430305481 Time:  21.45268440246582\n",
      "Epoch:  0 Loss:  0.6264616847038269 Time:  20.60355567932129\n",
      "Epoch:  0 Loss:  0.6701277494430542 Time:  20.64292025566101\n",
      "Epoch:  0 Loss:  0.5721197128295898 Time:  19.91779375076294\n",
      "Epoch:  0 Loss:  0.625248908996582 Time:  20.615432739257812\n",
      "Epoch:  0 Loss:  0.6471133232116699 Time:  20.397961378097534\n",
      "Epoch:  0 Loss:  0.5750055313110352 Time:  19.837819814682007\n",
      "Epoch:  0 Loss:  0.7214893102645874 Time:  19.91575312614441\n",
      "Epoch:  0 Loss:  0.6306936144828796 Time:  20.17391085624695\n",
      "Epoch:  0 Loss:  0.6893956661224365 Time:  19.068753719329834\n",
      "Epoch:  0 Loss:  0.6723148226737976 Time:  18.753857374191284\n",
      "Epoch:  0 Loss:  0.5541378259658813 Time:  20.133055210113525\n",
      "Epoch:  0 Loss:  0.5512472987174988 Time:  20.321884632110596\n",
      "Epoch:  0 Loss:  0.6943508386611938 Time:  20.966744661331177\n",
      "Epoch:  0 Loss:  0.5733522772789001 Time:  20.17992854118347\n",
      "Epoch:  0 Loss:  0.5753559470176697 Time:  20.39674997329712\n",
      "Epoch:  0 Loss:  0.48619845509529114 Time:  20.997648000717163\n",
      "Epoch:  0 Loss:  0.617763102054596 Time:  21.007494926452637\n",
      "Epoch:  0 Loss:  0.8494874835014343 Time:  21.23391580581665\n",
      "Epoch:  0 Loss:  0.6192726492881775 Time:  21.257861852645874\n",
      "Epoch:  0 Loss:  0.6684268116950989 Time:  22.107750177383423\n",
      "Epoch:  0 Loss:  0.6787883043289185 Time:  21.450266122817993\n",
      "Epoch:  0 Loss:  0.6766440272331238 Time:  20.742897272109985\n",
      "Epoch:  0 Loss:  0.7573796510696411 Time:  19.60957670211792\n",
      "Epoch:  0 Loss:  0.7883185148239136 Time:  20.967679977416992\n",
      "Epoch:  0 Loss:  0.5271583795547485 Time:  20.32256507873535\n",
      "Epoch:  0 Loss:  0.6281188130378723 Time:  20.8659610748291\n",
      "Epoch:  0 Loss:  0.5175474882125854 Time:  21.216846227645874\n",
      "Epoch:  0 Loss:  0.5207766890525818 Time:  20.95427918434143\n",
      "Epoch:  0 Loss:  0.6644151210784912 Time:  21.337810039520264\n",
      "Epoch:  0 Loss:  0.5767017006874084 Time:  20.31904125213623\n",
      "Epoch:  0 Loss:  0.4223325252532959 Time:  20.432098388671875\n",
      "Epoch:  0 Loss:  0.3086811900138855 Time:  20.761719465255737\n",
      "Epoch:  0 Loss:  0.8040602207183838 Time:  20.588085412979126\n",
      "Epoch:  0 Loss:  0.6699073314666748 Time:  21.60680627822876\n",
      "Epoch:  0 Loss:  0.8085970282554626 Time:  20.192155122756958\n",
      "Epoch:  0 Loss:  0.602794885635376 Time:  21.704684734344482\n",
      "Epoch:  0 Loss:  0.5908758640289307 Time:  20.513792753219604\n",
      "Epoch:  0 Loss:  0.5821965336799622 Time:  19.79072117805481\n",
      "Epoch:  0 Loss:  0.5873329043388367 Time:  20.641475915908813\n",
      "Epoch:  0 Loss:  0.8556928634643555 Time:  20.26057481765747\n",
      "Epoch:  0 Loss:  0.638524055480957 Time:  20.04148006439209\n",
      "Epoch:  0 Loss:  0.5322071313858032 Time:  20.116231203079224\n",
      "Epoch:  0 Loss:  0.8044658899307251 Time:  21.2338125705719\n",
      "Epoch:  0 Loss:  0.6102026700973511 Time:  21.156211137771606\n",
      "Epoch:  0 Loss:  0.5153716802597046 Time:  19.9466073513031\n",
      "Epoch:  0 Loss:  0.6869412660598755 Time:  20.653616189956665\n",
      "Epoch:  0 Loss:  0.6519282460212708 Time:  20.8083758354187\n",
      "Epoch:  0 Loss:  0.7851821184158325 Time:  23.537365913391113\n",
      "Epoch:  0 Loss:  0.5992602109909058 Time:  20.711925506591797\n",
      "Epoch:  0 Loss:  0.7406934499740601 Time:  20.588012218475342\n",
      "Epoch:  0 Loss:  0.6377590894699097 Time:  20.887292861938477\n",
      "Epoch:  0 Loss:  0.6806842088699341 Time:  21.357856035232544\n",
      "Epoch:  0 Loss:  0.6003178358078003 Time:  19.727657079696655\n",
      "Epoch:  0 Loss:  0.6335635185241699 Time:  21.05986762046814\n",
      "Epoch:  0 Loss:  0.5751579999923706 Time:  20.938390493392944\n",
      "Epoch:  0 Loss:  0.611353874206543 Time:  19.463849782943726\n",
      "Epoch:  0 Loss:  0.7765863537788391 Time:  20.527648210525513\n",
      "Epoch:  0 Loss:  0.5460888147354126 Time:  21.60896110534668\n",
      "Epoch:  0 Loss:  0.694998025894165 Time:  21.246438026428223\n",
      "Epoch:  0 Loss:  0.5054539442062378 Time:  20.418169260025024\n",
      "Epoch:  0 Loss:  0.6003059148788452 Time:  19.993407011032104\n",
      "Epoch:  0 Loss:  0.7819810509681702 Time:  19.553432941436768\n",
      "Epoch:  0 Loss:  0.669212818145752 Time:  20.257946491241455\n",
      "Epoch:  0 Loss:  0.6139487028121948 Time:  20.584531545639038\n",
      "Epoch:  0 Loss:  0.6782857775688171 Time:  21.29654812812805\n",
      "Epoch:  0 Loss:  0.7413289546966553 Time:  20.389201164245605\n",
      "Epoch:  0 Loss:  0.8054826855659485 Time:  21.893199682235718\n",
      "Epoch:  0 Loss:  0.5630961656570435 Time:  20.51052236557007\n",
      "Epoch:  0 Loss:  0.8169719576835632 Time:  20.336975812911987\n",
      "Epoch:  0 Loss:  0.7467693090438843 Time:  19.648199796676636\n",
      "Epoch:  0 Loss:  0.5346676707267761 Time:  19.83496928215027\n",
      "Epoch:  0 Loss:  0.5284522175788879 Time:  19.630375385284424\n",
      "Epoch:  0 Loss:  0.6942905783653259 Time:  19.255686283111572\n",
      "Epoch:  0 Loss:  0.6887532472610474 Time:  20.60527491569519\n",
      "Epoch:  0 Loss:  0.5311883091926575 Time:  21.14597487449646\n",
      "Epoch:  0 Loss:  0.6931505799293518 Time:  22.032726049423218\n",
      "Epoch:  0 Loss:  0.5784100890159607 Time:  20.132956743240356\n",
      "Epoch:  0 Loss:  0.7101442813873291 Time:  20.542856216430664\n",
      "Epoch:  0 Loss:  0.5139733552932739 Time:  20.026846885681152\n",
      "Epoch:  0 Loss:  0.4591738283634186 Time:  19.80745840072632\n",
      "Epoch:  0 Loss:  0.7931440472602844 Time:  20.786980390548706\n",
      "Epoch:  0 Loss:  0.7602816224098206 Time:  21.079857110977173\n",
      "Epoch:  0 Loss:  0.5220215320587158 Time:  20.449227809906006\n",
      "Epoch:  0 Loss:  0.5587964653968811 Time:  21.433839797973633\n",
      "Epoch:  0 Loss:  0.6985273361206055 Time:  20.57999062538147\n",
      "Epoch:  0 Loss:  0.6507657170295715 Time:  20.730165243148804\n",
      "Epoch:  0 Loss:  0.4966175854206085 Time:  21.35702395439148\n",
      "Epoch:  0 Loss:  0.7449594140052795 Time:  20.48018479347229\n",
      "Epoch:  0 Loss:  0.6860350370407104 Time:  21.091195106506348\n",
      "Epoch:  0 Loss:  0.6321600079536438 Time:  19.91505002975464\n",
      "Epoch:  0 Loss:  0.8594889044761658 Time:  20.972864389419556\n",
      "Epoch:  0 Loss:  0.5551128387451172 Time:  21.344650506973267\n",
      "Epoch:  0 Loss:  0.6482617259025574 Time:  20.557121753692627\n",
      "Epoch:  0 Loss:  0.7133259773254395 Time:  21.132447957992554\n",
      "Epoch:  0 Loss:  0.5714604258537292 Time:  21.091503381729126\n",
      "Epoch:  0 Loss:  0.6177496314048767 Time:  20.211706399917603\n",
      "Epoch:  0 Loss:  0.6666746139526367 Time:  19.564825773239136\n",
      "Epoch:  0 Loss:  0.6668980121612549 Time:  20.890141487121582\n",
      "Epoch:  0 Loss:  0.6515456438064575 Time:  20.970935583114624\n",
      "Epoch:  0 Loss:  0.6641510128974915 Time:  21.863476514816284\n",
      "Epoch:  0 Loss:  0.599590003490448 Time:  22.064751386642456\n",
      "Epoch:  0 Loss:  0.6971730589866638 Time:  21.1793372631073\n",
      "Epoch:  0 Loss:  0.6412420272827148 Time:  21.146841049194336\n",
      "Epoch:  0 Loss:  0.6110541820526123 Time:  22.01691436767578\n",
      "Epoch:  0 Loss:  0.5160026550292969 Time:  21.390722274780273\n",
      "Epoch:  0 Loss:  0.4365698993206024 Time:  21.807517290115356\n",
      "Epoch:  0 Loss:  0.5981383919715881 Time:  21.345287322998047\n",
      "Epoch:  0 Loss:  0.5780372023582458 Time:  20.54193663597107\n",
      "Epoch:  0 Loss:  0.45903652906417847 Time:  21.435202598571777\n",
      "Epoch:  0 Loss:  0.6485180258750916 Time:  19.976168155670166\n",
      "Epoch:  0 Loss:  0.595956027507782 Time:  20.528990745544434\n",
      "Epoch:  0 Loss:  0.6504800319671631 Time:  20.166055917739868\n",
      "Epoch:  0 Loss:  0.45649269223213196 Time:  19.553563117980957\n",
      "Epoch:  0 Loss:  0.6407508254051208 Time:  21.4066264629364\n",
      "Epoch:  0 Loss:  0.7516787648200989 Time:  21.121312856674194\n",
      "Epoch:  0 Loss:  0.6581946015357971 Time:  20.825960397720337\n",
      "Epoch:  0 Loss:  0.6236989498138428 Time:  20.799256801605225\n",
      "Epoch:  0 Loss:  0.547931969165802 Time:  20.87175440788269\n",
      "Epoch:  0 Loss:  0.6786102652549744 Time:  20.77598738670349\n",
      "Epoch:  0 Loss:  0.8819687366485596 Time:  20.889975786209106\n",
      "Epoch:  0 Loss:  0.7303432822227478 Time:  21.03631067276001\n",
      "Epoch:  0 Loss:  0.6005352735519409 Time:  21.00040555000305\n",
      "Epoch:  0 Loss:  0.7395429015159607 Time:  21.107262134552002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Loss:  0.6834241151809692 Time:  20.619192123413086\n",
      "Epoch:  0 Loss:  0.7769303917884827 Time:  19.812748908996582\n",
      "Epoch:  0 Loss:  0.7429196834564209 Time:  20.860563278198242\n",
      "Epoch:  0 Loss:  0.7074443697929382 Time:  21.06129813194275\n",
      "Epoch:  0 Loss:  0.5913800001144409 Time:  20.903676986694336\n",
      "Epoch:  0 Loss:  0.5447485446929932 Time:  21.92831563949585\n",
      "Epoch:  0 Loss:  0.7315080761909485 Time:  22.661722660064697\n",
      "Epoch:  0 Loss:  0.7133957147598267 Time:  22.69278383255005\n",
      "Epoch:  0 Loss:  0.6897356510162354 Time:  21.329027891159058\n",
      "Epoch:  0 Loss:  0.5053575038909912 Time:  21.502568244934082\n",
      "Epoch:  0 Loss:  0.6547545790672302 Time:  21.186514854431152\n",
      "Epoch:  0 Loss:  0.5963215827941895 Time:  21.827432870864868\n",
      "Epoch:  0 Loss:  0.7061643004417419 Time:  20.887218475341797\n",
      "Epoch:  0 Loss:  0.7829762697219849 Time:  20.590057134628296\n",
      "Epoch:  0 Loss:  0.5343675017356873 Time:  21.371639013290405\n",
      "Epoch:  0 Loss:  0.627540647983551 Time:  19.998685121536255\n",
      "Epoch:  0 Loss:  0.7288885116577148 Time:  20.72838282585144\n",
      "Epoch:  0 Loss:  0.568390429019928 Time:  20.670092821121216\n",
      "Epoch:  0 Loss:  0.6130982041358948 Time:  20.7222797870636\n",
      "Epoch:  0 Loss:  0.7391961812973022 Time:  19.635200023651123\n",
      "Epoch:  0 Loss:  0.7531012892723083 Time:  21.134137868881226\n",
      "Epoch:  0 Loss:  0.7970321774482727 Time:  20.431267261505127\n",
      "Epoch:  0 Loss:  0.7275636196136475 Time:  20.541988134384155\n",
      "Epoch:  0 Loss:  0.8050472140312195 Time:  20.252700090408325\n",
      "Epoch:  0 Loss:  0.6913897395133972 Time:  20.76524567604065\n",
      "Epoch:  0 Loss:  0.5742000937461853 Time:  19.756845951080322\n",
      "Epoch:  0 Loss:  0.5360344052314758 Time:  20.883110761642456\n",
      "Epoch:  0 Loss:  0.69537752866745 Time:  21.147810220718384\n",
      "Epoch:  0 Loss:  0.6305464506149292 Time:  20.30761456489563\n",
      "Epoch:  0 Loss:  0.6340165138244629 Time:  21.344213247299194\n",
      "Epoch:  0 Loss:  0.5442388653755188 Time:  20.495189666748047\n",
      "Epoch:  0 Loss:  0.6768627166748047 Time:  20.66879653930664\n",
      "Epoch:  0 Loss:  0.6391084790229797 Time:  22.192166090011597\n",
      "Epoch:  0 Loss:  0.5024670958518982 Time:  21.574743270874023\n",
      "Epoch:  0 Loss:  0.7061110138893127 Time:  20.4629545211792\n",
      "Epoch:  0 Loss:  0.6582285761833191 Time:  21.2073233127594\n",
      "Epoch:  0 Loss:  0.8776119947433472 Time:  20.948789596557617\n",
      "Epoch:  0 Loss:  0.6512412428855896 Time:  20.71801781654358\n",
      "Epoch:  0 Loss:  0.4884793758392334 Time:  20.85942578315735\n",
      "Epoch:  0 Loss:  0.7154235243797302 Time:  21.046417236328125\n",
      "Epoch:  0 Loss:  0.6552010178565979 Time:  21.278456211090088\n",
      "Epoch:  0 Loss:  0.7798972725868225 Time:  22.426276683807373\n",
      "Epoch:  0 Loss:  0.654998779296875 Time:  20.245645761489868\n",
      "Epoch:  0 Loss:  0.5335371494293213 Time:  21.29482102394104\n",
      "Epoch:  0 Loss:  0.5554369688034058 Time:  20.943530559539795\n",
      "Epoch:  0 Loss:  0.840571939945221 Time:  21.64689540863037\n",
      "Epoch:  0 Loss:  0.7552168369293213 Time:  21.622278213500977\n",
      "Epoch:  0 Loss:  0.5225407481193542 Time:  20.781160593032837\n",
      "Epoch:  0 Loss:  0.7564898133277893 Time:  21.563018798828125\n",
      "Epoch:  0 Loss:  0.6574602127075195 Time:  20.72704577445984\n",
      "Epoch:  0 Loss:  0.6427748203277588 Time:  20.709041118621826\n",
      "Epoch:  0 Loss:  0.6176556348800659 Time:  21.34457564353943\n",
      "Epoch:  0 Loss:  0.6612406373023987 Time:  19.979029893875122\n",
      "Epoch:  0 Loss:  0.6966838836669922 Time:  21.781768798828125\n",
      "Epoch:  0 Loss:  0.8164660930633545 Time:  20.763228178024292\n",
      "Epoch:  0 Loss:  0.6971998810768127 Time:  21.266824960708618\n",
      "Epoch:  0 Loss:  0.7063039541244507 Time:  20.488195657730103\n",
      "Epoch:  0 Loss:  0.6906019449234009 Time:  21.186711311340332\n",
      "Epoch:  0 Loss:  0.7821793556213379 Time:  19.446200370788574\n",
      "Epoch:  0 Loss:  0.5097302198410034 Time:  20.340454816818237\n",
      "Epoch:  0 Loss:  0.7259533405303955 Time:  20.978264570236206\n",
      "Epoch:  0 Loss:  0.7060088515281677 Time:  21.12699007987976\n",
      "Epoch:  0 Loss:  0.35584378242492676 Time:  20.275303840637207\n",
      "Epoch:  0 Loss:  0.5948269367218018 Time:  20.715076446533203\n",
      "Epoch:  0 Loss:  1.0791245698928833 Time:  20.12261462211609\n",
      "Epoch:  0 Loss:  0.4430556893348694 Time:  21.468225955963135\n",
      "Epoch:  0 Loss:  0.6387394070625305 Time:  20.781360387802124\n",
      "Epoch:  0 Loss:  0.7576194405555725 Time:  20.843773365020752\n",
      "Epoch:  0 Loss:  0.5937585830688477 Time:  20.416508674621582\n",
      "Epoch:  0 Loss:  0.44998544454574585 Time:  21.187968969345093\n",
      "Epoch:  0 Loss:  0.8529163002967834 Time:  20.858166694641113\n",
      "Epoch:  0 Loss:  0.7827815413475037 Time:  19.835643768310547\n",
      "Epoch:  0 Loss:  0.6573023796081543 Time:  21.044955253601074\n",
      "Epoch:  0 Loss:  0.5378201603889465 Time:  20.195220470428467\n",
      "Epoch:  0 Loss:  0.45729467272758484 Time:  21.175063133239746\n",
      "Epoch:  0 Loss:  0.41565123200416565 Time:  20.462753534317017\n",
      "Epoch:  0 Loss:  0.7305235266685486 Time:  20.807623386383057\n",
      "Epoch:  0 Loss:  0.6970897912979126 Time:  21.277870893478394\n",
      "Epoch:  0 Loss:  0.5020993947982788 Time:  20.81028413772583\n",
      "Epoch:  0 Loss:  0.6097756028175354 Time:  21.388158082962036\n",
      "Epoch:  0 Loss:  0.7501118183135986 Time:  22.029277801513672\n",
      "Epoch:  0 Loss:  0.8116349577903748 Time:  20.448380947113037\n",
      "Epoch:  0 Loss:  0.6036326885223389 Time:  21.35887360572815\n",
      "Epoch:  0 Loss:  0.7794786095619202 Time:  21.04602885246277\n",
      "Epoch:  0 Loss:  0.6592689156532288 Time:  18.944130420684814\n",
      "Epoch:  0 Loss:  0.5470063090324402 Time:  20.40266227722168\n",
      "Epoch:  0 Loss:  0.5248841047286987 Time:  20.971104860305786\n",
      "Epoch:  0 Loss:  0.662056565284729 Time:  19.397910594940186\n",
      "Epoch:  0 Loss:  0.6158075928688049 Time:  21.185127019882202\n",
      "Epoch:  0 Loss:  0.7123254537582397 Time:  21.389480113983154\n",
      "Epoch:  0 Loss:  0.6021551489830017 Time:  19.994539499282837\n",
      "Epoch:  0 Loss:  0.7567470073699951 Time:  19.885993480682373\n",
      "Epoch:  0 Loss:  0.595227062702179 Time:  21.093574285507202\n",
      "Epoch:  0 Loss:  0.5821652412414551 Time:  21.256619930267334\n",
      "Epoch:  0 Loss:  0.6717028021812439 Time:  20.768331050872803\n",
      "Epoch:  0 Loss:  0.5969361662864685 Time:  21.440368175506592\n",
      "Epoch:  0 Loss:  0.49769777059555054 Time:  20.832129955291748\n",
      "Epoch:  0 Loss:  0.6543910503387451 Time:  19.69345450401306\n",
      "Epoch:  0 Loss:  0.5009770393371582 Time:  20.442463159561157\n",
      "Epoch:  0 Loss:  0.7025529742240906 Time:  21.296403408050537\n",
      "Epoch:  0 Loss:  0.5795982480049133 Time:  21.08413577079773\n",
      "Epoch:  0 Loss:  0.7532976865768433 Time:  21.091742992401123\n",
      "Epoch:  0 Loss:  0.7441549897193909 Time:  21.38633942604065\n",
      "Epoch:  0 Loss:  0.7445279359817505 Time:  21.45623207092285\n",
      "Epoch:  0 Loss:  0.5875740051269531 Time:  20.99723196029663\n",
      "Epoch:  0 Loss:  0.6339141726493835 Time:  21.04609227180481\n",
      "Epoch:  0 Loss:  0.5808244347572327 Time:  20.069721460342407\n",
      "Epoch:  0 Loss:  0.615700364112854 Time:  20.8890962600708\n",
      "Epoch:  0 Loss:  0.6216127276420593 Time:  20.648881196975708\n",
      "Epoch:  0 Loss:  0.49996450543403625 Time:  21.041353464126587\n",
      "Epoch:  0 Loss:  0.5948415398597717 Time:  21.09524154663086\n",
      "Epoch:  0 Loss:  0.5313024520874023 Time:  20.041825532913208\n",
      "Epoch:  0 Loss:  0.8118553161621094 Time:  20.82657241821289\n",
      "Epoch:  0 Loss:  0.4435514807701111 Time:  20.886268377304077\n",
      "Epoch:  0 Loss:  0.501687228679657 Time:  21.777183771133423\n",
      "Epoch:  0 Loss:  0.5527608394622803 Time:  20.763311862945557\n",
      "Epoch:  0 Loss:  0.6311965584754944 Time:  20.449816703796387\n",
      "Epoch:  0 Loss:  0.643432080745697 Time:  21.346983194351196\n",
      "Epoch:  0 Loss:  0.7350543737411499 Time:  20.357211351394653\n",
      "Epoch:  0 Loss:  0.9133225679397583 Time:  21.220366716384888\n",
      "Epoch:  0 Loss:  0.7940444350242615 Time:  20.857744216918945\n",
      "Epoch:  0 Loss:  0.47152096033096313 Time:  20.81245756149292\n",
      "Epoch:  0 Loss:  0.627086341381073 Time:  21.62267541885376\n",
      "Epoch:  0 Loss:  0.7813342213630676 Time:  20.8739914894104\n",
      "Epoch:  0 Loss:  0.4456229507923126 Time:  22.29723811149597\n",
      "Epoch:  0 Loss:  0.4055238366127014 Time:  21.17171859741211\n",
      "Epoch:  0 Loss:  0.6275623440742493 Time:  20.15882158279419\n",
      "Epoch:  0 Loss:  0.7001194953918457 Time:  20.159390449523926\n",
      "Epoch:  0 Loss:  0.594971776008606 Time:  19.48979377746582\n",
      "Epoch:  0 Loss:  0.791656494140625 Time:  20.372564554214478\n",
      "Epoch:  0 Loss:  0.6335461735725403 Time:  21.18239998817444\n",
      "Epoch:  0 Loss:  0.5422043800354004 Time:  20.088560581207275\n",
      "Epoch:  0 Loss:  0.7208898663520813 Time:  20.986468076705933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Loss:  0.4687875807285309 Time:  20.263583183288574\n",
      "Epoch:  0 Loss:  0.7440374493598938 Time:  20.15229320526123\n",
      "Epoch:  0 Loss:  0.6948707103729248 Time:  20.589704275131226\n",
      "Epoch:  0 Loss:  0.7085357308387756 Time:  20.728777408599854\n",
      "Epoch:  0 Loss:  0.6014183759689331 Time:  20.54012942314148\n",
      "Epoch:  0 Loss:  0.6141164898872375 Time:  20.565803289413452\n",
      "Epoch:  0 Loss:  0.6155883073806763 Time:  20.322304725646973\n",
      "Epoch:  0 Loss:  0.6572198271751404 Time:  20.560873985290527\n",
      "Epoch:  0 Loss:  0.534093976020813 Time:  21.30225133895874\n",
      "Epoch:  0 Loss:  0.7276625037193298 Time:  21.7876558303833\n",
      "Epoch:  0 Loss:  0.6522824764251709 Time:  20.948595762252808\n",
      "Epoch:  0 Loss:  0.6863168478012085 Time:  21.747814655303955\n",
      "Epoch:  0 Loss:  0.6285355091094971 Time:  20.24599814414978\n",
      "Epoch:  0 Loss:  0.6213638782501221 Time:  19.21228837966919\n",
      "Epoch:  0 Loss:  0.6188992857933044 Time:  18.735788583755493\n",
      "Epoch:  0 Loss:  0.5361508727073669 Time:  20.02568244934082\n",
      "Epoch:  0 Loss:  0.6628908514976501 Time:  19.770384788513184\n",
      "Epoch:  0 Loss:  0.5499405860900879 Time:  21.640877962112427\n",
      "Epoch:  0 Loss:  0.6694993376731873 Time:  20.071588277816772\n",
      "Epoch:  0 Loss:  0.6138290762901306 Time:  21.131985187530518\n",
      "Epoch:  0 Loss:  0.6698486804962158 Time:  20.088944911956787\n",
      "Epoch:  0 Loss:  0.6205418705940247 Time:  21.05825901031494\n",
      "Epoch:  0 Loss:  0.510987401008606 Time:  21.082650661468506\n",
      "Epoch:  0 Loss:  0.539753258228302 Time:  21.266701698303223\n",
      "Epoch:  0 Loss:  0.6679191589355469 Time:  20.888596057891846\n",
      "Epoch:  0 Loss:  0.6490672826766968 Time:  20.571464776992798\n",
      "Epoch:  0 Loss:  0.6925990581512451 Time:  21.439051628112793\n",
      "Epoch:  0 Loss:  0.6371870040893555 Time:  21.056068897247314\n",
      "Epoch:  0 Loss:  0.6780725121498108 Time:  21.484792709350586\n",
      "Epoch:  0 Loss:  0.8665022253990173 Time:  20.276190996170044\n",
      "Epoch:  0 Loss:  0.5604069232940674 Time:  20.617279529571533\n",
      "Epoch:  0 Loss:  0.7441433072090149 Time:  20.826720476150513\n",
      "Epoch:  0 Loss:  0.5090416669845581 Time:  20.308630228042603\n",
      "Epoch:  0 Loss:  0.5769446492195129 Time:  20.37364649772644\n",
      "Epoch:  0 Loss:  0.6244518160820007 Time:  20.920722723007202\n",
      "Epoch:  0 Loss:  0.6927525401115417 Time:  21.707334518432617\n",
      "Epoch:  0 Loss:  0.7682600021362305 Time:  22.245092630386353\n",
      "Epoch:  0 Loss:  0.6654221415519714 Time:  21.529047966003418\n",
      "Epoch:  0 Loss:  0.7736547589302063 Time:  19.990326404571533\n",
      "Epoch:  0 Loss:  0.592473030090332 Time:  19.694308042526245\n",
      "Epoch:  0 Loss:  0.6654620170593262 Time:  25.093266248703003\n",
      "Epoch:  0 Loss:  0.7779722213745117 Time:  22.157816171646118\n",
      "Epoch:  0 Loss:  0.5489903092384338 Time:  21.59659481048584\n",
      "Epoch:  0 Loss:  0.5588119029998779 Time:  27.52852725982666\n",
      "Epoch:  0 Loss:  0.8103508353233337 Time:  27.69988203048706\n",
      "Epoch:  0 Loss:  0.642659068107605 Time:  25.76538395881653\n",
      "Epoch:  0 Loss:  0.5808912515640259 Time:  21.438785791397095\n",
      "Epoch:  0 Loss:  0.7000261545181274 Time:  21.53003215789795\n",
      "Epoch:  0 Loss:  0.8997521996498108 Time:  21.198912858963013\n",
      "Epoch:  0 Loss:  0.6856269240379333 Time:  22.177570819854736\n",
      "Epoch:  0 Loss:  0.6349309682846069 Time:  22.804941177368164\n",
      "Epoch:  0 Loss:  0.4677116274833679 Time:  21.15638828277588\n",
      "Epoch:  0 Loss:  0.5335381031036377 Time:  21.119450569152832\n",
      "Epoch:  0 Loss:  0.6245720982551575 Time:  21.40791392326355\n",
      "Epoch:  0 Loss:  0.7500518560409546 Time:  21.108912467956543\n",
      "Epoch:  0 Loss:  0.6180734634399414 Time:  21.019282817840576\n",
      "Epoch:  0 Loss:  0.6561934947967529 Time:  20.591314554214478\n",
      "Epoch:  0 Loss:  0.6981287002563477 Time:  21.125073194503784\n",
      "Epoch:  0 Loss:  0.645331084728241 Time:  21.04763960838318\n",
      "Epoch:  0 Loss:  0.7071747183799744 Time:  20.48199725151062\n",
      "Epoch:  0 Loss:  0.7797406315803528 Time:  20.8383891582489\n",
      "Epoch:  0 Loss:  0.49859288334846497 Time:  22.314414024353027\n",
      "Epoch:  0 Loss:  0.5688982605934143 Time:  20.593419075012207\n",
      "Epoch:  0 Loss:  0.6361412405967712 Time:  21.848268747329712\n",
      "Epoch:  0 Loss:  0.48755353689193726 Time:  21.766595602035522\n",
      "Epoch:  0 Loss:  0.42832356691360474 Time:  21.379896640777588\n",
      "Epoch:  0 Loss:  0.5846118330955505 Time:  21.577807664871216\n",
      "Epoch:  0 Loss:  0.6631672978401184 Time:  21.516696453094482\n",
      "Epoch:  0 Loss:  0.6535857915878296 Time:  21.68896198272705\n",
      "Epoch:  0 Loss:  0.5361996293067932 Time:  20.965553998947144\n",
      "Epoch:  0 Loss:  0.30119913816452026 Time:  20.779057264328003\n",
      "Epoch:  0 Loss:  0.9374798536300659 Time:  21.624624729156494\n",
      "Epoch:  0 Loss:  0.5341613292694092 Time:  20.002509832382202\n",
      "Epoch:  0 Loss:  0.6723759770393372 Time:  20.523698091506958\n",
      "Epoch:  0 Loss:  0.7054787278175354 Time:  21.154196739196777\n",
      "Epoch:  0 Loss:  0.7072750329971313 Time:  19.735363960266113\n",
      "Epoch:  0 Loss:  0.5700032711029053 Time:  21.156272172927856\n",
      "Epoch:  0 Loss:  0.6597417593002319 Time:  21.668588399887085\n",
      "Epoch:  0 Loss:  0.7719033360481262 Time:  21.968034744262695\n",
      "Epoch:  0 Loss:  0.7606158256530762 Time:  20.703670978546143\n",
      "Epoch:  0 Loss:  0.614220380783081 Time:  21.483378648757935\n",
      "Epoch:  0 Loss:  0.694133460521698 Time:  20.857236862182617\n",
      "Epoch:  0 Loss:  0.6102232336997986 Time:  21.22035264968872\n",
      "Epoch:  0 Loss:  0.6064833998680115 Time:  21.014028072357178\n",
      "Epoch:  0 Loss:  0.7367960214614868 Time:  20.2909996509552\n",
      "Epoch:  0 Loss:  0.5597484111785889 Time:  20.81169295310974\n",
      "Epoch:  0 Loss:  0.6968643069267273 Time:  20.76555109024048\n",
      "Epoch:  0 Loss:  0.6755567193031311 Time:  20.88544201850891\n",
      "Epoch:  0 Loss:  0.715857982635498 Time:  19.630170583724976\n",
      "Epoch:  0 Loss:  0.676625669002533 Time:  21.66133165359497\n",
      "Epoch:  0 Loss:  0.6157884001731873 Time:  19.8659508228302\n",
      "Epoch:  0 Loss:  0.7087857127189636 Time:  22.000552892684937\n",
      "Epoch:  0 Loss:  0.6403039693832397 Time:  20.826284646987915\n",
      "Epoch:  0 Loss:  0.7854717969894409 Time:  20.600045680999756\n",
      "Epoch:  0 Loss:  0.512476921081543 Time:  20.775405168533325\n",
      "Epoch:  0 Loss:  0.6029128432273865 Time:  22.896782398223877\n",
      "Epoch:  0 Loss:  0.5544987320899963 Time:  20.93611478805542\n",
      "Epoch:  0 Loss:  0.666435956954956 Time:  19.5388240814209\n",
      "Epoch:  0 Loss:  0.6592154502868652 Time:  19.806294202804565\n",
      "Epoch:  0 Loss:  0.6849105954170227 Time:  21.379258632659912\n",
      "Epoch:  0 Loss:  0.46517834067344666 Time:  20.980604887008667\n",
      "Epoch:  0 Loss:  0.6853985786437988 Time:  22.03055477142334\n",
      "Epoch:  0 Loss:  0.7497032880783081 Time:  21.078768730163574\n",
      "Epoch:  0 Loss:  0.7197679281234741 Time:  22.160479068756104\n",
      "Epoch:  0 Loss:  0.6093094348907471 Time:  20.09527039527893\n",
      "Epoch:  0 Loss:  0.5926459431648254 Time:  20.525193452835083\n",
      "Epoch:  0 Loss:  0.5785487294197083 Time:  20.841137647628784\n",
      "Epoch:  0 Loss:  0.4938482940196991 Time:  20.736196041107178\n",
      "Epoch:  0 Loss:  0.4883356988430023 Time:  21.310949087142944\n",
      "Epoch:  0 Loss:  0.7307435274124146 Time:  20.77888774871826\n",
      "Epoch:  0 Loss:  0.5694544911384583 Time:  20.935070991516113\n",
      "Epoch:  0 Loss:  0.5710451602935791 Time:  20.6794114112854\n",
      "Epoch:  0 Loss:  0.6838106513023376 Time:  21.21815299987793\n",
      "Epoch:  0 Loss:  0.6581811904907227 Time:  20.721150875091553\n",
      "Epoch:  0 Loss:  0.7077746391296387 Time:  21.077765226364136\n",
      "Epoch:  0 Loss:  0.6810004711151123 Time:  20.85436177253723\n",
      "Epoch:  0 Loss:  0.5605307817459106 Time:  21.943161725997925\n",
      "Epoch:  0 Loss:  0.6792954206466675 Time:  19.93056893348694\n",
      "Epoch:  0 Loss:  0.5372457504272461 Time:  20.832268714904785\n",
      "Epoch:  0 Loss:  0.7174074649810791 Time:  20.990825653076172\n",
      "Epoch:  0 Loss:  0.4421981871128082 Time:  20.93935799598694\n",
      "Epoch:  0 Loss:  0.5022266507148743 Time:  19.465375423431396\n",
      "Epoch:  0 Loss:  0.6214781999588013 Time:  19.618860244750977\n",
      "Epoch:  0 Loss:  0.8400560021400452 Time:  20.78645372390747\n",
      "Epoch:  0 Loss:  0.6464490294456482 Time:  21.4359130859375\n",
      "Epoch:  0 Loss:  0.517979621887207 Time:  20.71630835533142\n",
      "Epoch:  0 Loss:  0.4611082375049591 Time:  21.56501340866089\n",
      "Epoch:  0 Loss:  0.6028440594673157 Time:  21.390910148620605\n",
      "Epoch:  0 Loss:  0.547555148601532 Time:  20.97624158859253\n",
      "Epoch:  0 Loss:  0.7685666084289551 Time:  20.864569187164307\n",
      "Epoch:  0 Loss:  0.6205931901931763 Time:  20.781521320343018\n",
      "Epoch:  0 Loss:  0.6473645567893982 Time:  19.712427616119385\n",
      "Epoch:  0 Loss:  0.47274622321128845 Time:  19.585406064987183\n",
      "Epoch:  0 Loss:  1.0294609069824219 Time:  21.407385110855103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Loss:  0.6157232522964478 Time:  20.670082092285156\n",
      "Epoch:  0 Loss:  0.6214390397071838 Time:  20.752856969833374\n",
      "Epoch:  0 Loss:  0.8000715374946594 Time:  20.528850317001343\n",
      "Epoch:  0 Loss:  0.6997711658477783 Time:  21.061527013778687\n",
      "Epoch:  0 Loss:  0.7144673466682434 Time:  21.02929949760437\n",
      "Epoch:  0 Loss:  0.48705342411994934 Time:  20.414997816085815\n",
      "Epoch:  0 Loss:  0.7035113573074341 Time:  21.301850080490112\n",
      "Epoch:  0 Loss:  0.8239322900772095 Time:  20.546032667160034\n",
      "Epoch:  0 Loss:  0.4572034478187561 Time:  21.376622200012207\n",
      "Epoch:  0 Loss:  0.6448628306388855 Time:  21.109833002090454\n",
      "Epoch:  0 Loss:  0.6715883612632751 Time:  21.927071571350098\n",
      "Epoch:  0 Loss:  0.757529079914093 Time:  20.889256477355957\n",
      "Epoch:  0 Loss:  0.6586288213729858 Time:  21.781731367111206\n",
      "Epoch:  0 Loss:  0.6199397444725037 Time:  20.291361331939697\n",
      "Epoch:  0 Loss:  0.5342100858688354 Time:  19.663460969924927\n",
      "Epoch:  0 Loss:  0.9381760954856873 Time:  19.664896965026855\n",
      "Epoch:  0 Loss:  0.6343742609024048 Time:  19.990561962127686\n",
      "Epoch:  0 Loss:  0.6223544478416443 Time:  20.246440172195435\n",
      "Epoch:  0 Loss:  0.5271530151367188 Time:  19.866901874542236\n",
      "Epoch:  0 Loss:  0.4821953773498535 Time:  20.870530605316162\n",
      "Epoch:  0 Loss:  0.7178348302841187 Time:  20.4774432182312\n",
      "Epoch:  0 Loss:  0.5211482644081116 Time:  20.309319257736206\n",
      "Epoch:  0 Loss:  0.6031243801116943 Time:  21.845202922821045\n",
      "Epoch:  0 Loss:  0.5413591265678406 Time:  21.350418090820312\n",
      "Epoch:  0 Loss:  0.5347428917884827 Time:  19.69339919090271\n",
      "Epoch:  0 Loss:  0.6504794955253601 Time:  20.69740867614746\n",
      "Epoch:  0 Loss:  0.4746023714542389 Time:  20.543879747390747\n",
      "Epoch:  0 Loss:  0.4555225074291229 Time:  21.147018432617188\n",
      "Epoch:  0 Loss:  0.6502004265785217 Time:  21.198868989944458\n",
      "Epoch:  0 Loss:  0.7693417072296143 Time:  20.73788857460022\n",
      "Epoch:  0 Loss:  0.7570823431015015 Time:  21.18862223625183\n",
      "Epoch:  0 Loss:  0.7280693650245667 Time:  20.562471628189087\n",
      "Epoch:  0 Loss:  0.6765822172164917 Time:  21.264516592025757\n",
      "Epoch:  0 Loss:  0.38270843029022217 Time:  20.670260906219482\n",
      "Epoch:  0 Loss:  0.48885127902030945 Time:  20.73308801651001\n",
      "Epoch:  0 Loss:  0.547959566116333 Time:  20.842403411865234\n",
      "Epoch:  0 Loss:  0.4607316255569458 Time:  21.76689386367798\n",
      "Epoch:  0 Loss:  0.8279273509979248 Time:  20.73210310935974\n",
      "Epoch:  0 Loss:  0.45591339468955994 Time:  21.05323886871338\n",
      "Epoch:  0 Loss:  0.5769193768501282 Time:  21.695652961730957\n",
      "Epoch:  0 Loss:  0.5044987201690674 Time:  21.626755952835083\n",
      "Epoch:  0 Loss:  0.6749333143234253 Time:  21.061336278915405\n",
      "Epoch:  0 Loss:  0.4068165421485901 Time:  20.76997423171997\n",
      "Epoch:  0 Loss:  0.4011857211589813 Time:  21.597727298736572\n",
      "Epoch:  0 Loss:  0.4529159963130951 Time:  21.392213344573975\n",
      "Epoch:  0 Loss:  0.6731898188591003 Time:  20.748623609542847\n",
      "Epoch:  0 Loss:  0.7252103090286255 Time:  20.43464708328247\n",
      "Epoch:  0 Loss:  0.8332236409187317 Time:  21.45499563217163\n",
      "Epoch:  0 Loss:  0.547275722026825 Time:  21.935079336166382\n",
      "Epoch:  0 Loss:  0.5181187987327576 Time:  20.591022968292236\n",
      "Epoch:  0 Loss:  0.666133463382721 Time:  21.125828742980957\n",
      "Epoch:  0 Loss:  0.554513692855835 Time:  20.903245210647583\n",
      "Epoch:  0 Loss:  0.7365027070045471 Time:  21.76542639732361\n",
      "Epoch:  0 Loss:  0.6841365694999695 Time:  20.341087341308594\n",
      "Epoch:  0 Loss:  0.759046196937561 Time:  21.013007640838623\n",
      "Epoch:  0 Loss:  0.5845643877983093 Time:  20.969715118408203\n",
      "Epoch:  0 Loss:  0.5850179195404053 Time:  21.329256296157837\n",
      "Epoch:  0 Loss:  0.5281246304512024 Time:  20.837410926818848\n",
      "Epoch:  0 Loss:  0.7561614513397217 Time:  20.414803981781006\n",
      "Epoch:  0 Loss:  0.4528006911277771 Time:  19.730865001678467\n",
      "Epoch:  0 Loss:  0.9295174479484558 Time:  20.463961601257324\n",
      "Epoch:  0 Loss:  0.5142279863357544 Time:  20.416858911514282\n",
      "Epoch:  0 Loss:  0.6807193160057068 Time:  20.00841212272644\n",
      "Epoch:  0 Loss:  0.6857683658599854 Time:  20.997668266296387\n",
      "Epoch:  0 Loss:  0.6346229314804077 Time:  22.571918487548828\n",
      "Epoch:  0 Loss:  0.7223087549209595 Time:  21.267040014266968\n",
      "Epoch:  0 Loss:  0.5705375671386719 Time:  20.93401527404785\n",
      "Epoch:  0 Loss:  0.49292829632759094 Time:  21.3914213180542\n",
      "Epoch:  0 Loss:  0.5124262571334839 Time:  20.026435136795044\n",
      "Epoch:  0 Loss:  0.38928520679473877 Time:  19.497084856033325\n",
      "Epoch:  0 Loss:  0.6028435230255127 Time:  21.09394907951355\n",
      "Epoch:  0 Loss:  0.4962385296821594 Time:  21.701968669891357\n",
      "Epoch:  0 Loss:  0.6268571615219116 Time:  20.005416870117188\n",
      "Epoch:  0 Loss:  0.4636322557926178 Time:  20.701145887374878\n",
      "Epoch:  0 Loss:  0.8188069462776184 Time:  20.71113133430481\n",
      "Epoch:  0 Loss:  0.5826793909072876 Time:  21.092111110687256\n",
      "Epoch:  0 Loss:  0.5649012923240662 Time:  21.085848569869995\n",
      "Epoch:  0 Loss:  0.5589026212692261 Time:  21.532944679260254\n",
      "Epoch:  0 Loss:  0.4217066466808319 Time:  20.26420569419861\n",
      "Epoch:  0 Loss:  0.7199699282646179 Time:  20.93824577331543\n",
      "Epoch:  0 Loss:  0.5614595413208008 Time:  21.269115209579468\n",
      "Epoch:  0 Loss:  0.5175519585609436 Time:  19.083180904388428\n",
      "Epoch:  0 Loss:  0.8102975487709045 Time:  20.509166955947876\n",
      "Epoch:  0 Loss:  0.8300555348396301 Time:  20.718793869018555\n",
      "Epoch:  0 Loss:  0.7691155672073364 Time:  20.25933051109314\n",
      "Epoch:  0 Loss:  0.7984840273857117 Time:  20.670255422592163\n",
      "Epoch:  0 Loss:  0.603239119052887 Time:  21.000892162322998\n",
      "Epoch:  0 Loss:  0.6342335939407349 Time:  21.059521913528442\n",
      "Epoch:  0 Loss:  0.6214178204536438 Time:  21.62470579147339\n",
      "Epoch:  0 Loss:  0.8552325367927551 Time:  21.07633662223816\n",
      "Epoch:  0 Loss:  0.5704717040061951 Time:  20.70089554786682\n",
      "Epoch:  0 Loss:  0.5880252122879028 Time:  21.740726709365845\n",
      "Epoch:  0 Loss:  0.514468789100647 Time:  21.781920909881592\n",
      "Epoch:  0 Loss:  0.7808594107627869 Time:  20.45348620414734\n",
      "Epoch:  0 Loss:  0.7603341937065125 Time:  20.820629358291626\n",
      "Epoch:  0 Loss:  0.5641980171203613 Time:  20.92179226875305\n",
      "Epoch:  0 Loss:  0.5859082937240601 Time:  21.58830237388611\n",
      "Epoch:  0 Loss:  0.5591347813606262 Time:  20.950308799743652\n",
      "Epoch:  0 Loss:  0.6425904631614685 Time:  21.549118757247925\n",
      "Epoch:  0 Loss:  0.6301091313362122 Time:  21.544748067855835\n",
      "Epoch:  0 Loss:  0.5740851163864136 Time:  21.016328811645508\n",
      "Epoch:  0 Loss:  0.5647866129875183 Time:  21.6704523563385\n",
      "Epoch:  0 Loss:  0.7457778453826904 Time:  20.91963243484497\n",
      "Epoch:  0 Loss:  0.6746935844421387 Time:  20.92028546333313\n",
      "Epoch:  0 Loss:  0.5088675022125244 Time:  21.260229349136353\n",
      "Epoch:  0 Loss:  0.49097055196762085 Time:  20.354790210723877\n",
      "Epoch:  0 Loss:  0.674260139465332 Time:  20.482157945632935\n",
      "Epoch:  0 Loss:  0.6814991235733032 Time:  20.387508869171143\n",
      "Epoch:  0 Loss:  0.8958842158317566 Time:  21.75062394142151\n",
      "Epoch:  0 Loss:  0.5985908508300781 Time:  20.906373739242554\n",
      "Epoch:  0 Loss:  0.5685116052627563 Time:  21.024229764938354\n",
      "Epoch:  0 Loss:  0.5895124673843384 Time:  20.90332269668579\n",
      "Epoch:  0 Loss:  0.6704962253570557 Time:  20.669854402542114\n",
      "Epoch:  0 Loss:  0.6627289652824402 Time:  19.88913583755493\n",
      "Epoch:  0 Loss:  0.5973362326622009 Time:  20.907185554504395\n",
      "Epoch:  0 Loss:  0.5140953660011292 Time:  21.519136428833008\n",
      "Epoch:  0 Loss:  0.664026141166687 Time:  20.247606992721558\n",
      "Epoch:  0 Loss:  0.6208081245422363 Time:  20.37395668029785\n",
      "Epoch:  0 Loss:  0.7525333762168884 Time:  20.74383306503296\n",
      "Epoch:  0 Loss:  0.6894353032112122 Time:  21.24630856513977\n",
      "Epoch:  0 Loss:  0.5109351873397827 Time:  21.016335487365723\n",
      "Epoch:  0 Loss:  0.522304356098175 Time:  20.842406034469604\n",
      "Epoch:  0 Loss:  0.6614461541175842 Time:  22.629851818084717\n",
      "Epoch:  0 Loss:  0.4665012061595917 Time:  21.407800674438477\n",
      "Epoch:  0 Loss:  0.7476063370704651 Time:  21.471670627593994\n",
      "Epoch:  0 Loss:  0.5673936605453491 Time:  21.265488147735596\n",
      "Epoch:  0 Loss:  0.7350291609764099 Time:  21.440897226333618\n",
      "Epoch:  0 Loss:  0.6057275533676147 Time:  21.87799048423767\n",
      "Epoch:  0 Loss:  0.6545153260231018 Time:  21.44195032119751\n",
      "Epoch:  0 Loss:  0.597026526927948 Time:  20.589412212371826\n",
      "Epoch:  0 Loss:  0.5828286409378052 Time:  21.071764945983887\n",
      "Epoch:  0 Loss:  0.691530168056488 Time:  21.499858379364014\n",
      "Epoch:  0 Loss:  0.6185060739517212 Time:  19.72894787788391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Loss:  0.5260967016220093 Time:  20.228395700454712\n",
      "Epoch:  0 Loss:  0.5535160303115845 Time:  20.197516202926636\n",
      "Epoch:  0 Loss:  0.551975667476654 Time:  19.589038372039795\n",
      "Epoch:  0 Loss:  0.8182209134101868 Time:  19.93853998184204\n",
      "Epoch:  0 Loss:  0.8116888403892517 Time:  20.419193744659424\n",
      "Epoch:  0 Loss:  0.6764140725135803 Time:  20.860447645187378\n",
      "Epoch:  0 Loss:  0.5786451101303101 Time:  20.57509160041809\n",
      "Epoch:  0 Loss:  0.5762582421302795 Time:  20.086223363876343\n",
      "Epoch:  0 Loss:  0.4810486435890198 Time:  21.153929710388184\n",
      "Epoch:  0 Loss:  0.9431651830673218 Time:  20.895312547683716\n",
      "Epoch:  0 Loss:  0.5315118432044983 Time:  20.337680339813232\n",
      "Epoch:  0 Loss:  0.555220365524292 Time:  20.196385860443115\n",
      "Epoch:  0 Loss:  0.5213050246238708 Time:  20.091079473495483\n",
      "Epoch:  0 Loss:  0.732254683971405 Time:  20.820432901382446\n",
      "Epoch:  0 Loss:  0.6394925117492676 Time:  20.526256799697876\n",
      "Epoch:  0 Loss:  0.6005788445472717 Time:  22.176804304122925\n",
      "Epoch:  0 Loss:  0.5616642832756042 Time:  21.01568078994751\n",
      "Epoch:  0 Loss:  0.7150131464004517 Time:  20.985232830047607\n",
      "Epoch:  0 Loss:  0.5353096127510071 Time:  21.989373207092285\n",
      "Epoch:  0 Loss:  0.7687417268753052 Time:  20.94638156890869\n",
      "Epoch:  0 Loss:  0.7520755529403687 Time:  21.31152892112732\n",
      "Epoch:  0 Loss:  0.5675845742225647 Time:  21.535260677337646\n",
      "Epoch:  0 Loss:  0.6895697116851807 Time:  20.16900086402893\n",
      "Epoch:  0 Loss:  0.6287978291511536 Time:  20.763957738876343\n",
      "Epoch:  0 Loss:  0.6872396469116211 Time:  20.966790437698364\n",
      "Epoch:  0 Loss:  0.7317769527435303 Time:  19.96170449256897\n",
      "Epoch:  0 Loss:  0.4326688051223755 Time:  21.56211018562317\n",
      "Epoch:  0 Loss:  0.7783340811729431 Time:  20.022114753723145\n",
      "Epoch:  0 Loss:  0.44604435563087463 Time:  21.124415636062622\n",
      "Epoch:  0 Loss:  0.8549619317054749 Time:  20.950603723526\n",
      "Epoch:  0 Loss:  0.7531757354736328 Time:  21.487929582595825\n",
      "Epoch:  0 Loss:  0.572143018245697 Time:  19.242974281311035\n",
      "Epoch:  0 Loss:  0.637151837348938 Time:  19.69120740890503\n",
      "Epoch:  0 Loss:  0.7405244708061218 Time:  20.057204484939575\n",
      "Epoch:  0 Loss:  0.607416033744812 Time:  20.776495695114136\n",
      "Epoch:  0 Loss:  0.6551716923713684 Time:  21.407766342163086\n",
      "Epoch:  0 Loss:  0.5990645885467529 Time:  20.92248296737671\n",
      "Epoch:  0 Loss:  0.5832342505455017 Time:  20.68588924407959\n",
      "Epoch:  0 Loss:  0.7142853140830994 Time:  21.23021173477173\n",
      "Epoch:  0 Loss:  0.6110489368438721 Time:  20.640755891799927\n",
      "Epoch:  0 Loss:  0.8200202584266663 Time:  21.68830180168152\n",
      "Epoch:  0 Loss:  0.6141775250434875 Time:  20.111492156982422\n",
      "Epoch:  0 Loss:  0.606236457824707 Time:  20.350733995437622\n",
      "Epoch:  0 Loss:  0.6529285311698914 Time:  21.319689512252808\n",
      "Epoch:  0 Loss:  0.6403933763504028 Time:  19.741802215576172\n",
      "Epoch:  0 Loss:  0.6674473881721497 Time:  20.920177459716797\n",
      "Epoch:  0 Loss:  0.5185917019844055 Time:  21.334794998168945\n",
      "Epoch:  0 Loss:  0.44845232367515564 Time:  21.02414584159851\n",
      "Epoch:  0 Loss:  0.6408687829971313 Time:  20.91406488418579\n",
      "Epoch:  0 Loss:  0.7841147184371948 Time:  19.994293212890625\n",
      "Epoch:  0 Loss:  0.8007214069366455 Time:  22.83374834060669\n",
      "Epoch:  0 Loss:  0.5693036913871765 Time:  20.746088981628418\n",
      "Epoch:  0 Loss:  0.6569978594779968 Time:  19.99206519126892\n",
      "Epoch:  0 Loss:  0.6934645771980286 Time:  23.198345184326172\n",
      "Epoch:  0 Loss:  0.6865306496620178 Time:  21.246220350265503\n",
      "Epoch:  0 Loss:  0.5841254591941833 Time:  22.096981287002563\n",
      "Epoch:  0 Loss:  0.6517428755760193 Time:  21.09493136405945\n",
      "Epoch:  0 Loss:  0.6638587117195129 Time:  21.76431918144226\n",
      "Epoch:  0 Loss:  0.6881871223449707 Time:  20.653695821762085\n",
      "Epoch:  0 Loss:  0.616626501083374 Time:  21.171854972839355\n",
      "Epoch:  0 Loss:  0.6770939826965332 Time:  21.361851453781128\n",
      "Epoch:  0 Loss:  0.7143682241439819 Time:  21.642396450042725\n",
      "Epoch:  0 Loss:  0.6291290521621704 Time:  20.493033170700073\n",
      "Epoch:  0 Loss:  0.7094996571540833 Time:  21.29665970802307\n",
      "Epoch:  0 Loss:  0.4494746923446655 Time:  21.18683624267578\n",
      "Epoch:  0 Loss:  0.5953975915908813 Time:  19.76000142097473\n",
      "Epoch:  0 Loss:  0.5140960812568665 Time:  22.537895917892456\n",
      "Epoch:  0 Loss:  0.5486144423484802 Time:  21.705589294433594\n",
      "Epoch:  0 Loss:  0.6580273509025574 Time:  21.107970714569092\n",
      "Epoch:  0 Loss:  0.8133590221405029 Time:  21.169588088989258\n",
      "Epoch:  0 Loss:  0.6811420321464539 Time:  20.883544206619263\n",
      "Epoch:  0 Loss:  0.58470618724823 Time:  20.889066457748413\n",
      "Epoch:  0 Loss:  0.704828143119812 Time:  21.030296802520752\n",
      "Epoch:  0 Loss:  0.6929172873497009 Time:  21.99311065673828\n",
      "Epoch:  0 Loss:  0.5236549377441406 Time:  21.485865831375122\n",
      "Epoch:  0 Loss:  0.6700436472892761 Time:  21.611968755722046\n",
      "Epoch:  0 Loss:  0.7254793047904968 Time:  20.905646085739136\n",
      "Epoch:  0 Loss:  0.8943823575973511 Time:  21.562735319137573\n",
      "Epoch:  0 Loss:  0.6287986040115356 Time:  21.42544674873352\n",
      "Epoch:  0 Loss:  0.7136245965957642 Time:  21.617435693740845\n",
      "Epoch:  0 Loss:  0.5199536681175232 Time:  21.371234893798828\n",
      "Epoch:  0 Loss:  0.6689742207527161 Time:  20.96214532852173\n",
      "Epoch:  0 Loss:  0.5400997400283813 Time:  20.030765533447266\n",
      "Epoch:  0 Loss:  0.7455965876579285 Time:  20.68514084815979\n",
      "Epoch:  0 Loss:  0.5125551819801331 Time:  20.182100534439087\n",
      "Epoch:  0 Loss:  0.5328686833381653 Time:  20.666773319244385\n",
      "Epoch:  0 Loss:  0.6782142519950867 Time:  20.746108531951904\n",
      "Epoch:  0 Loss:  0.6907060742378235 Time:  21.34196376800537\n",
      "Epoch:  0 Loss:  0.5430797338485718 Time:  20.645045042037964\n",
      "Epoch:  0 Loss:  0.5246948599815369 Time:  20.527093172073364\n",
      "Epoch:  0 Loss:  0.5300192832946777 Time:  20.072885036468506\n",
      "Epoch:  0 Loss:  0.6861540079116821 Time:  20.39734387397766\n",
      "Epoch:  0 Loss:  0.5186025500297546 Time:  20.97045111656189\n",
      "Epoch:  0 Loss:  0.6028804779052734 Time:  20.14879035949707\n",
      "Epoch:  0 Loss:  0.5074442028999329 Time:  22.12714695930481\n",
      "Epoch:  0 Loss:  0.31604233384132385 Time:  21.38986563682556\n",
      "Epoch:  0 Loss:  0.6597938537597656 Time:  21.231480598449707\n",
      "Epoch:  0 Loss:  0.563121497631073 Time:  21.34024214744568\n",
      "Epoch:  0 Loss:  0.6117874979972839 Time:  21.112674474716187\n",
      "Epoch:  0 Loss:  0.4722246527671814 Time:  20.850645780563354\n",
      "Epoch:  0 Loss:  0.4102286398410797 Time:  21.53157329559326\n",
      "Epoch:  0 Loss:  0.6168055534362793 Time:  21.953647136688232\n",
      "Epoch:  0 Loss:  0.9120250940322876 Time:  20.509483814239502\n",
      "Epoch:  0 Loss:  0.4308004677295685 Time:  21.15363883972168\n",
      "Epoch:  0 Loss:  0.7333570122718811 Time:  20.91903591156006\n",
      "Epoch:  0 Loss:  0.5285499691963196 Time:  20.60691213607788\n",
      "Epoch:  0 Loss:  0.8250005841255188 Time:  21.137991428375244\n",
      "Epoch:  0 Loss:  0.4963298738002777 Time:  20.888691902160645\n",
      "Epoch:  0 Loss:  0.5137030482292175 Time:  20.76239824295044\n",
      "Epoch:  0 Loss:  0.7538737058639526 Time:  21.23634362220764\n",
      "Epoch:  0 Loss:  0.6149595379829407 Time:  19.711164474487305\n",
      "Epoch:  0 Loss:  0.6372759342193604 Time:  19.878618478775024\n",
      "Epoch:  0 Loss:  0.529929518699646 Time:  20.274538278579712\n",
      "Epoch:  0 Loss:  0.7149437665939331 Time:  21.106003284454346\n",
      "Epoch:  0 Loss:  0.7324540615081787 Time:  21.079530954360962\n",
      "Epoch:  0 Loss:  0.49833211302757263 Time:  21.048123836517334\n",
      "Epoch:  0 Loss:  0.601012110710144 Time:  19.96260404586792\n",
      "Epoch:  0 Loss:  0.7481010556221008 Time:  19.585315465927124\n",
      "Epoch:  0 Loss:  0.6672069430351257 Time:  20.919296264648438\n",
      "Epoch:  0 Loss:  0.7794013023376465 Time:  20.46559453010559\n",
      "Epoch:  0 Loss:  0.6627246141433716 Time:  21.235031843185425\n",
      "Epoch:  0 Loss:  0.6577475070953369 Time:  22.14505386352539\n",
      "Epoch:  0 Loss:  0.5290916562080383 Time:  21.233901977539062\n",
      "Epoch:  0 Loss:  0.8160191774368286 Time:  22.27254009246826\n",
      "Epoch:  0 Loss:  0.6163354516029358 Time:  21.57878875732422\n",
      "Epoch:  0 Loss:  0.5552754998207092 Time:  21.325611352920532\n",
      "Epoch:  0 Loss:  0.4651768207550049 Time:  21.454541206359863\n",
      "Epoch:  0 Loss:  0.8786523938179016 Time:  21.578766345977783\n",
      "Epoch:  0 Loss:  0.6643005013465881 Time:  21.580716848373413\n",
      "Epoch:  0 Loss:  0.5975490212440491 Time:  20.55940890312195\n",
      "Epoch:  0 Loss:  0.572303295135498 Time:  21.07712149620056\n",
      "Epoch:  0 Loss:  0.6703965067863464 Time:  20.13800287246704\n",
      "Epoch:  0 Loss:  0.5889062881469727 Time:  19.947230100631714\n",
      "Epoch:  0 Loss:  0.4520668089389801 Time:  22.39374351501465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Loss:  0.6463426947593689 Time:  23.05247926712036\n",
      "Epoch:  0 Loss:  0.6755572557449341 Time:  21.461836099624634\n",
      "Epoch:  0 Loss:  0.5614104866981506 Time:  21.752849102020264\n",
      "Epoch:  0 Loss:  0.760784387588501 Time:  20.445417404174805\n",
      "Epoch:  0 Loss:  0.7924647331237793 Time:  21.578705072402954\n",
      "Epoch:  0 Loss:  0.5571088790893555 Time:  21.920318841934204\n",
      "Epoch:  0 Loss:  0.7522976994514465 Time:  21.2447566986084\n",
      "Epoch:  0 Loss:  0.495459645986557 Time:  20.44277858734131\n",
      "Epoch:  0 Loss:  0.647773265838623 Time:  19.916817665100098\n",
      "Epoch:  0 Loss:  0.703596293926239 Time:  21.051733016967773\n",
      "Epoch:  0 Loss:  0.49434491991996765 Time:  20.24582076072693\n",
      "Epoch:  0 Loss:  0.5489065051078796 Time:  21.889233112335205\n",
      "Epoch:  0 Loss:  0.5267086625099182 Time:  20.683708667755127\n",
      "Epoch:  0 Loss:  0.53385990858078 Time:  20.677809715270996\n",
      "Epoch:  0 Loss:  0.7740535736083984 Time:  22.166014671325684\n",
      "Epoch:  0 Loss:  0.6803619265556335 Time:  20.007529258728027\n",
      "Epoch:  0 Loss:  0.7732067704200745 Time:  20.855595350265503\n",
      "Epoch:  0 Loss:  0.5437672138214111 Time:  21.36149549484253\n",
      "Epoch:  0 Loss:  0.6014385223388672 Time:  21.297401189804077\n",
      "Epoch:  0 Loss:  0.8158646821975708 Time:  20.921563148498535\n",
      "Epoch:  0 Loss:  0.667453408241272 Time:  21.263481855392456\n",
      "Epoch:  0 Loss:  0.808491051197052 Time:  20.64183211326599\n",
      "Epoch:  0 Loss:  0.7833153009414673 Time:  21.248421669006348\n",
      "Epoch:  0 Loss:  0.5689760446548462 Time:  20.71453309059143\n",
      "Epoch:  0 Loss:  0.6427776217460632 Time:  21.60282301902771\n",
      "Epoch:  0 Loss:  0.49366241693496704 Time:  20.63746404647827\n",
      "Epoch:  0 Loss:  0.5848537683486938 Time:  19.58671545982361\n",
      "Epoch:  0 Loss:  0.5385465621948242 Time:  21.914037227630615\n",
      "Epoch:  0 Loss:  0.7072504758834839 Time:  21.671021461486816\n",
      "Epoch:  0 Loss:  0.5084314346313477 Time:  21.086036205291748\n",
      "Epoch:  0 Loss:  0.4801079034805298 Time:  20.592313528060913\n",
      "Epoch:  0 Loss:  0.5179100632667542 Time:  21.2767915725708\n",
      "Epoch:  0 Loss:  0.4585331082344055 Time:  21.795525550842285\n",
      "Epoch:  0 Loss:  0.6733916997909546 Time:  21.193885564804077\n",
      "Epoch:  0 Loss:  0.817031741142273 Time:  21.452994346618652\n",
      "Epoch:  0 Loss:  0.5530344247817993 Time:  20.746737480163574\n",
      "Epoch:  0 Loss:  0.6905869245529175 Time:  20.957239627838135\n",
      "Epoch:  0 Loss:  0.6838265061378479 Time:  20.88067054748535\n",
      "Epoch:  0 Loss:  0.5557011961936951 Time:  20.69315767288208\n",
      "Epoch:  0 Loss:  0.5286009907722473 Time:  21.01481318473816\n",
      "Epoch:  0 Loss:  0.6514367461204529 Time:  19.756479024887085\n",
      "Epoch:  0 Loss:  0.46828487515449524 Time:  21.404393434524536\n",
      "Epoch:  0 Loss:  0.70761638879776 Time:  20.780232906341553\n",
      "Epoch:  0 Loss:  0.5575739741325378 Time:  21.644484758377075\n",
      "Epoch:  0 Loss:  0.6142902970314026 Time:  21.172014474868774\n",
      "Epoch:  0 Loss:  0.6603445410728455 Time:  21.43421983718872\n",
      "Epoch:  0 Loss:  0.9186808466911316 Time:  21.815757274627686\n",
      "Epoch:  0 Loss:  0.6442911028862 Time:  20.268246173858643\n",
      "Epoch:  0 Loss:  0.6532812714576721 Time:  21.5207576751709\n",
      "Epoch:  0 Loss:  0.6937882304191589 Time:  20.480812311172485\n",
      "Epoch:  0 Loss:  0.697003960609436 Time:  21.616915702819824\n",
      "Epoch:  0 Loss:  0.48922625184059143 Time:  20.05583691596985\n",
      "Epoch:  0 Loss:  0.6672041416168213 Time:  20.674692392349243\n",
      "Epoch:  0 Loss:  0.56423020362854 Time:  20.268821239471436\n",
      "Epoch:  0 Loss:  0.5822396874427795 Time:  20.73304557800293\n",
      "Epoch:  0 Loss:  0.5793734788894653 Time:  19.874430179595947\n",
      "Epoch:  0 Loss:  0.524560809135437 Time:  20.335687398910522\n",
      "Epoch:  0 Loss:  0.5740163326263428 Time:  20.134894132614136\n",
      "Epoch:  0 Loss:  0.6213610768318176 Time:  19.772361278533936\n",
      "Epoch:  0 Loss:  0.4168717861175537 Time:  20.804348945617676\n",
      "Epoch:  0 Loss:  0.6111769676208496 Time:  21.741151571273804\n",
      "Epoch:  0 Loss:  0.5078387260437012 Time:  20.957332611083984\n",
      "Epoch:  0 Loss:  0.35893553495407104 Time:  20.51709222793579\n",
      "Epoch:  0 Loss:  0.45046570897102356 Time:  21.676394939422607\n",
      "Epoch:  0 Loss:  0.430162638425827 Time:  20.417680501937866\n",
      "Epoch:  0 Loss:  0.6598745584487915 Time:  19.144341707229614\n",
      "Epoch:  0 Loss:  0.6737409830093384 Time:  19.762554168701172\n",
      "Epoch:  0 Loss:  0.656370997428894 Time:  20.303908824920654\n",
      "Epoch:  0 Loss:  0.6769170165061951 Time:  21.17484426498413\n",
      "Epoch:  0 Loss:  0.6888182163238525 Time:  29.373997449874878\n",
      "Epoch:  0 Loss:  0.40426838397979736 Time:  23.23780131340027\n",
      "Epoch:  0 Loss:  0.6442794799804688 Time:  22.21713376045227\n",
      "Epoch:  0 Loss:  0.593235194683075 Time:  22.224815845489502\n",
      "Epoch:  0 Loss:  0.669483482837677 Time:  22.135870456695557\n",
      "Epoch:  0 Loss:  0.6565194725990295 Time:  21.14257526397705\n",
      "Epoch:  0 Loss:  0.5432630777359009 Time:  21.101719617843628\n",
      "Epoch:  0 Loss:  0.7301144003868103 Time:  20.08958101272583\n",
      "Epoch:  0 Loss:  0.5095996856689453 Time:  21.973387718200684\n",
      "Epoch:  0 Loss:  0.5091191530227661 Time:  21.242523670196533\n",
      "Epoch:  0 Loss:  0.8574061393737793 Time:  21.232083797454834\n",
      "Epoch:  0 Loss:  0.6624231934547424 Time:  21.907801628112793\n",
      "Epoch:  0 Loss:  0.609496533870697 Time:  21.155770301818848\n",
      "Epoch:  0 Loss:  0.5182749629020691 Time:  22.9947407245636\n",
      "Epoch:  0 Loss:  0.6702404022216797 Time:  21.256754636764526\n",
      "Epoch:  0 Loss:  0.6807734966278076 Time:  22.157708406448364\n",
      "Epoch:  0 Loss:  0.6888394951820374 Time:  22.319514751434326\n",
      "Epoch:  0 Loss:  0.7506806254386902 Time:  22.583641529083252\n",
      "Epoch:  0 Loss:  0.5999935269355774 Time:  21.391576051712036\n",
      "Epoch:  0 Loss:  0.7093989849090576 Time:  21.83013105392456\n",
      "Epoch:  0 Loss:  0.6624957323074341 Time:  21.74899411201477\n",
      "Epoch:  0 Loss:  0.5113199949264526 Time:  21.352407693862915\n",
      "Epoch:  0 Loss:  0.7930296063423157 Time:  21.39541745185852\n",
      "Epoch:  0 Loss:  0.5071526765823364 Time:  20.63834810256958\n",
      "Epoch:  0 Loss:  0.5774354934692383 Time:  20.096417903900146\n",
      "Epoch:  0 Loss:  1.0284210443496704 Time:  22.731156826019287\n",
      "Epoch:  0 Loss:  0.6355805993080139 Time:  22.12344789505005\n",
      "Epoch:  0 Loss:  0.545879602432251 Time:  21.968743324279785\n",
      "Epoch:  0 Loss:  0.6187859177589417 Time:  20.810617446899414\n",
      "Epoch:  0 Loss:  0.6936804056167603 Time:  21.32586121559143\n",
      "Epoch:  0 Loss:  0.33533376455307007 Time:  21.34662437438965\n",
      "Epoch:  0 Loss:  0.43277591466903687 Time:  21.48144030570984\n",
      "Epoch:  0 Loss:  0.5822741389274597 Time:  21.518266916275024\n",
      "Epoch:  0 Loss:  0.5099796056747437 Time:  22.294004201889038\n",
      "Epoch:  0 Loss:  0.41725990176200867 Time:  21.591007709503174\n",
      "Epoch:  0 Loss:  0.5350210666656494 Time:  21.180758476257324\n",
      "Epoch:  0 Loss:  0.453294962644577 Time:  21.070398092269897\n",
      "Epoch:  0 Loss:  0.6013443470001221 Time:  22.68224000930786\n",
      "Epoch:  0 Loss:  0.5732613205909729 Time:  21.0979425907135\n",
      "Epoch:  0 Loss:  0.5092751979827881 Time:  21.18039560317993\n",
      "Epoch:  0 Loss:  0.7262411117553711 Time:  22.066115140914917\n",
      "Epoch:  0 Loss:  0.7224036455154419 Time:  22.06683349609375\n",
      "Epoch:  0 Loss:  0.7274375557899475 Time:  21.723159313201904\n",
      "Epoch:  0 Loss:  0.676891028881073 Time:  21.30444359779358\n",
      "Epoch:  0 Loss:  0.8922431468963623 Time:  20.747694492340088\n",
      "Epoch:  0 Loss:  0.5560781955718994 Time:  20.463253498077393\n",
      "Epoch:  0 Loss:  0.6779479384422302 Time:  22.17422366142273\n",
      "Epoch:  0 Loss:  0.6065120697021484 Time:  22.163976430892944\n",
      "Epoch:  0 Loss:  0.4848363995552063 Time:  22.155879974365234\n",
      "Epoch:  0 Loss:  0.7080013155937195 Time:  23.052082300186157\n",
      "Epoch:  0 Loss:  0.5426766872406006 Time:  22.10684561729431\n",
      "Epoch:  0 Loss:  0.6921428442001343 Time:  22.470550537109375\n",
      "Epoch:  0 Loss:  0.6675211787223816 Time:  21.545870780944824\n",
      "Epoch:  0 Loss:  0.6336519122123718 Time:  21.343249082565308\n",
      "Epoch:  0 Loss:  0.5457907915115356 Time:  22.363760232925415\n",
      "Epoch:  0 Loss:  0.8313583731651306 Time:  20.758376598358154\n",
      "Epoch:  0 Loss:  0.42395123839378357 Time:  20.998051643371582\n",
      "Epoch:  0 Loss:  0.7599948048591614 Time:  21.07164239883423\n",
      "Epoch:  0 Loss:  0.5565921068191528 Time:  21.452547788619995\n",
      "Epoch:  0 Loss:  0.878297746181488 Time:  21.361223697662354\n",
      "Epoch:  0 Loss:  0.3488568663597107 Time:  21.34338116645813\n",
      "Epoch:  0 Loss:  0.6365000009536743 Time:  20.46329092979431\n",
      "Epoch:  0 Loss:  0.6462602019309998 Time:  20.261009454727173\n",
      "Epoch:  0 Loss:  0.6946784853935242 Time:  20.402546882629395\n",
      "Epoch:  0 Loss:  0.6465542316436768 Time:  20.957570552825928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Loss:  0.5772175788879395 Time:  22.846139430999756\n",
      "Epoch:  0 Loss:  0.6827038526535034 Time:  21.57692527770996\n",
      "Epoch:  0 Loss:  0.7749144434928894 Time:  21.691584825515747\n",
      "Epoch:  0 Loss:  0.6028000116348267 Time:  21.36836576461792\n",
      "Epoch:  0 Loss:  0.6552608609199524 Time:  20.319737434387207\n",
      "Epoch:  0 Loss:  0.6131236553192139 Time:  21.50457191467285\n",
      "Epoch:  0 Loss:  0.7702848315238953 Time:  22.549481868743896\n",
      "Epoch:  0 Loss:  0.730263352394104 Time:  21.120176792144775\n",
      "Epoch:  0 Loss:  0.6031690835952759 Time:  21.930808305740356\n",
      "Epoch:  0 Loss:  0.5271138548851013 Time:  21.141757011413574\n",
      "Epoch:  0 Loss:  0.6126202940940857 Time:  21.596765518188477\n",
      "Epoch:  0 Loss:  0.5714100003242493 Time:  21.792054891586304\n",
      "Epoch:  0 Loss:  0.6031731367111206 Time:  21.236915588378906\n",
      "Epoch:  0 Loss:  0.9035236835479736 Time:  21.73631739616394\n",
      "Epoch:  0 Loss:  0.6164431571960449 Time:  21.714070796966553\n",
      "Epoch:  0 Loss:  0.6723035573959351 Time:  21.08076786994934\n",
      "Epoch:  0 Loss:  0.6208456754684448 Time:  20.721848249435425\n",
      "Epoch:  0 Loss:  0.6621105670928955 Time:  21.186723709106445\n",
      "Epoch:  0 Loss:  0.7032976746559143 Time:  22.03915309906006\n",
      "Epoch:  0 Loss:  0.7368923425674438 Time:  21.975619316101074\n",
      "Epoch:  0 Loss:  0.467451274394989 Time:  21.26861596107483\n",
      "Epoch:  0 Loss:  0.49067193269729614 Time:  21.371724128723145\n",
      "Epoch:  0 Loss:  0.6518464684486389 Time:  22.733304738998413\n",
      "Epoch:  0 Loss:  0.48209068179130554 Time:  22.58133578300476\n",
      "Epoch:  0 Loss:  0.6622228026390076 Time:  21.28683352470398\n",
      "Epoch:  0 Loss:  0.7599011659622192 Time:  21.246673345565796\n",
      "Epoch:  0 Loss:  0.6121221780776978 Time:  21.88822913169861\n",
      "Epoch:  0 Loss:  0.6040167808532715 Time:  21.78874707221985\n",
      "Epoch:  0 Loss:  0.5233339071273804 Time:  22.108903646469116\n",
      "Epoch:  0 Loss:  0.5204344391822815 Time:  20.31452441215515\n",
      "Epoch:  0 Loss:  0.5694473385810852 Time:  22.780174493789673\n",
      "Epoch:  0 Loss:  0.6319593191146851 Time:  21.518514156341553\n",
      "Epoch:  0 Loss:  0.5719404220581055 Time:  21.05487847328186\n",
      "Epoch:  0 Loss:  0.8085589408874512 Time:  21.7383770942688\n",
      "Epoch:  0 Loss:  0.6802797317504883 Time:  22.015414237976074\n",
      "Epoch:  0 Loss:  0.7894919514656067 Time:  21.993489265441895\n",
      "Epoch:  0 Loss:  0.537532389163971 Time:  22.985232830047607\n",
      "Epoch:  0 Loss:  0.8015522360801697 Time:  21.475037097930908\n",
      "Epoch:  0 Loss:  0.5450294017791748 Time:  21.074758052825928\n",
      "Epoch:  0 Loss:  0.790398120880127 Time:  22.585613489151\n",
      "Epoch:  0 Loss:  0.4916568100452423 Time:  21.576985359191895\n",
      "Epoch:  0 Loss:  0.6269832253456116 Time:  20.311296939849854\n",
      "Epoch:  0 Loss:  0.7373427748680115 Time:  21.392598867416382\n",
      "Epoch:  0 Loss:  0.7026064991950989 Time:  21.411501169204712\n",
      "Epoch:  0 Loss:  0.7715370059013367 Time:  23.217321157455444\n",
      "Epoch:  0 Loss:  0.5271003842353821 Time:  22.884238958358765\n",
      "Epoch:  0 Loss:  0.586299479007721 Time:  21.380801916122437\n",
      "Epoch:  0 Loss:  0.5447847247123718 Time:  22.1585636138916\n",
      "Epoch:  0 Loss:  0.47664493322372437 Time:  22.260783433914185\n",
      "Epoch:  0 Loss:  0.7083473205566406 Time:  22.669384717941284\n",
      "Epoch:  0 Loss:  0.6698667407035828 Time:  22.03466558456421\n",
      "Epoch:  0 Loss:  0.7077443599700928 Time:  24.6416118144989\n",
      "Epoch:  0 Loss:  0.6824197173118591 Time:  21.496294736862183\n",
      "Epoch:  0 Loss:  0.62989741563797 Time:  21.49094533920288\n",
      "Epoch:  0 Loss:  0.6630136966705322 Time:  22.262608766555786\n",
      "Epoch:  0 Loss:  0.5775309205055237 Time:  21.45490789413452\n",
      "Epoch:  0 Loss:  0.8315185308456421 Time:  22.08400845527649\n",
      "Epoch:  0 Loss:  0.6182619333267212 Time:  21.791916131973267\n",
      "Epoch:  0 Loss:  0.5742749571800232 Time:  22.00927996635437\n",
      "Epoch:  0 Loss:  0.6175192594528198 Time:  20.808772802352905\n",
      "Epoch:  0 Loss:  0.7237184643745422 Time:  21.955753326416016\n",
      "Epoch:  0 Loss:  0.6945991516113281 Time:  20.792790412902832\n",
      "Epoch:  0 Loss:  0.6230981945991516 Time:  21.88698935508728\n",
      "Epoch:  0 Loss:  0.6371001601219177 Time:  21.223608016967773\n",
      "Epoch:  0 Loss:  0.6273502707481384 Time:  21.62271809577942\n",
      "Epoch:  0 Loss:  0.7027873396873474 Time:  24.043890714645386\n",
      "Epoch:  0 Loss:  0.5807488560676575 Time:  22.206074714660645\n",
      "Epoch:  0 Loss:  0.6914401650428772 Time:  21.223567962646484\n",
      "Epoch:  0 Loss:  0.6517863273620605 Time:  20.515648365020752\n",
      "Epoch:  0 Loss:  0.6298385262489319 Time:  21.88405132293701\n",
      "Epoch:  0 Loss:  0.6838313341140747 Time:  21.806684970855713\n",
      "Epoch:  0 Loss:  0.7071425318717957 Time:  21.83963179588318\n",
      "Epoch:  0 Loss:  0.7189399600028992 Time:  22.624948978424072\n",
      "Epoch:  0 Loss:  0.7289934754371643 Time:  21.927897214889526\n",
      "Epoch:  0 Loss:  0.6422908902168274 Time:  22.457547664642334\n",
      "Epoch:  0 Loss:  0.6872444152832031 Time:  21.309268951416016\n",
      "Epoch:  0 Loss:  0.44418978691101074 Time:  21.805257081985474\n",
      "Epoch:  0 Loss:  0.6396141648292542 Time:  21.093281507492065\n",
      "Epoch:  0 Loss:  0.7289746999740601 Time:  20.514208555221558\n",
      "Epoch:  0 Loss:  0.6445015668869019 Time:  20.875022411346436\n",
      "Epoch:  0 Loss:  0.58744877576828 Time:  22.214728116989136\n",
      "Epoch:  0 Loss:  0.46432748436927795 Time:  22.112717151641846\n",
      "Epoch:  0 Loss:  0.7242686748504639 Time:  22.14307951927185\n",
      "Epoch:  0 Loss:  0.7169556617736816 Time:  21.832576751708984\n",
      "Epoch:  0 Loss:  0.6239797472953796 Time:  21.08323359489441\n",
      "Epoch:  0 Loss:  0.6045466065406799 Time:  21.731207609176636\n",
      "Epoch:  0 Loss:  0.6390354037284851 Time:  22.011715412139893\n",
      "Epoch:  0 Loss:  0.7019296288490295 Time:  21.80805516242981\n",
      "Epoch:  0 Loss:  0.5425043702125549 Time:  22.977245092391968\n",
      "Epoch:  0 Loss:  0.5681222081184387 Time:  21.747836351394653\n",
      "Epoch:  0 Loss:  0.7159052491188049 Time:  21.325791835784912\n",
      "Epoch:  0 Loss:  0.5577437877655029 Time:  24.525813341140747\n",
      "Epoch:  0 Loss:  0.5075544118881226 Time:  22.51296091079712\n",
      "Epoch:  0 Loss:  0.7899816632270813 Time:  22.217491626739502\n",
      "Epoch:  0 Loss:  0.6834552884101868 Time:  21.972180604934692\n",
      "Epoch:  0 Loss:  0.5096262097358704 Time:  22.3543598651886\n",
      "Epoch:  0 Loss:  0.6577434539794922 Time:  20.996868133544922\n",
      "Epoch:  0 Loss:  0.5927645564079285 Time:  22.012327909469604\n",
      "Epoch:  0 Loss:  0.5353819131851196 Time:  21.854880332946777\n",
      "Epoch:  0 Loss:  0.5717806816101074 Time:  21.289300441741943\n",
      "Epoch:  0 Loss:  0.6904553174972534 Time:  21.641290426254272\n",
      "Epoch:  0 Loss:  0.5101975202560425 Time:  21.943365812301636\n",
      "Epoch:  0 Loss:  0.5962398052215576 Time:  20.505969524383545\n",
      "Epoch:  0 Loss:  0.529110848903656 Time:  20.697844982147217\n",
      "Epoch:  0 Loss:  0.7811588644981384 Time:  20.922658920288086\n",
      "Epoch:  0 Loss:  0.6336861252784729 Time:  21.403607606887817\n",
      "Epoch:  0 Loss:  0.5804253220558167 Time:  20.82614254951477\n",
      "Epoch:  0 Loss:  0.5795689821243286 Time:  21.696780681610107\n",
      "Epoch:  0 Loss:  0.6631506681442261 Time:  20.64919662475586\n",
      "Epoch:  0 Loss:  0.7701457142829895 Time:  20.508511781692505\n",
      "Epoch:  0 Loss:  0.6669265627861023 Time:  20.951871633529663\n",
      "Epoch:  0 Loss:  0.6295750737190247 Time:  20.776520490646362\n",
      "Epoch:  0 Loss:  0.42020151019096375 Time:  21.105341911315918\n",
      "Epoch:  0 Loss:  0.8198692202568054 Time:  20.37112045288086\n",
      "Epoch:  0 Loss:  0.7009145617485046 Time:  22.708604097366333\n",
      "Epoch:  0 Loss:  0.3825134038925171 Time:  21.26346516609192\n",
      "Epoch:  0 Loss:  0.4383901059627533 Time:  22.28465723991394\n",
      "Epoch:  0 Loss:  0.7716424465179443 Time:  22.369198322296143\n",
      "Epoch:  0 Loss:  0.44159504771232605 Time:  21.162368535995483\n",
      "Epoch:  0 Loss:  0.46643948554992676 Time:  21.642098903656006\n",
      "Epoch:  0 Loss:  0.6831381320953369 Time:  21.548063039779663\n",
      "Epoch:  0 Loss:  0.7994924187660217 Time:  20.66968822479248\n",
      "Epoch:  0 Loss:  0.708854079246521 Time:  20.839571952819824\n",
      "Epoch:  0 Loss:  0.6815841197967529 Time:  19.618762493133545\n",
      "Epoch:  0 Loss:  0.6196986436843872 Time:  20.605615615844727\n",
      "Epoch:  0 Loss:  0.5944944024085999 Time:  20.18279767036438\n",
      "Epoch:  0 Loss:  0.6018496751785278 Time:  20.448949575424194\n",
      "Epoch:  0 Loss:  0.5630491971969604 Time:  20.24594807624817\n",
      "Epoch:  0 Loss:  0.6961557269096375 Time:  20.07443594932556\n",
      "Epoch:  0 Loss:  0.534260094165802 Time:  19.852667808532715\n",
      "Epoch:  0 Loss:  0.8466299176216125 Time:  20.857657194137573\n",
      "Epoch:  0 Loss:  0.4848077595233917 Time:  21.811533451080322\n",
      "Epoch:  0 Loss:  0.7067527174949646 Time:  21.306893825531006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Loss:  0.49714675545692444 Time:  20.072754859924316\n",
      "Epoch:  0 Loss:  0.6254611611366272 Time:  21.2176296710968\n",
      "Epoch:  0 Loss:  0.8132237195968628 Time:  20.447760581970215\n",
      "Epoch:  0 Loss:  0.6713950634002686 Time:  21.31118392944336\n",
      "Epoch:  0 Loss:  0.4665634334087372 Time:  20.339218616485596\n",
      "Epoch:  0 Loss:  0.6814296245574951 Time:  20.52920365333557\n",
      "Epoch:  0 Loss:  0.5226960778236389 Time:  20.686487436294556\n",
      "Epoch:  0 Loss:  0.6827058792114258 Time:  20.24388027191162\n",
      "Epoch:  0 Loss:  0.6125113368034363 Time:  21.108948945999146\n",
      "Epoch:  0 Loss:  0.7113676071166992 Time:  20.65348482131958\n",
      "Epoch:  0 Loss:  0.573614776134491 Time:  22.438480854034424\n",
      "Epoch:  0 Loss:  0.6392103433609009 Time:  21.472023487091064\n",
      "Epoch:  0 Loss:  0.7080140709877014 Time:  23.000685453414917\n",
      "Epoch:  0 Loss:  0.5825631618499756 Time:  21.20598030090332\n",
      "Epoch:  0 Loss:  0.6260749697685242 Time:  20.63750410079956\n",
      "Epoch:  0 Loss:  0.6076040267944336 Time:  21.389968872070312\n",
      "Epoch:  0 Loss:  0.69612717628479 Time:  20.117201566696167\n",
      "Epoch:  0 Loss:  0.6247242093086243 Time:  19.44635534286499\n",
      "Epoch:  0 Loss:  0.5067353248596191 Time:  21.3137264251709\n",
      "Epoch:  0 Loss:  0.5106945633888245 Time:  20.290421724319458\n",
      "Epoch:  0 Loss:  0.7193264961242676 Time:  21.0910587310791\n",
      "Epoch:  0 Loss:  0.6888456344604492 Time:  20.834203958511353\n",
      "Epoch:  0 Loss:  0.6030015349388123 Time:  20.419092416763306\n",
      "Epoch:  0 Loss:  0.5941344499588013 Time:  21.02794885635376\n",
      "Epoch:  0 Loss:  0.6686810851097107 Time:  21.20324730873108\n",
      "Epoch:  0 Loss:  0.594206690788269 Time:  20.467119216918945\n",
      "Epoch:  0 Loss:  0.6723576784133911 Time:  21.253664255142212\n",
      "Epoch:  0 Loss:  0.7031567692756653 Time:  21.03040361404419\n",
      "Epoch:  0 Loss:  0.809856116771698 Time:  21.061453342437744\n",
      "Epoch:  0 Loss:  0.6111714243888855 Time:  20.37050461769104\n",
      "Epoch:  0 Loss:  0.714561402797699 Time:  20.622113466262817\n",
      "Epoch:  0 Loss:  0.648703932762146 Time:  20.290845155715942\n",
      "Epoch:  0 Loss:  0.4817214012145996 Time:  19.396177530288696\n",
      "Epoch:  0 Loss:  0.6125006675720215 Time:  18.852397441864014\n",
      "Epoch:  0 Loss:  0.6113907694816589 Time:  20.324854373931885\n",
      "Epoch:  0 Loss:  0.4704861342906952 Time:  20.83978271484375\n",
      "Epoch:  0 Loss:  0.7318805456161499 Time:  20.41799545288086\n",
      "Epoch:  0 Loss:  0.7129508256912231 Time:  20.683271884918213\n",
      "Epoch:  0 Loss:  0.5898852348327637 Time:  21.91410255432129\n",
      "Epoch:  0 Loss:  0.810753345489502 Time:  20.07458782196045\n",
      "Epoch:  0 Loss:  0.6351803541183472 Time:  22.451562643051147\n",
      "Epoch:  0 Loss:  0.5920895338058472 Time:  22.02574372291565\n",
      "Epoch:  0 Loss:  0.5170630216598511 Time:  21.201318979263306\n",
      "Epoch:  0 Loss:  0.6911106109619141 Time:  20.403746843338013\n",
      "Epoch:  0 Loss:  0.6870704889297485 Time:  21.28165626525879\n",
      "Epoch:  0 Loss:  0.6107743382453918 Time:  21.062065362930298\n",
      "Epoch:  0 Loss:  0.6633709073066711 Time:  21.343326807022095\n",
      "Epoch:  0 Loss:  0.3563096523284912 Time:  20.690307140350342\n",
      "Epoch:  0 Loss:  0.5108200311660767 Time:  21.50651478767395\n",
      "Epoch:  0 Loss:  0.437255859375 Time:  19.759114742279053\n",
      "Epoch:  0 Loss:  0.38575971126556396 Time:  21.866159915924072\n",
      "Epoch:  0 Loss:  0.5222445130348206 Time:  20.702301502227783\n",
      "Epoch:  0 Loss:  0.5789017677307129 Time:  21.571948766708374\n",
      "Epoch:  0 Loss:  0.6155378222465515 Time:  20.542301416397095\n",
      "Epoch:  0 Loss:  0.5537578463554382 Time:  21.436045169830322\n",
      "Epoch:  0 Loss:  0.5827274918556213 Time:  20.519171476364136\n",
      "Epoch:  0 Loss:  0.6662322282791138 Time:  20.541496753692627\n",
      "Epoch:  0 Loss:  0.3661666512489319 Time:  21.18617343902588\n",
      "Epoch:  0 Loss:  0.9784796237945557 Time:  21.219534158706665\n",
      "Epoch:  0 Loss:  0.65334153175354 Time:  20.039230823516846\n",
      "Epoch:  0 Loss:  0.5988077521324158 Time:  21.340070486068726\n",
      "Epoch:  0 Loss:  0.7004347443580627 Time:  21.810890913009644\n",
      "Epoch:  0 Loss:  0.773562490940094 Time:  20.826698303222656\n",
      "Epoch:  0 Loss:  0.5022270679473877 Time:  20.43667435646057\n",
      "Epoch:  0 Loss:  0.6163591146469116 Time:  21.011564254760742\n",
      "Epoch:  0 Loss:  0.42363741993904114 Time:  20.825748205184937\n",
      "Epoch:  0 Loss:  0.7805514931678772 Time:  22.47206473350525\n",
      "Epoch:  0 Loss:  0.6450850367546082 Time:  21.358049631118774\n",
      "Epoch:  0 Loss:  0.6811766624450684 Time:  21.697417497634888\n",
      "Epoch:  0 Loss:  0.602569580078125 Time:  20.918808460235596\n",
      "Epoch:  0 Loss:  0.5602229237556458 Time:  19.946316957473755\n",
      "Epoch:  0 Loss:  0.46541112661361694 Time:  22.47143030166626\n",
      "Epoch:  0 Loss:  0.7391479015350342 Time:  21.987475156784058\n",
      "Epoch:  0 Loss:  0.44018158316612244 Time:  20.513275384902954\n",
      "Epoch:  0 Loss:  0.4180549383163452 Time:  21.060103178024292\n",
      "Epoch:  0 Loss:  0.6840217709541321 Time:  20.87043857574463\n",
      "Epoch:  0 Loss:  0.3828422427177429 Time:  19.867013454437256\n",
      "Epoch:  0 Loss:  0.4828750789165497 Time:  21.37182903289795\n",
      "Epoch:  0 Loss:  0.6736775636672974 Time:  20.526537895202637\n",
      "Epoch:  0 Loss:  0.5525203347206116 Time:  21.324669122695923\n",
      "Epoch:  0 Loss:  0.7436363101005554 Time:  22.198269844055176\n",
      "Epoch:  0 Loss:  0.6857484579086304 Time:  21.15406370162964\n",
      "Epoch:  0 Loss:  0.5675865411758423 Time:  20.87172293663025\n",
      "Epoch:  0 Loss:  0.5227273106575012 Time:  20.638785123825073\n",
      "Epoch:  0 Loss:  0.44516199827194214 Time:  20.680963039398193\n",
      "Epoch:  0 Loss:  0.9268274903297424 Time:  21.327651500701904\n",
      "Epoch:  0 Loss:  0.6129835844039917 Time:  21.84657096862793\n",
      "Epoch:  0 Loss:  0.7096515893936157 Time:  21.499225616455078\n",
      "Epoch:  0 Loss:  0.6748502850532532 Time:  21.88548493385315\n",
      "Epoch:  0 Loss:  0.647508442401886 Time:  20.705373525619507\n",
      "Epoch:  0 Loss:  0.9418687224388123 Time:  21.429131984710693\n",
      "Epoch:  0 Loss:  0.5080834031105042 Time:  20.9175763130188\n",
      "Epoch:  0 Loss:  0.5968848466873169 Time:  21.27893376350403\n",
      "Epoch:  0 Loss:  0.6307350397109985 Time:  21.898189544677734\n",
      "Epoch:  0 Loss:  0.45803743600845337 Time:  21.51598024368286\n",
      "Epoch:  0 Loss:  0.6375288963317871 Time:  21.155234575271606\n",
      "Epoch:  0 Loss:  0.5673087239265442 Time:  20.99903917312622\n",
      "Epoch:  0 Loss:  0.6785470247268677 Time:  21.20145583152771\n",
      "Epoch:  0 Loss:  0.5325062870979309 Time:  21.248438835144043\n",
      "Epoch:  0 Loss:  0.8285854458808899 Time:  20.787930965423584\n",
      "Epoch:  0 Loss:  0.5411984920501709 Time:  21.655301094055176\n",
      "Epoch:  0 Loss:  0.5957163572311401 Time:  20.62095808982849\n",
      "Epoch:  0 Loss:  0.6210011839866638 Time:  20.55661678314209\n",
      "Epoch:  0 Loss:  0.738818883895874 Time:  20.46692156791687\n",
      "Epoch:  0 Loss:  0.5952511429786682 Time:  19.804961442947388\n",
      "Epoch:  0 Loss:  0.41080784797668457 Time:  20.272499322891235\n",
      "Epoch:  0 Loss:  0.48172610998153687 Time:  19.66853928565979\n",
      "Epoch:  0 Loss:  0.5434389710426331 Time:  20.385751485824585\n",
      "Epoch:  0 Loss:  0.7319002747535706 Time:  20.404073476791382\n",
      "Epoch:  0 Loss:  0.5288516879081726 Time:  21.34340214729309\n",
      "Epoch:  0 Loss:  0.5753673315048218 Time:  20.81118369102478\n",
      "Epoch:  0 Loss:  0.606949508190155 Time:  20.824702978134155\n",
      "Epoch:  0 Loss:  0.5334839224815369 Time:  20.94901132583618\n",
      "Epoch:  0 Loss:  0.5744017362594604 Time:  21.088234901428223\n",
      "Epoch:  0 Loss:  0.7682652473449707 Time:  20.899131774902344\n",
      "Epoch:  0 Loss:  0.5829814076423645 Time:  20.7162868976593\n",
      "Epoch:  0 Loss:  0.5060109496116638 Time:  21.387349605560303\n",
      "Epoch:  0 Loss:  0.6306071281433105 Time:  19.68154239654541\n",
      "Epoch:  0 Loss:  0.5813911557197571 Time:  21.647642850875854\n",
      "Epoch:  0 Loss:  0.3966571092605591 Time:  21.184289693832397\n",
      "Epoch:  0 Loss:  0.4481528401374817 Time:  23.375258445739746\n",
      "Epoch:  0 Loss:  0.824194610118866 Time:  21.11481809616089\n",
      "Epoch:  0 Loss:  0.45154470205307007 Time:  20.95257878303528\n",
      "Epoch:  0 Loss:  0.6773620843887329 Time:  21.080045700073242\n",
      "Epoch:  0 Loss:  0.5575931668281555 Time:  20.667351007461548\n",
      "Epoch:  0 Loss:  0.7868490815162659 Time:  21.580745935440063\n",
      "Epoch:  0 Loss:  0.4816491901874542 Time:  21.30709147453308\n",
      "Epoch:  0 Loss:  0.5646986961364746 Time:  21.9856960773468\n",
      "Epoch:  0 Loss:  0.46077948808670044 Time:  21.01065230369568\n",
      "Epoch:  0 Loss:  0.6733564138412476 Time:  19.60339593887329\n",
      "Epoch:  0 Loss:  0.44230061769485474 Time:  20.12077236175537\n",
      "Epoch:  0 Loss:  0.6113446354866028 Time:  20.215375900268555\n",
      "Epoch:  0 Loss:  0.6946004629135132 Time:  20.5440456867218\n",
      "Epoch:  0 Loss:  0.6192091107368469 Time:  20.43913722038269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Loss:  0.5894476175308228 Time:  21.173486948013306\n",
      "Epoch:  0 Loss:  0.4862755537033081 Time:  23.290499925613403\n",
      "Epoch:  0 Loss:  0.6306678056716919 Time:  20.23138976097107\n",
      "Epoch:  0 Loss:  0.5604301691055298 Time:  20.365519046783447\n",
      "Epoch:  0 Loss:  0.6539994478225708 Time:  20.783619165420532\n",
      "Epoch:  0 Loss:  0.5064821243286133 Time:  19.396920680999756\n",
      "Epoch:  0 Loss:  0.6865912675857544 Time:  19.461453437805176\n",
      "Epoch:  0 Loss:  0.5423657894134521 Time:  20.668046951293945\n",
      "Epoch:  0 Loss:  0.7376050353050232 Time:  21.125709295272827\n",
      "Epoch:  0 Loss:  0.45649999380111694 Time:  20.323060989379883\n",
      "Epoch:  0 Loss:  0.5504552721977234 Time:  21.53085470199585\n",
      "Epoch:  0 Loss:  0.7250035405158997 Time:  20.625279426574707\n",
      "Epoch:  0 Loss:  0.5872507691383362 Time:  19.799081563949585\n",
      "Epoch:  0 Loss:  0.6552936434745789 Time:  21.09902024269104\n",
      "Epoch:  0 Loss:  0.5909794569015503 Time:  20.356279850006104\n",
      "Epoch:  0 Loss:  0.4929842948913574 Time:  22.88325071334839\n",
      "Epoch:  0 Loss:  0.5359781384468079 Time:  21.060181856155396\n",
      "Epoch:  0 Loss:  0.6514257192611694 Time:  21.440030097961426\n",
      "Epoch:  0 Loss:  0.40257206559181213 Time:  20.87083411216736\n",
      "Epoch:  0 Loss:  0.4322679042816162 Time:  26.750007152557373\n",
      "Epoch:  0 Loss:  0.6685726046562195 Time:  23.274999618530273\n",
      "Epoch:  0 Loss:  0.5702940821647644 Time:  22.556415557861328\n",
      "Epoch:  0 Loss:  0.6144775152206421 Time:  22.58759570121765\n",
      "Epoch:  0 Loss:  0.6390032768249512 Time:  21.019192695617676\n",
      "Epoch:  0 Loss:  0.561388373374939 Time:  21.973658561706543\n",
      "Epoch:  0 Loss:  0.9161933064460754 Time:  21.45408844947815\n",
      "Epoch:  0 Loss:  0.6297367215156555 Time:  22.11173415184021\n",
      "Epoch:  0 Loss:  0.7260246276855469 Time:  21.05431604385376\n",
      "Epoch:  0 Loss:  0.5225507616996765 Time:  21.346724271774292\n",
      "Epoch:  0 Loss:  0.6279177069664001 Time:  21.138683557510376\n",
      "Epoch:  0 Loss:  0.7500770688056946 Time:  21.969481468200684\n",
      "Epoch:  0 Loss:  0.5707217454910278 Time:  21.466776132583618\n",
      "Epoch:  0 Loss:  0.5764188170433044 Time:  21.444212198257446\n",
      "Epoch:  0 Loss:  0.45954087376594543 Time:  22.317660093307495\n",
      "Epoch:  0 Loss:  0.4911973178386688 Time:  21.872233867645264\n",
      "Epoch:  0 Loss:  0.6256607174873352 Time:  20.870731592178345\n",
      "Epoch:  0 Loss:  0.5272578001022339 Time:  21.227588891983032\n",
      "Epoch:  0 Loss:  0.6197167634963989 Time:  21.54350519180298\n",
      "Epoch:  0 Loss:  0.5060132145881653 Time:  24.42775058746338\n",
      "Epoch:  0 Loss:  0.7535840272903442 Time:  23.144878149032593\n",
      "Epoch:  0 Loss:  0.4366722106933594 Time:  21.260913848876953\n",
      "Epoch:  0 Loss:  0.40436986088752747 Time:  21.489829540252686\n",
      "Epoch:  0 Loss:  0.5686913728713989 Time:  23.012123584747314\n",
      "Epoch:  0 Loss:  0.6143705248832703 Time:  21.85087561607361\n",
      "Epoch:  0 Loss:  0.7235874533653259 Time:  21.707990884780884\n",
      "Epoch:  0 Loss:  0.5508586764335632 Time:  22.038469076156616\n",
      "Epoch:  0 Loss:  0.5504903793334961 Time:  20.91391611099243\n",
      "Epoch:  0 Loss:  0.6495323181152344 Time:  20.940775394439697\n",
      "Epoch:  0 Loss:  0.5317028760910034 Time:  22.148011445999146\n",
      "Epoch:  0 Loss:  0.4885612428188324 Time:  22.259904861450195\n",
      "Epoch:  0 Loss:  0.47791123390197754 Time:  21.790018558502197\n",
      "Epoch:  0 Loss:  0.6189704537391663 Time:  21.647433042526245\n",
      "Epoch:  0 Loss:  0.3661419451236725 Time:  21.100029706954956\n",
      "Epoch:  0 Loss:  0.6716748476028442 Time:  21.664701461791992\n",
      "Epoch:  0 Loss:  0.793999433517456 Time:  21.36421227455139\n",
      "Epoch:  0 Loss:  0.5215293169021606 Time:  20.27187943458557\n",
      "Epoch:  0 Loss:  0.7299287915229797 Time:  20.34554362297058\n",
      "Epoch:  0 Loss:  0.43945562839508057 Time:  21.361005544662476\n",
      "Epoch:  0 Loss:  0.42336976528167725 Time:  20.7738676071167\n",
      "Epoch:  0 Loss:  0.6790716052055359 Time:  21.95160961151123\n",
      "Epoch:  0 Loss:  0.6511611342430115 Time:  21.60789132118225\n",
      "Epoch:  0 Loss:  0.5250974893569946 Time:  21.563321352005005\n",
      "Epoch:  0 Loss:  0.6405541300773621 Time:  21.166600704193115\n",
      "Epoch:  0 Loss:  0.5641541481018066 Time:  21.285387992858887\n",
      "Epoch:  0 Loss:  0.5664408802986145 Time:  20.485188007354736\n",
      "Epoch:  0 Loss:  0.49233752489089966 Time:  21.034485578536987\n",
      "Epoch:  0 Loss:  0.7870587110519409 Time:  22.326732635498047\n",
      "Epoch:  0 Loss:  0.6133162379264832 Time:  20.288336277008057\n",
      "Epoch:  0 Loss:  0.5727221965789795 Time:  21.43132495880127\n",
      "Epoch:  0 Loss:  0.6085600852966309 Time:  21.64581322669983\n",
      "Epoch:  0 Loss:  0.9800999164581299 Time:  21.39494299888611\n",
      "Epoch:  0 Loss:  0.6482726335525513 Time:  21.614948511123657\n",
      "Epoch:  0 Loss:  0.5740833282470703 Time:  21.0977566242218\n",
      "Epoch:  0 Loss:  0.6104859113693237 Time:  22.13475775718689\n",
      "Epoch:  0 Loss:  0.5813287496566772 Time:  21.249435901641846\n",
      "Epoch:  0 Loss:  0.5288733243942261 Time:  21.482266187667847\n",
      "Epoch:  0 Loss:  0.5183626413345337 Time:  21.768182277679443\n",
      "Epoch:  0 Loss:  0.5304578542709351 Time:  20.9886212348938\n",
      "Epoch:  0 Loss:  0.5201147794723511 Time:  21.170953512191772\n",
      "Epoch:  0 Loss:  0.724407434463501 Time:  21.22299838066101\n",
      "Epoch:  0 Loss:  0.5510357022285461 Time:  20.80448293685913\n",
      "Epoch:  0 Loss:  0.5756970047950745 Time:  21.043612003326416\n",
      "Epoch:  0 Loss:  0.8075425624847412 Time:  21.141951084136963\n",
      "Epoch:  0 Loss:  0.5083933472633362 Time:  20.196442127227783\n",
      "Epoch:  0 Loss:  0.6077375411987305 Time:  22.17117214202881\n",
      "Epoch:  0 Loss:  0.6044805645942688 Time:  23.41245937347412\n",
      "Epoch:  0 Loss:  0.6168926954269409 Time:  21.58542275428772\n",
      "Epoch:  0 Loss:  0.6323345303535461 Time:  21.73366093635559\n",
      "Epoch:  0 Loss:  0.5754416584968567 Time:  20.63327383995056\n",
      "Epoch:  0 Loss:  0.686191201210022 Time:  21.8907630443573\n",
      "Epoch:  0 Loss:  0.6132534146308899 Time:  21.11626672744751\n",
      "Epoch:  0 Loss:  0.7892916798591614 Time:  21.816394567489624\n",
      "Epoch:  0 Loss:  0.6804733276367188 Time:  24.851681232452393\n",
      "Epoch:  0 Loss:  0.7758644819259644 Time:  22.84687638282776\n",
      "Epoch:  0 Loss:  0.7529627680778503 Time:  22.540197610855103\n",
      "Epoch:  0 Loss:  0.6180315613746643 Time:  21.357192754745483\n",
      "Epoch:  0 Loss:  0.5491840839385986 Time:  23.32450222969055\n",
      "Epoch:  0 Loss:  0.5351495146751404 Time:  20.87606191635132\n",
      "Epoch:  0 Loss:  0.4843275547027588 Time:  21.217626810073853\n",
      "Epoch:  0 Loss:  0.590578019618988 Time:  21.822744369506836\n",
      "Epoch:  0 Loss:  0.6351173520088196 Time:  21.601909637451172\n",
      "Epoch:  0 Loss:  0.6220641732215881 Time:  21.980895519256592\n",
      "Epoch:  0 Loss:  0.6353638768196106 Time:  22.225799798965454\n",
      "Epoch:  0 Loss:  0.4499775171279907 Time:  21.20973300933838\n",
      "Epoch:  0 Loss:  0.6010381579399109 Time:  22.263839960098267\n",
      "Epoch:  0 Loss:  0.740211546421051 Time:  23.698177814483643\n",
      "Epoch:  0 Loss:  0.5886415243148804 Time:  21.737367391586304\n",
      "Epoch:  0 Loss:  0.5315219759941101 Time:  21.816614627838135\n",
      "Epoch:  0 Loss:  0.5166441798210144 Time:  21.15669560432434\n",
      "Epoch:  0 Loss:  0.4903869926929474 Time:  21.578149557113647\n",
      "Epoch:  0 Loss:  0.8295959234237671 Time:  23.28682017326355\n",
      "Epoch:  0 Loss:  0.6157658100128174 Time:  21.661580085754395\n",
      "Epoch:  0 Loss:  0.7046167254447937 Time:  21.596964597702026\n",
      "Epoch:  0 Loss:  0.6978594660758972 Time:  22.45059108734131\n",
      "Epoch:  0 Loss:  0.7324888110160828 Time:  22.677476167678833\n",
      "Epoch:  0 Loss:  0.5254072546958923 Time:  21.99288272857666\n",
      "Epoch:  0 Loss:  0.5808703303337097 Time:  21.874841451644897\n",
      "Epoch:  0 Loss:  0.8682078123092651 Time:  21.76396656036377\n",
      "Epoch:  0 Loss:  0.42084991931915283 Time:  24.047202348709106\n",
      "Epoch:  0 Loss:  0.5225130915641785 Time:  23.984107971191406\n",
      "Epoch:  0 Loss:  0.7879021167755127 Time:  22.138233423233032\n",
      "Epoch:  0 Loss:  0.5984483957290649 Time:  20.265525102615356\n",
      "Epoch:  0 Loss:  0.5220960974693298 Time:  20.022855281829834\n",
      "Epoch:  0 Loss:  0.6840423941612244 Time:  20.22634506225586\n",
      "Epoch:  0 Loss:  0.594391405582428 Time:  21.155568599700928\n",
      "Epoch:  0 Loss:  0.5241098999977112 Time:  20.305224895477295\n",
      "Epoch:  0 Loss:  0.5689530968666077 Time:  21.86365008354187\n",
      "Epoch:  0 Loss:  0.7389853596687317 Time:  21.25747847557068\n",
      "Epoch:  0 Loss:  0.5988503694534302 Time:  21.935787439346313\n",
      "Epoch:  0 Loss:  0.667034924030304 Time:  21.070312976837158\n",
      "Epoch:  0 Loss:  0.6788482666015625 Time:  21.267837285995483\n",
      "Epoch:  0 Loss:  0.6538183689117432 Time:  22.33171772956848\n",
      "Epoch:  0 Loss:  0.6079410314559937 Time:  22.300739526748657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Loss:  0.5192825198173523 Time:  21.308470964431763\n",
      "Epoch:  0 Loss:  0.6592599749565125 Time:  20.369400024414062\n",
      "Epoch:  0 Loss:  0.5313524603843689 Time:  21.547585010528564\n",
      "Epoch:  0 Loss:  0.4658326208591461 Time:  21.088003635406494\n",
      "Epoch:  0 Loss:  0.5822165608406067 Time:  22.11459970474243\n",
      "Epoch:  0 Loss:  0.5440664291381836 Time:  22.27067232131958\n",
      "Epoch:  0 Loss:  0.7159848213195801 Time:  22.192543745040894\n",
      "Epoch:  0 Loss:  0.636481523513794 Time:  21.134622812271118\n",
      "Epoch:  0 Loss:  0.6293905377388 Time:  21.774882793426514\n",
      "Epoch:  0 Loss:  0.5959954857826233 Time:  21.412108421325684\n",
      "Epoch:  0 Loss:  0.44762560725212097 Time:  21.17754864692688\n",
      "Epoch:  0 Loss:  0.7095305323600769 Time:  23.12039279937744\n",
      "Epoch:  0 Loss:  0.7339109778404236 Time:  22.190491199493408\n",
      "Epoch:  0 Loss:  0.6116176247596741 Time:  21.239078283309937\n",
      "Epoch:  0 Loss:  0.7079934477806091 Time:  22.98056960105896\n",
      "Epoch:  0 Loss:  0.6676545739173889 Time:  21.86104106903076\n",
      "Epoch:  0 Loss:  0.5268814563751221 Time:  21.079878091812134\n",
      "Epoch:  0 Loss:  0.5843132138252258 Time:  21.296828746795654\n",
      "Epoch:  0 Loss:  0.6461743712425232 Time:  20.895035982131958\n",
      "Epoch:  0 Loss:  0.6923320889472961 Time:  21.309494495391846\n",
      "Epoch:  0 Loss:  0.48479893803596497 Time:  22.301731824874878\n",
      "Epoch:  0 Loss:  0.5562857985496521 Time:  24.13858723640442\n",
      "Epoch:  0 Loss:  0.4937805235385895 Time:  21.986871242523193\n",
      "Epoch:  0 Loss:  0.7374787330627441 Time:  22.324244499206543\n",
      "Epoch:  0 Loss:  0.7325802445411682 Time:  22.474801540374756\n",
      "Epoch:  0 Loss:  0.8404174447059631 Time:  25.9756441116333\n",
      "Epoch:  0 Loss:  0.6481073498725891 Time:  43.134578227996826\n",
      "Epoch:  0 Loss:  0.6237056851387024 Time:  33.32719969749451\n",
      "Epoch:  0 Loss:  0.5001569986343384 Time:  25.44476842880249\n",
      "Epoch:  0 Loss:  0.31210312247276306 Time:  24.34666132926941\n",
      "Epoch:  0 Loss:  0.5955567359924316 Time:  26.754981517791748\n",
      "Epoch:  0 Loss:  0.4823303818702698 Time:  26.160770893096924\n",
      "Epoch:  0 Loss:  0.6812853813171387 Time:  24.48414134979248\n",
      "Epoch:  0 Loss:  0.662651777267456 Time:  24.976766109466553\n",
      "Epoch:  0 Loss:  0.6609761118888855 Time:  24.711626291275024\n",
      "Epoch:  0 Loss:  0.6187766790390015 Time:  23.95221757888794\n",
      "Epoch:  0 Loss:  0.7563880085945129 Time:  23.535365104675293\n",
      "Epoch:  0 Loss:  0.5859894752502441 Time:  24.14238166809082\n",
      "Epoch:  0 Loss:  0.7046352028846741 Time:  25.107100009918213\n",
      "Epoch:  0 Loss:  0.5753563642501831 Time:  25.543142080307007\n",
      "Epoch:  0 Loss:  0.5947538614273071 Time:  26.194464445114136\n",
      "Epoch:  0 Loss:  0.4711018204689026 Time:  26.34296488761902\n",
      "Epoch:  0 Loss:  0.7672385573387146 Time:  26.289653778076172\n",
      "Epoch:  0 Loss:  0.36935895681381226 Time:  23.819897413253784\n",
      "Epoch:  0 Loss:  0.5997763872146606 Time:  24.082181215286255\n",
      "Epoch:  0 Loss:  0.8809536099433899 Time:  23.435399532318115\n",
      "Epoch:  0 Loss:  0.4822419285774231 Time:  23.269484043121338\n",
      "Epoch:  0 Loss:  0.7182965874671936 Time:  22.48670244216919\n",
      "Epoch:  0 Loss:  0.4031458795070648 Time:  22.535633087158203\n",
      "Epoch:  0 Loss:  0.46101516485214233 Time:  24.358781814575195\n",
      "Epoch:  0 Loss:  0.7003574967384338 Time:  23.70061182975769\n",
      "Epoch:  0 Loss:  0.49081656336784363 Time:  22.32647681236267\n",
      "Epoch:  0 Loss:  0.728392481803894 Time:  21.778257369995117\n",
      "Epoch:  0 Loss:  0.7295060753822327 Time:  22.050399780273438\n",
      "Epoch:  0 Loss:  0.6120966672897339 Time:  21.208447456359863\n",
      "Epoch:  0 Loss:  0.46603918075561523 Time:  22.33172369003296\n",
      "Epoch:  0 Loss:  0.6042082905769348 Time:  23.275436639785767\n",
      "Epoch:  0 Loss:  0.684760570526123 Time:  21.761903524398804\n",
      "Epoch:  0 Loss:  0.5379758477210999 Time:  23.262493133544922\n",
      "Epoch:  0 Loss:  0.7489727735519409 Time:  22.5424747467041\n",
      "Epoch:  0 Loss:  0.4876521825790405 Time:  21.80428194999695\n",
      "Epoch:  0 Loss:  0.5797256827354431 Time:  23.068565607070923\n",
      "Epoch:  0 Loss:  0.571462094783783 Time:  23.448813676834106\n",
      "Epoch:  0 Loss:  0.6612493395805359 Time:  21.81804847717285\n",
      "Epoch:  0 Loss:  0.43105292320251465 Time:  21.77592706680298\n",
      "Epoch:  0 Loss:  0.7573859095573425 Time:  22.86914587020874\n",
      "Epoch:  0 Loss:  0.6973415613174438 Time:  24.256245851516724\n",
      "Epoch:  0 Loss:  0.7429553866386414 Time:  32.25881266593933\n",
      "Epoch:  0 Loss:  0.4208104908466339 Time:  24.271369457244873\n",
      "Epoch:  0 Loss:  0.5057101249694824 Time:  22.396461248397827\n",
      "Epoch:  0 Loss:  0.695062518119812 Time:  24.595442295074463\n",
      "Epoch:  0 Loss:  0.6872702240943909 Time:  22.849472522735596\n",
      "Epoch:  0 Loss:  0.5121980905532837 Time:  22.6633083820343\n",
      "Epoch:  0 Loss:  0.7003260850906372 Time:  22.776007413864136\n",
      "Epoch:  0 Loss:  0.5245954394340515 Time:  21.276154279708862\n",
      "Epoch:  0 Loss:  0.48159775137901306 Time:  21.798537731170654\n",
      "Epoch:  0 Loss:  0.8518025279045105 Time:  22.01037311553955\n",
      "Epoch:  0 Loss:  0.8192092180252075 Time:  21.624411821365356\n",
      "Epoch:  0 Loss:  0.4815663695335388 Time:  23.205044269561768\n",
      "Epoch:  0 Loss:  0.57558673620224 Time:  23.716415882110596\n",
      "Epoch:  0 Loss:  0.7232726216316223 Time:  21.91087055206299\n",
      "Epoch:  0 Loss:  0.6555442810058594 Time:  23.11692237854004\n",
      "Epoch:  0 Loss:  0.44893965125083923 Time:  21.96489930152893\n",
      "Epoch:  0 Loss:  0.5891746282577515 Time:  22.71902823448181\n",
      "Epoch:  0 Loss:  0.6385472416877747 Time:  22.43102264404297\n",
      "Epoch:  0 Loss:  0.6033357977867126 Time:  21.92516279220581\n",
      "Epoch:  0 Loss:  0.5956699252128601 Time:  21.14939546585083\n",
      "Epoch:  0 Loss:  0.5213591456413269 Time:  23.978327751159668\n",
      "Epoch:  0 Loss:  0.43133974075317383 Time:  24.27802872657776\n",
      "Epoch:  0 Loss:  0.8355352282524109 Time:  21.951599597930908\n",
      "Epoch:  0 Loss:  0.573249876499176 Time:  21.881887197494507\n",
      "Epoch:  0 Loss:  0.5099919438362122 Time:  21.8578143119812\n",
      "Epoch:  0 Loss:  0.5961671471595764 Time:  20.95557951927185\n",
      "Epoch:  0 Loss:  0.5424042344093323 Time:  21.934555768966675\n",
      "Epoch:  0 Loss:  0.7654513716697693 Time:  21.99103808403015\n",
      "Epoch:  0 Loss:  0.6285151839256287 Time:  21.282140970230103\n",
      "Epoch:  0 Loss:  0.6162562370300293 Time:  21.744359970092773\n",
      "Epoch:  0 Loss:  0.481606125831604 Time:  21.441184520721436\n",
      "Epoch:  0 Loss:  0.4629179537296295 Time:  21.767322063446045\n",
      "Epoch:  0 Loss:  0.535281240940094 Time:  23.522605180740356\n",
      "Epoch:  0 Loss:  0.4398391544818878 Time:  22.89390468597412\n",
      "Epoch:  0 Loss:  0.6067463755607605 Time:  21.99396848678589\n",
      "Epoch:  0 Loss:  0.5780192613601685 Time:  22.876264333724976\n",
      "Epoch:  0 Loss:  0.6822370290756226 Time:  22.341501235961914\n",
      "Epoch:  0 Loss:  0.6017178297042847 Time:  22.370670080184937\n",
      "Epoch:  0 Loss:  0.5218397378921509 Time:  22.80814242362976\n",
      "Epoch:  0 Loss:  0.5177385210990906 Time:  21.35559391975403\n",
      "Epoch:  0 Loss:  0.8175111413002014 Time:  21.947925806045532\n",
      "Epoch:  0 Loss:  0.5926085710525513 Time:  20.694735050201416\n",
      "Epoch:  0 Loss:  0.6497496366500854 Time:  22.696333408355713\n",
      "Epoch:  0 Loss:  0.8963435292243958 Time:  20.560023069381714\n",
      "Epoch:  0 Loss:  0.6072114109992981 Time:  21.256189107894897\n",
      "Epoch:  0 Loss:  0.6955116987228394 Time:  21.60047721862793\n",
      "Epoch:  0 Loss:  0.7064409852027893 Time:  21.76408886909485\n",
      "Epoch:  0 Loss:  0.5762544274330139 Time:  21.535347938537598\n",
      "Epoch:  0 Loss:  0.678614616394043 Time:  22.26111340522766\n",
      "Epoch:  0 Loss:  0.8031231760978699 Time:  21.270185708999634\n",
      "Epoch:  0 Loss:  0.7684766054153442 Time:  22.82668900489807\n",
      "Epoch:  0 Loss:  0.5349060893058777 Time:  22.084729433059692\n",
      "Epoch:  0 Loss:  0.6263412833213806 Time:  21.869643926620483\n",
      "Epoch:  0 Loss:  0.7832916975021362 Time:  21.816290616989136\n",
      "Epoch:  0 Loss:  0.42663249373435974 Time:  21.15091872215271\n",
      "Epoch:  0 Loss:  0.5873977541923523 Time:  20.89080238342285\n",
      "Epoch:  0 Loss:  0.49763020873069763 Time:  22.67155885696411\n",
      "Epoch:  0 Loss:  0.5968728065490723 Time:  21.370351314544678\n",
      "Epoch:  0 Loss:  0.5923985242843628 Time:  20.91435146331787\n",
      "Epoch:  0 Loss:  0.5562836527824402 Time:  21.988133192062378\n",
      "Epoch:  0 Loss:  0.582306981086731 Time:  20.875412940979004\n",
      "Epoch:  0 Loss:  0.5095762610435486 Time:  21.50005841255188\n",
      "Epoch:  0 Loss:  0.4243716597557068 Time:  22.734033823013306\n",
      "Epoch:  0 Loss:  0.6824949979782104 Time:  22.671636819839478\n",
      "Epoch:  0 Loss:  0.5860189199447632 Time:  22.011128187179565\n",
      "Epoch:  0 Loss:  0.4419783055782318 Time:  22.26434302330017\n",
      "Epoch:  0 Loss:  0.7555798292160034 Time:  22.025829792022705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Loss:  0.6449962854385376 Time:  23.16273069381714\n",
      "Epoch:  0 Loss:  0.6443101763725281 Time:  23.21101689338684\n",
      "Epoch:  0 Loss:  0.8421434164047241 Time:  24.054510354995728\n",
      "Epoch:  0 Loss:  0.5735876560211182 Time:  24.370686054229736\n",
      "Epoch:  0 Loss:  0.7125294804573059 Time:  23.85941195487976\n",
      "Epoch:  0 Loss:  0.3742516338825226 Time:  24.26249623298645\n",
      "Epoch:  0 Loss:  0.7089082598686218 Time:  23.545748949050903\n",
      "Epoch:  0 Loss:  0.6751551032066345 Time:  22.466310501098633\n",
      "Epoch:  0 Loss:  0.818016529083252 Time:  22.486172437667847\n",
      "Epoch:  0 Loss:  0.6928835511207581 Time:  23.019739866256714\n",
      "Epoch:  0 Loss:  0.4787633419036865 Time:  22.8200044631958\n",
      "Epoch:  0 Loss:  0.3742695748806 Time:  22.63399314880371\n",
      "Epoch:  0 Loss:  0.5782677531242371 Time:  22.528327226638794\n",
      "Epoch:  0 Loss:  0.6044556498527527 Time:  22.007671356201172\n",
      "Epoch:  0 Loss:  0.6717483401298523 Time:  22.34947180747986\n",
      "Epoch:  0 Loss:  0.7201865315437317 Time:  23.06312108039856\n",
      "Epoch:  0 Loss:  0.4445279538631439 Time:  22.612220287322998\n",
      "Epoch:  0 Loss:  0.6402038931846619 Time:  21.730538845062256\n",
      "Epoch:  0 Loss:  0.7347763776779175 Time:  22.11965823173523\n",
      "Epoch:  0 Loss:  0.47365260124206543 Time:  26.07093644142151\n",
      "Epoch:  0 Loss:  0.49378064274787903 Time:  22.3568696975708\n",
      "Epoch:  0 Loss:  0.5467758178710938 Time:  21.245060205459595\n",
      "Epoch:  0 Loss:  0.6445573568344116 Time:  21.55843162536621\n",
      "Epoch:  0 Loss:  0.6057265996932983 Time:  21.249064922332764\n",
      "Epoch:  0 Loss:  0.6259449124336243 Time:  22.983678579330444\n",
      "Epoch:  0 Loss:  0.5166221261024475 Time:  24.213069438934326\n",
      "Epoch:  0 Loss:  0.5559183955192566 Time:  23.225702047348022\n",
      "Epoch:  0 Loss:  0.5803754925727844 Time:  23.999369621276855\n",
      "Epoch:  0 Loss:  0.7461804747581482 Time:  22.3342387676239\n",
      "Epoch:  0 Loss:  0.825123131275177 Time:  22.782484531402588\n",
      "Epoch:  0 Loss:  0.5785841345787048 Time:  21.93453574180603\n",
      "Epoch:  0 Loss:  0.6088407635688782 Time:  22.53419780731201\n",
      "Epoch:  0 Loss:  0.6199925541877747 Time:  21.64191246032715\n",
      "Epoch:  0 Loss:  0.5974836945533752 Time:  22.463497638702393\n",
      "Epoch:  0 Loss:  0.5135462284088135 Time:  22.45537281036377\n",
      "Epoch:  0 Loss:  0.5712937712669373 Time:  22.320316553115845\n",
      "Epoch:  0 Loss:  0.5791612267494202 Time:  22.190767765045166\n",
      "Epoch:  0 Loss:  0.7209380269050598 Time:  22.1053524017334\n",
      "Epoch:  0 Loss:  0.6402317881584167 Time:  22.67422580718994\n",
      "Epoch:  0 Loss:  0.5131943821907043 Time:  22.7564377784729\n",
      "Epoch:  0 Loss:  0.6971322298049927 Time:  22.019679069519043\n",
      "Epoch:  0 Loss:  0.5919265151023865 Time:  21.458826065063477\n",
      "Epoch:  0 Loss:  0.625065803527832 Time:  20.523546934127808\n",
      "Epoch:  0 Loss:  0.4243091940879822 Time:  22.129011392593384\n",
      "Epoch:  0 Loss:  0.5478506088256836 Time:  24.67123508453369\n",
      "Epoch:  0 Loss:  0.8955390453338623 Time:  23.3144850730896\n",
      "Epoch:  0 Loss:  0.5562534928321838 Time:  22.79095149040222\n",
      "Epoch:  0 Loss:  0.5943068861961365 Time:  22.309749841690063\n",
      "Epoch:  0 Loss:  0.43132755160331726 Time:  24.47664523124695\n",
      "Epoch:  0 Loss:  0.6420765519142151 Time:  27.09719729423523\n",
      "Epoch:  0 Loss:  0.6729538440704346 Time:  25.770070552825928\n",
      "Epoch:  0 Loss:  0.40403738617897034 Time:  25.883728504180908\n",
      "Epoch:  0 Loss:  0.448104590177536 Time:  22.791563987731934\n",
      "Epoch:  0 Loss:  0.593988299369812 Time:  24.032955408096313\n",
      "Epoch:  0 Loss:  0.7939307689666748 Time:  25.700953722000122\n",
      "Epoch:  0 Loss:  0.5720153450965881 Time:  28.364794492721558\n",
      "Epoch:  0 Loss:  0.7646325826644897 Time:  24.176479816436768\n",
      "Epoch:  0 Loss:  0.5476876497268677 Time:  23.95687699317932\n",
      "Epoch:  0 Loss:  0.636971116065979 Time:  24.28428602218628\n",
      "Epoch:  0 Loss:  0.45709624886512756 Time:  23.020113706588745\n",
      "Epoch:  0 Loss:  0.5785143971443176 Time:  28.453998804092407\n",
      "Epoch:  0 Loss:  0.608555257320404 Time:  23.469157218933105\n",
      "Epoch:  0 Loss:  0.6360543370246887 Time:  21.016429662704468\n",
      "Epoch:  0 Loss:  0.6560531258583069 Time:  23.7469425201416\n",
      "Epoch:  0 Loss:  0.618994951248169 Time:  22.08715271949768\n",
      "Epoch:  0 Loss:  0.5880914330482483 Time:  26.4717378616333\n",
      "Epoch:  0 Loss:  0.8050819635391235 Time:  24.35869002342224\n",
      "Epoch:  0 Loss:  0.5907387137413025 Time:  21.242829084396362\n",
      "Epoch:  0 Loss:  0.5664466023445129 Time:  23.213990211486816\n",
      "Epoch:  0 Loss:  0.4994538724422455 Time:  22.68817448616028\n",
      "Epoch:  0 Loss:  0.4615756869316101 Time:  22.30041766166687\n",
      "Epoch:  0 Loss:  0.4761779010295868 Time:  22.684285163879395\n",
      "Epoch:  0 Loss:  0.6750802993774414 Time:  23.69287109375\n",
      "Epoch:  0 Loss:  0.48421427607536316 Time:  22.976648807525635\n",
      "Epoch:  0 Loss:  0.3713572025299072 Time:  23.272873878479004\n",
      "Epoch:  0 Loss:  0.43354153633117676 Time:  22.332946300506592\n",
      "Epoch:  0 Loss:  0.4881957769393921 Time:  21.983147621154785\n",
      "Epoch:  0 Loss:  0.5250621438026428 Time:  21.46696639060974\n",
      "Epoch:  0 Loss:  0.6252621412277222 Time:  22.22032880783081\n",
      "Epoch:  0 Loss:  0.5811632871627808 Time:  21.362828731536865\n",
      "Epoch:  0 Loss:  0.5686264634132385 Time:  21.615938186645508\n",
      "Epoch:  0 Loss:  0.45835766196250916 Time:  21.18782639503479\n",
      "Epoch:  0 Loss:  0.5724973678588867 Time:  21.727561235427856\n",
      "Epoch:  0 Loss:  0.6509731411933899 Time:  22.889591932296753\n",
      "Epoch:  0 Loss:  0.5920424461364746 Time:  22.46616220474243\n",
      "Epoch:  0 Loss:  0.6682853698730469 Time:  23.37653923034668\n",
      "Epoch:  0 Loss:  0.425518274307251 Time:  23.20003890991211\n",
      "Epoch:  0 Loss:  0.5689939260482788 Time:  22.215100049972534\n",
      "Epoch:  0 Loss:  0.529502809047699 Time:  25.179029941558838\n",
      "Epoch:  0 Loss:  0.7077593207359314 Time:  23.700100660324097\n",
      "Epoch:  0 Loss:  0.6518986821174622 Time:  22.793187618255615\n",
      "Epoch:  0 Loss:  0.5532670617103577 Time:  22.638622045516968\n",
      "Epoch:  0 Loss:  0.4172000288963318 Time:  23.900511980056763\n",
      "Epoch:  0 Loss:  0.48502564430236816 Time:  22.34017324447632\n",
      "Epoch:  0 Loss:  0.584112823009491 Time:  22.871453285217285\n",
      "Epoch:  0 Loss:  0.6308566331863403 Time:  21.66530466079712\n",
      "Epoch:  0 Loss:  0.7034247517585754 Time:  23.480223417282104\n",
      "Epoch:  0 Loss:  0.7440837621688843 Time:  22.96552538871765\n",
      "Epoch:  0 Loss:  0.6072188019752502 Time:  21.900660753250122\n",
      "Epoch:  0 Loss:  0.47890573740005493 Time:  24.212936878204346\n",
      "Epoch:  0 Loss:  0.48437535762786865 Time:  24.091535329818726\n",
      "Epoch:  0 Loss:  0.6059572696685791 Time:  22.144705295562744\n",
      "Epoch:  0 Loss:  0.668006956577301 Time:  23.922120332717896\n",
      "Epoch:  0 Loss:  0.6712145805358887 Time:  22.030259609222412\n",
      "Epoch:  0 Loss:  0.6563972234725952 Time:  23.112788677215576\n",
      "Epoch:  0 Loss:  0.6148680448532104 Time:  24.721697092056274\n",
      "Epoch:  0 Loss:  0.6536574959754944 Time:  22.211925983428955\n",
      "Epoch:  0 Loss:  0.5593600869178772 Time:  21.21588397026062\n",
      "Epoch:  0 Loss:  0.72079998254776 Time:  22.48590850830078\n",
      "Epoch:  0 Loss:  0.5071263313293457 Time:  21.766292333602905\n",
      "Epoch:  0 Loss:  0.4270212948322296 Time:  22.362043380737305\n",
      "Epoch:  0 Loss:  0.679269552230835 Time:  22.272321939468384\n",
      "Epoch:  0 Loss:  0.5330636501312256 Time:  22.118412733078003\n",
      "Epoch:  0 Loss:  0.763155460357666 Time:  21.620821714401245\n",
      "Epoch:  0 Loss:  0.6461709141731262 Time:  24.908466815948486\n",
      "Epoch:  0 Loss:  0.586489200592041 Time:  23.720312118530273\n",
      "Epoch:  0 Loss:  0.5390201807022095 Time:  24.129503965377808\n",
      "Epoch:  0 Loss:  0.5919675230979919 Time:  21.83841848373413\n",
      "Epoch:  0 Loss:  0.5809564590454102 Time:  22.008212327957153\n",
      "Epoch:  0 Loss:  0.6036584377288818 Time:  21.511277675628662\n",
      "Epoch:  0 Loss:  0.5896567106246948 Time:  22.53791618347168\n",
      "Epoch:  0 Loss:  0.4879922568798065 Time:  24.540858030319214\n",
      "Epoch:  0 Loss:  0.6059931516647339 Time:  24.83980655670166\n",
      "Epoch:  0 Loss:  0.5119825601577759 Time:  23.74366855621338\n",
      "Epoch:  0 Loss:  0.6580499410629272 Time:  24.290372133255005\n",
      "Epoch:  0 Loss:  0.6769991517066956 Time:  22.62217140197754\n",
      "Epoch:  0 Loss:  0.7227978110313416 Time:  22.342589139938354\n",
      "Epoch:  0 Loss:  0.6704811453819275 Time:  24.10017442703247\n",
      "Epoch:  0 Loss:  0.46619564294815063 Time:  23.07910180091858\n",
      "Epoch:  0 Loss:  0.4223902225494385 Time:  22.110381603240967\n",
      "Epoch:  0 Loss:  0.42568662762641907 Time:  21.43692636489868\n",
      "Epoch:  0 Loss:  0.6258668899536133 Time:  22.235601663589478\n",
      "Epoch:  0 Loss:  0.6291962265968323 Time:  21.097524642944336\n",
      "Epoch:  0 Loss:  0.6146005392074585 Time:  21.88907527923584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Loss:  0.7389694452285767 Time:  22.9824321269989\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12972/378770988.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiffusion_imputer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiffusion_imputer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12972/1193506027.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, data, epochs, lr, loss_func, device, verbose)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mpredicted_noise\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'random'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_noise\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m             )\n\u001b[1;32m--> 487\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    198\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    for batch in data:\n",
    "        train(diffusion_imputer, batch, epochs = 1, lr = 0.001, loss_func = diffusion_imputer.loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = torch.stack([torch.Tensor(test_set[i][\"observed_data\"]) for i in range(batch_size)], dim=0)\n",
    "imputation_mask = torch.zeros_like(test_data)\n",
    "imputation_mask[:, 33:35, :] = 1\n",
    "\n",
    "given_points = test_data * (1-imputation_mask)\n",
    "\n",
    "eval_points = test_data * imputation_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 859963392 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12972/1735265373.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_number\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0msamples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiffusion_imputer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimputation_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12972/44158967.py\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, data, imputation_mask)\u001b[0m\n\u001b[0;32m    160\u001b[0m             \u001b[0mdata_to_transformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_to_transformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mti\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed_dim_transformer_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m             \u001b[0mpredicted_noise\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_to_transformer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m             \u001b[0mpredicted_noise\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_noise\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m             \u001b[0mpredicted_noise\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredicted_noise\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mimputation_mask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12972/2350406515.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mencoder_module\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder_modules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;31m#y = self.layer_norm(x)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12972/223121928.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \"\"\"\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mattention\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime_series_attention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mattention\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mattention\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12972/2400571656.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, query, key, value, mask)\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mkq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bhtfe,bhxye->bhtfxy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey_reshaped\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_reshaped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0mdot_prod_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkq\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim_per_head\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;31m# if mask is not None:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 859963392 bytes."
     ]
    }
   ],
   "source": [
    "sample_number = 40\n",
    "samples = []\n",
    "for i in range(sample_number):\n",
    "    samples.append(diffusion_imputer.eval(test_data, imputation_mask))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qlist =[0.05,0.25,0.5,0.75,0.95]\n",
    "quantiles_imp= []\n",
    "for q in qlist:\n",
    "    quantiles_imp.append(torch.quantile(samples, q, dim=1).cpu().numpy()*(1-given_points) + test_data * given_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = test_data.shape[1]\n",
    "K = test_data.shape[2]\n",
    "\n",
    "###airquality###\n",
    "dataind = 10 #change to visualize a different sample\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 16\n",
    "fig, axes = plt.subplots(nrows=9, ncols=4,figsize=(24.0, 36.0))\n",
    "fig.delaxes(axes[-1][-1])\n",
    "\n",
    "for k in range(K):\n",
    "    df = pd.DataFrame({\"x\":np.arange(0,L), \"val\":test_data[dataind,:,k], \"y\":eval_points[dataind,:,k]})\n",
    "    df = df[df.y != 0]\n",
    "    df2 = pd.DataFrame({\"x\":np.arange(0,L), \"val\":test_data[dataind,:,k], \"y\":given_points[dataind,:,k]})\n",
    "    df2 = df2[df2.y != 0]\n",
    "    row = k // 4\n",
    "    col = k % 4\n",
    "    axes[row][col].plot(range(0,L), quantiles_imp[2][dataind,:,k], color = 'g',linestyle='solid',label='CSDI')\n",
    "    axes[row][col].fill_between(range(0,L), quantiles_imp[0][dataind,:,k],quantiles_imp[4][dataind,:,k],\n",
    "                    color='g', alpha=0.3)\n",
    "    axes[row][col].plot(df.x,df.val, color = 'b',marker = 'o', linestyle='None')\n",
    "    axes[row][col].plot(df2.x,df2.val, color = 'r',marker = 'x', linestyle='None')\n",
    "    if col == 0:\n",
    "        plt.setp(axes[row, 0], ylabel='value')\n",
    "    if row == -1:\n",
    "        plt.setp(axes[-1, col], xlabel='time')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP+aGCD97qu+sWn5anktvG3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
